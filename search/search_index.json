{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<ul> <li>Git-inspired data version control</li> <li>Cross-table transactions and visibility</li> <li>Open data lake approach, supporting Hive, Spark, Dremio, Trino, etc.</li> <li>Works with Apache Iceberg tables</li> <li>Run as a Docker image or on Kubernetes</li> <li>Fork it on GitHub</li> </ul> <p>Get in touch via our Google Group and our Zulip Chat and follow us on Twitter. Nessie source code, code contributions and bug reports are on GitHub.</p> <p> </p>"},{"location":"community/","title":"Community","text":"<p>Nessie is developed as a consensus-driven open source product under the Apache 2.0 license. Development is done in the open leveraging GitHub issues, PRs and using Google Groups as a mailing list.</p> <p></p>"},{"location":"community/#get-in-touch","title":"Get In Touch","text":"The developers on Nessie frequent the project\u2019s Zulip Chat.    Use this link to join. If long form is more your thing, we also have created a mailing list on Google groups   that you can subscribe to. Nessie is developed via GitHub issues and pull requests. If you see a problem   or want to enhance the product, we suggest you file a GitHub issue for developers to   review. The @projectnessie account on Twitter is our official account. Follow-up to keep   to date on what is happening with Project Nessie! Video content for Nessie will be hosted on our YouTube channel. Our website is all maintained in our source repository. If there is something you think   can be improved, feel free to fork our repository and post a pull request."},{"location":"community/#contribution","title":"Contribution","text":"<p>All contributors are welcome to Project Nessie. To get started, feel free to introduce yourself on Zulip or our Google Group. Nessie is open to everyone!</p> <p>Please see the CONTRIBUTING.md guide for more information on how to contribute.</p>"},{"location":"releases-0.49/","title":"Release Notes up to 0.49","text":"<p>See Nessie Server upgrade notes for supported upgrade paths.</p>"},{"location":"releases-0.49/#recent-releases","title":"Recent releases","text":"<p>See Main Release Notes page for the most recent releases.</p>"},{"location":"releases-0.49/#0490-release-february-17-2023","title":"0.49.0 Release (February 17, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Update GH WFs to publish Java images for many platforms (#6110)</li> <li>GH uploaded artifacts retention (#6115)</li> <li>Test: Update the validation to unblock query-engine-integration-tests (#6117)</li> <li>GH workflows: prevent running a bunch of stuff on forks (#6116)</li> <li>Revert changes to Chart.yaml maintainers section (#6113)</li> <li>Fail test discovery in MultiEnvTestEngine if no environments are defined (#6108)</li> <li>Disable :nessie-compatibility-common tests on macOS (#6106)</li> <li>Disable NPM tests on Windows (#6107)</li> <li>Simplify Quarkus build configuration (#6103)</li> <li>Automatically generate Helm chart documentation (#6095)</li> <li>Add Jakarta\u2019s ws-rs to <code>javax.ws.rs.*</code> (#6067)</li> <li>Add Jakarta\u2019s validation constraints to <code>javax.validation.constraints.*</code> (#6096)</li> <li>Add Jakarta\u2019s <code>@Inject</code> to <code>javax.Inject</code> (#6093)</li> <li>Add Jakarta\u2019s <code>@RequestScoped</code> to <code>javax.enterprise.context.RequestScoped</code> (#6092)</li> <li>Add Jakarta\u2019s <code>@Nullable</code> to <code>javax.annotation.Nullable</code> (#6088)</li> <li>Add Jakarta\u2019s <code>@Nonnull</code> to <code>javax.annotation.Nonnull</code> (#6066)</li> <li>Ability to configure PVC selector labels (#6074)</li> <li>Gatling simulation for a mixed workload (#6068)</li> <li>Split ws-rs-api implementors into javax and jakarta (#6065)</li> <li>Add API v2 test for commit parents (#6073)</li> <li>Nit: replace Jetbrains not-null annotation import (#6072)</li> <li>CI: Fix PR integrations workflow (#6071)</li> <li>Make Nessie build with <code>javax.*</code> and <code>jakarta.*</code> dependencies (#6064)</li> <li>Remove <code>@ThreadSafe</code> annotation, no <code>jakarta.*</code> counterpart (#6063)</li> <li>Remove <code>@CheckForNull</code> annotation, no <code>jakarta.*</code> counterpart (#6062)</li> <li>Tests: let tests in nessie-compatibility-common use soft-assertions (#6061)</li> </ul>"},{"location":"releases-0.49/#0482-release-february-08-2023","title":"0.48.2 Release (February 08, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Fix dependencies in :nessie-compatibility-common (#6055)</li> <li>fix website link in README (#6056)</li> <li>Nit: rename build utility function to <code>buildForJava11</code> (#6053)</li> <li>Reenable JVM monitoring in native images (#6052)</li> <li>Cleanup InmemoryStore after tests (#6048)</li> <li>Cleanup README.md badges (#6051)</li> <li>CI: Prevent OOM/GCLocker-issue in tests / disable Micrometer JVM-Metrics for tests (#6050)</li> <li>Clear in-memory database-adapter between REST-tests in Quarkus (#6049)</li> </ul>"},{"location":"releases-0.49/#0481-release-february-06-2023","title":"0.48.1 Release (February 06, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Release-WF: Fix openapi yaml attachment (#6040)</li> </ul>"},{"location":"releases-0.49/#0480-release-february-06-2023","title":"0.48.0 Release (February 06, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Produce NessieBadRequestException when paging is not available (#6020)</li> <li>Quarkus builds: modernize &amp; simplify (#6019)</li> <li>Expose definitive Nessie API version to tests (#6021)</li> <li>CI: Gradle cache read-only for snapshot publication workflow (#6018)</li> <li>REST v2: return parent commit IDs in <code>CommitMeta.getParentCommitHashes</code> (#6001)</li> <li>REST API v2: get-multiple-namespaces w/ option for direct children (#6010)</li> <li>Quarkus-nit: build script cleanup (#6016)</li> <li>Refactor API v2 reference resolution logic into a utility class (#6012)</li> <li>Quarkus: make <code>QuarkusBuild</code> not caching (#6015)</li> <li>Add missing explicit rename-table test (#6008)</li> <li>Do not memoize CommitMeta.properties, it is derived (#6006)</li> <li>CI: Do not cache Python stuff (#6004)</li> <li>Add support for API v2 to the compatibility test framework (#6000)</li> <li>Add java client-based test for Entry.getContentId() (#5996)</li> <li>UI: Sort properties in commit details view (#5987)</li> <li>Allow retrieval of <code>Content</code> with <code>get-entries</code> for API v2 (#5985)</li> <li>CI: fix helm-testing action refs (#6005)</li> <li>CI: Helm testing (#6003)</li> <li>Revert \u201cMake <code>QuarkusBuild</code> cacheable\u201d (#5997)</li> <li>Nessie client doc improvements (#5992)</li> <li>Add effective-reference to get-entries, diff and modify-namespace responses (#5984)</li> <li>Build: Fix task dependency warnings for code coverage (#5990)</li> <li>Let content-generator use REST API v2 (#5948)</li> <li>REST API v2 : Let \u2018get-content(s)\u2019 return the reference (#5947)</li> <li>Minor convenience methods <code>getElementsArray()</code> to <code>ContentKey</code> + <code>Namespace</code> (#5983)</li> <li>Nit: Prevent java compilation warning (#5978)</li> <li>Re-make Gatling sims debuggable after the move to Gradle (#5972)</li> <li>Gatling Simulations - error out Nessie actions by default (#5971)</li> <li>Helm: Add storage class for RockDB PVC (#5981)</li> <li>Fix <code>MathAbsoluteNegative</code> in <code>NonTransactionalDatabaseAdapter</code> (#5979)</li> <li>Make <code>QuarkusBuild</code> cacheable (#5974)</li> <li>Add <code>@JsonView</code> really everywhere (#5965)</li> <li>Add PVC for RocksDB in Nessie Helm chart (#5952)</li> <li>Let Gatling simulations use Nessie API v2 (#5970)</li> <li>Build: Make <code>QuarkusGenerateCode</code> tasks cacheable (#5973)</li> <li>Nit: <code>var</code> -&gt; <code>val</code> in quarkus builds (#5975)</li> <li>Remove usage of deprecated <code>StreamingUtil</code> (#5954)</li> <li>Nit: typo (#5951)</li> <li>Add <code>@JsonView</code> everywhere (#5953)</li> <li>REST v2: Correct namespace key evaluation for delete + get-multiple namespaces (#5946)</li> <li>Minor: immutables contstructors for <code>Branch</code> + <code>Tag</code> + <code>Detached</code> (#5944)</li> <li>Replace use of unpaged <code>get()</code> when listing references from SQL extensions (#5950)</li> <li>Add utility method <code>CommitResponse.toAddedContentsMap()</code> (#5945)</li> <li>Minor Namespace.isEmpty() optimization (#5943)</li> <li>Use the Smallrye OpenAPI Gradle plugin (#5936)</li> <li>Add <code>.scalafmt.conf</code> for IntelliJ (#5942)</li> <li>Update merge \u201cno hashes\u201d error message (#5938)</li> <li>Separate version catalog for <code>buildSrc/</code> (#5935)</li> <li>Re-add batched access checks &amp; replace <code>j.s.AccessControlException</code> (#5930)</li> <li>Move Jersey test dependencies to where they belong to (#5933)</li> <li>Refactor Service/REST/API testing (#5924)</li> <li>Unify <code>*ApiImplWithAuthorization</code> and \u201cbase\u201d <code>*ApiImpl</code> classes (#5927)</li> <li>REST v2 - fetch HEAD if hashOnRef is not specified (#5921)</li> <li>Prepare protocol independent pagination support for diff, get entries, commit log, get all refs (#5901)</li> <li>Just rename test classes (#5920)</li> <li>DatabaseAdapterExtension: configure database-adapter via a nested test class\u2019 parent context (#5919)</li> <li>Bump Spark to 3.1.3/3.2.3/3.3.1 (#5909)</li> <li>CI/Win: exclude UI tests (#5908)</li> </ul>"},{"location":"releases-0.49/#0471-release-january-20-2023","title":"0.47.1 Release (January 20, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Fix Jacoco reporting in Nessie Quarkus projects (#5902)</li> <li>Return latest state of deleted reference (#5905)</li> <li>Disable JVM monitoring in native images (#5903)</li> </ul>"},{"location":"releases-0.49/#0470-release-january-18-2023","title":"0.47.0 Release (January 18, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Use custom CDI extension for injecting Principals (#5883)</li> <li>Update V2 commit-response to include generated content-IDs (#5880)</li> <li>Add content-ID to REST API v2 entries endpoint (#5879)</li> <li>Gatling: properly prefer <code>sim.duration.seconds</code> over <code>sim.commits</code> (#5847)</li> <li>Spark extensions tests - no CID for new content &amp; common-ancestor (#5877)</li> <li>Content generator: no CID for new content, properly use expectedContent+CID for existing (#5876)</li> <li>Deltalake: no CID for new content, properly use expectedContent+CID for existing (#5875)</li> <li>Quarkus &amp; JAX-RS tests - no CID for new content &amp; common-ancestor (#5874)</li> <li>Do not expose junit-jupiter-engine via test artifacts (#5870)</li> <li>Gatling: allow running the Gatling simulations against an external Nessie (#5848)</li> <li>Testing: unify container start retry mechanism (#5844)</li> <li>Testing/benchmarking: fix configuration for persist-bench via system properties (#5843)</li> <li>Add macOS + Windows build check badges to README.md (#5849)</li> <li>Use REST API v2 in Gatling simulations (#5846)</li> <li>Testing: use full Quarkus listen-URL instead of just the port (#5845)</li> <li>GH WF: simplify some steps (#5869)</li> </ul>"},{"location":"releases-0.49/#0467-release-january-12-2023","title":"0.46.7 Release (January 12, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Expose Nessie API version to custom client builders in tests (#5839)</li> </ul>"},{"location":"releases-0.49/#0465-release-january-12-2023","title":"0.46.5 Release (January 12, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>CI: Fix jobs.if condition for mac+win workflows (#5840)</li> <li>nessie-quarkus-cli/jacoco: use a fresh jacoco data file (#5833)</li> <li>CI: cross-check macOS + Windows (#5705)</li> <li>Validate runtime parameters in service implementations. (#5828)</li> <li>Testcontainers: retry container launch (#5831)</li> <li>Test/CI: Introduce test parallelism constraints for test tasks (#5824)</li> <li>Actually assign <code>main</code> in AbstractContentGeneratorTest (#5830)</li> <li>Exclude compatibility tests from code-coverage (#5826)</li> <li>Disable Deltalog integration tests on Windows (#5825)</li> <li>Disable Deltalog integration tests on macOS (#5822)</li> <li>Disable tests on projects using testcontainers on Windows (#5817)</li> <li>Disable quarkus-jacoco on <code>:nessie-quarkus-cli</code> (#5820)</li> <li>Fix newline when testing under Windows (#5814)</li> <li>model/test: Platform independent line separator (#5815)</li> <li>Testing: add logging for projects using testcontainers (#5813)</li> <li>Testing: allow <code>test.log.level</code> in compatibility tests (#5812)</li> <li>Nit: compatibility-tests: remove wrong comment (#5816)</li> <li>Disable tests on Windows that have strict time requirements (#5807)</li> <li>Disable compatibility tests on Mac (#5805)</li> <li>GC/Tests: Ignore schema-less base-uri-test on Windows (#5804)</li> <li>Prevent using the Minio Extension on non-Linux OS (#5803)</li> <li>Proper Path handling in GC tool CLI (#5799)</li> <li>Make LocalMongoResource a bit more resilient (#5802)</li> <li>Testcontainers: increase number of start attempts from 1 to 5 (#5801)</li> <li>Disable Quarkus dev services for tests (#5800)</li> <li>Proper Path handling in IcebergContentToFiles (#5798)</li> <li>Proper Path handling in TestNessieIcebergViews (#5797)</li> <li>Fix ZIP importer handle leak (#5796)</li> <li>CI: Fix results-upload for PRs (#5795)</li> <li>CI: Remove deprecated <code>::set-output</code> (#5794)</li> <li>Nit: Suppress ClassCanBeStatic warnings (#5793)</li> </ul>"},{"location":"releases-0.49/#0463-release-january-06-2023","title":"0.46.3 Release (January 06, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Remove grep -v from the HISTORY.rst generation script (#5778)</li> <li>Fix MultiEnvTestFilter to pass inner test classes (#5775)</li> <li>Make OlderNessieServersExtension inject Nessie URIs (#5770)</li> <li>Remove duplicate startHash call in <code>HttpGetCommitLog</code> (#5756)</li> </ul>"},{"location":"releases-0.49/#0462-release-december-28-2022","title":"0.46.2 Release (December 28, 2022)","text":"<p>See Release information on GitHub.</p> <ul> <li>Add validation annotations to backend service interfaces (#5740)</li> <li>Docs: Update README.md for 0.46.0 release (#5734)</li> </ul>"},{"location":"releases-0.49/#0460-release-december-20-2022","title":"0.46.0 Release (December 20, 2022)","text":"<p>See Release information on GitHub.</p> <ul> <li>Make generated OpenAPI type names more readable (#5732)</li> <li>Add serialized form tests for <code>CommitMeta</code> v2 (#5731)</li> <li>Add v2 attributes to <code>CommitMeta</code> (#5706)</li> <li>CI: No duplicate checkstyle task runs (#5703)</li> <li>Allow publishing Gradle build scans from CI (#5701)</li> <li>Remove legacy Spark extensions (#5704)</li> <li>Fix flaky TestKeyGenerator (#5702)</li> <li>Support merge/transplant message overrides (#5686)</li> <li>Add common ancestor commit in merge tests (#5696)</li> <li>Add global test timeout for versioned/persist/ (#5690)</li> <li>Add DeleteContent command (#4773)</li> <li>Expose updated reference data in java client APIs (#5670)</li> <li>Fix OpenAPI docs for the required \u201cexpected hash\u201d parameters (#5665)</li> <li>Expose MergeBehavior to java clients (#5682)</li> <li>Expose <code>CommitResponse</code> in java client API (#5673)</li> <li>Migrate <code>FetchOption</code> to the <code>model</code> package (#5667)</li> <li>Add README.md to model (#5668)</li> <li>Use API v2 in GC CLI (#5660)</li> <li>Add pagination params to the Diff API v2 (#5452)</li> <li>Replace deprecated native build parameters (#5662)</li> <li>Move Graal/native registration to nessie-quarkus-common (#5661)</li> <li>Increase gradle memory in integration tests (#5654)</li> <li>Remove unused Jaeger properties (#5647)</li> <li>Validate ID for explicitly created Namespaces (#5644)</li> <li>Gradle: no longer run test classes concurrently (#5653)</li> <li>Nit: remove TODO (#5652)</li> <li>Fix GetReferenceParams description for API v2 (#5639)</li> <li>Import: move repo-setup part to nessie-versioned-transfer (#5640)</li> <li>Export using reference/commit-log scanning (#5635)</li> <li>Switch nessie-client to use OpenTelemetry (#5607)</li> <li>Version-store tests: use proper Put operations (#5614)</li> <li>DatabaseAdapter/identify-heads-and-forks: Allow commit-log scanning (#5638)</li> <li>Version-store tests: Adjust test to currently undetected transplant conflict (#5613)</li> <li>DatabaseAdapterExtension: Allow adapter-configurations on fields (#5637)</li> <li>Export: store exporting Nessie version and show during import (#5636)</li> <li>Adopt REST tests to stricter validations (#5612)</li> <li>PersistVersionStore: expose missing additional-parents (#5633)</li> <li>Export/import: explicit tests for exporter/importer implementations (#5631)</li> <li>DatabaseAdapter/identifyHeadsAndForks: Don\u2019t return <code>NO_ANCESTOR</code> as fork point (#5632)</li> <li>Export/import: remove unused functionality from BatchWriter (#5630)</li> <li>Rely on new <code>NessieVersion</code> in CLI tools (#5629)</li> <li>Allow <code>-</code> as a reference name in v2 REST paths (#5618)</li> <li>Export/import: move file related code to separate package (#5628)</li> <li>Use soft-assertions in jaxrs-tests (#5610)</li> <li>Adopt versioned-tests to not set cid for \u201cnew\u201d <code>Content</code>; minor refactoring for merge/transplant (#5611)</li> <li>Deprecate commit lists in merge-results (#5599)</li> <li>Add Nessie version into nessie-model.jar (#5609)</li> <li>Minor verstion-store commit-log test refactoring (#5604)</li> <li>Minor merge test enhancements (#5603)</li> <li>Add \u201cdiff test\u201d with a key \u201cin between\u201d (#5602)</li> <li>Add a some test cases for \u201creference not found\u201d scenarios (#5601)</li> <li>Factory methods for EntriesResponse.Entry + DiffResponse.Entry + MergeResult.KeyDetails (#5600)</li> <li>Add expected content to v2 REST update namespace (#5598)</li> <li>Switch to OpenTelemetry in Nessie Quarkus server (#5605)</li> <li>Minor code clean-up in client-side Namespace operations (#5594)</li> <li>Fix v2 URL path mapping for getReferenceByName (#5596)</li> <li>Add REST API v2 (#5004)</li> <li>Fix <code>@Parameter</code> annotations in <code>DiffParams</code> (#5589)</li> <li>Build UI against Nessie API v1 from 0.45.0 (#5587)</li> </ul>"},{"location":"releases-0.49/#0450-release-november-29-2022","title":"0.45.0 Release (November 29, 2022)","text":"<p>See Release information on GitHub.</p> <ul> <li>Fix OpenAPI spec for DiffApi (#5584)</li> <li>Content generator: allow functional key patterns and more (#5575)</li> <li>CI: auto-destruct spawned Nessie Quarkus runner JVM (#5583)</li> <li>Scala/Gradle: keep Scala compiler alive, bump Scala 2.13 to 2.13.10 (#5576)</li> <li>Fix BaseExceptionMapper\u2019s NPE when an Exception\u2019s message is null (#5563)</li> <li>Add RefreshContent command (#5551)</li> <li>Fix erasing repo descriptions (#5557)</li> <li>Perftest: allow passing system properties to launched Nessie (#5556)</li> <li>Transfer: completely abstract <code>DatbaseAdapter</code> from core export/import (#5555)</li> <li>Remove dependency to database-adapter type from <code>ImportResult</code> (#5553)</li> <li>Deprecate <code>VersionStore.getRefLog()</code> (#5554)</li> <li>Nessie export/import: update abstraction for file-typed stuff (#5545)</li> <li>Nessie CLI: rename <code>call()</code> to <code>callWithDatabaseAdapter()</code> (#5540)</li> <li>Nessie import: check export version (#5543)</li> <li>Nessie CLI: Update wording for in-memory warning (#5542)</li> <li>Nessie/Quarkus: use <code>Instance&lt;DatabaseAdapter&gt;</code> (#5541)</li> <li>Update verification code for Nessie CLI erase-erpository (#5539)</li> <li>Add content-info Quarkus CLI command (#5501)</li> <li>Reduce memory pressure during Quarkus CLI integration tests. (#5530)</li> <li>Bump+change Quarkus builder image to Mandrel + 22.3 (#5526)</li> <li>renovate: put python back on weekly schedule (#5519)</li> <li>Docs: Update Spark Python/Java API docs (#5517)</li> <li>Fix latest Nessie and Iceberg versions in side docs (#5499)</li> <li>Refactor Nessie-Jax-RS extension a little bit (#5486)</li> <li>renovate: fix fileMatch regexp (#5489)</li> <li>fix renovate python requirements file pattern (#5481)</li> <li>Proper put-update-operation for namespace-update (#5484)</li> <li>Move tests from o.p.jaxrs to o.p.jaxrs.tests (#5483)</li> <li>Fix a few checkstyle warnings (#5482)</li> <li>Update protobuf plugin to 0.9.1 (#5457)</li> <li>Documentation accommodating new version format of Iceberg and Nessie artifacts (#5371)</li> <li>Pull <code>ProtobufHelperPlugin</code> from <code>gradle-build-plugins</code> (#5456)</li> <li>Refactor some test code to soft-assertions (#5446)</li> <li>Move Merge/Transplant classes to the api.v1.params package (#5435)</li> <li>Move old API classes into the api.v1 package (#5419)</li> <li>Move some tests from db-adapter test code to verstion-store test code (#5445)</li> <li>Support method-level NessieApiVersions annotations (#5436)</li> <li>Introduce multi-version API test framework (#5420)</li> <li>Extract service-side interfaces for RefLog and Namespace services (#5418)</li> <li>Extract common java client builder code (#5411)</li> <li>Extract service-side interfaces for client-facing services (#5412)</li> <li>Remove OpenAPI spec properties from build script. (#5413)</li> <li>Build UI against Nessie API v1 from 0.44.0 (#5408)</li> <li>Refactor TestAuthorizationRules (#5399)</li> <li>Java 11 HttpClient (#5280)</li> <li>Testing/nit: Logging for :nessie-s3minio ITs (#5398)</li> <li>Add non-trivial tests for assign branch/tag operations (#5395)</li> <li>Fix version in the Nessie Helm Chart (#5392)</li> <li>Extract a multi-env test engine into a module. (#5339)</li> <li>Move Nessie Client construction into a JUnit5 extension (#5370)</li> <li>Record next development version (#5387)</li> <li>GH workflows: Add missing <code>cache-read-only: true</code> (#5385)</li> <li>GH create release WF: next version not properly recorded (#5386)</li> </ul>"},{"location":"releases-0.49/#0440-release-october-18-2022","title":"0.44.0 Release (October 18, 2022)","text":"<p>See Release information on GitHub.</p> <ul> <li>Ninja: fix create-release WF</li> <li>Ninja: fix GH env reference</li> <li>Update README mentioning Iceberg 1.0 (#5384)</li> <li>Add Dan + Vladimir to devs list (#5381)</li> <li>GH release WF: default to \u201cminor\u201d version bump (#5380)</li> <li>GH release WF: Fix wrong task name (#5379)</li> <li>Automatically update release text files (#5377)</li> <li>GH release WF - fix log exclusion + log filter for rel-notes (#5378)</li> <li>Nessie GC: Docs (#5209)</li> <li>Nessie GC: Command line tool (#5227)</li> <li>Nessie GC: Iceberg functionality (#5207)</li> <li>Avoid direct dependency on iceberg-bundled-guava (#5366)</li> <li>quarkus-server tests use dynamic port from env (#5352)</li> <li>Fix missing placeholder for Preconditons.checkState (#5360)</li> <li>fix GH workflows still mentioning maven (#5353)</li> <li>Remove httpClient param from AbstractRest.init (#5354)</li> <li>Update pretty-ms to 8.x (#5341)</li> <li>DynamoDB related test changes (#5338)</li> <li>Split unsquashed merge tests into dedicated test methods (#5328)</li> <li>Update testing-library-react to 13.x (#5334)</li> <li>Bump actions/checkout from v3 to v3.1.0 (#5335)</li> <li>Update material-ui to mui 5.x.x (#5326)</li> <li>nessie-client/test: compress responses for all relevant HTTP methods (#5323)</li> <li>Fix micrometer path replacement patterns (#5321)</li> <li>Renovate: labels for java/javascript/python (#5318)</li> <li>Revert protobuf to 3.21.6 (#5317)</li> <li>Renovate: limit to 2 PRs per hour (#5319)</li> <li>Isolate http-level test from java client-level tests (#5314)</li> <li>Skip newer-java workflow on forks (#5294)</li> <li>Nessie client tests: Replace JDK\u2019s HTTP server w/ Jetty (#5285)</li> <li>Move internal classes of the Nessie HTTP client (#5286)</li> <li>GH/WF: Use <code>temurin</code> instead of \u2018zulu` (#5289)</li> <li>Nessie-client tests: do not compile w/ older Jackson versions (#5287)</li> <li>Test Nessie client with Java 8 (#5284)</li> <li>Integrate Jackson-version tests into Gradle build (#5279)</li> <li>Migrate to Gradle version catalogs (#5167)</li> <li>Unify Postgres container version declaration for tests</li> <li>Migrate from dependabot to renovate (#5166)</li> <li>Testing pre-requisites for Nessie GC: Two S3 testing projects (#5142)</li> <li>Fix iceberg verison on web site (#5222)</li> <li>Java 19 testing (#5221)</li> <li>Automatic patch releases (#5214)</li> <li>Unsupport 0.30.x versions (#5212)</li> <li>WF: Remove Maven part (#5213)</li> <li>Nessie GC: JDBC Live-Set-Repository (#5208)</li> <li>Allow Hadoop Spark config in tests (#5206)</li> <li>Nessie GC: mark &amp; sweep (<code>gc-base</code> module only) (#5144)</li> <li>Slight build scripts change for Java11+ target compat (#5203)</li> <li>Fix test failure caused by #5147 (#5204)</li> <li>No longer write ref-log entries for commit/merge/transplant (#5147)</li> <li>Schedule dependabot for npm + pip less frequently (#5201)</li> </ul>"},{"location":"releases-0.49/#0430-release-september-15-2022","title":"0.43.0 Release (September 15, 2022)","text":"<p>Rolling upgrades from versions before 0.40.0 are not supported!</p> <ul> <li>Prepare for Nessie GC</li> <li>Nessie export/import functionality</li> <li>Use Graal 22.2-java17 for native images</li> <li>Several test and build improvements</li> </ul>"},{"location":"releases-0.49/#0420-release-august-26-2022","title":"0.42.0 Release (August 26, 2022)","text":"<p>Rolling upgrades from versions before 0.40.0 are not supported!</p> <ul> <li>Key list creation fixes</li> <li>Pluggable content types</li> </ul>"},{"location":"releases-0.49/#0410-release-august-5-2022","title":"0.41.0 Release (August 5, 2022)","text":"<p>Rolling upgrades from versions before 0.40.0 are not supported!</p> <ul> <li>Do not persist and expose attached content metadata</li> <li>Fix issue when looking up key in an open-addressing-key-list</li> </ul>"},{"location":"releases-0.49/#0403-release-august-1-2022","title":"0.40.3 Release (August 1, 2022)","text":"<p>Rolling upgrades from versions before 0.40.0 are not supported!</p> <ul> <li>Remove Quarkus-BOM dependency from non-Quarkus projects</li> </ul>"},{"location":"releases-0.49/#0402-release-august-1-2022","title":"0.40.2 Release (August 1, 2022)","text":"<p>Rolling upgrades from versions before 0.40.0 are not supported!</p> <ul> <li>Dependency issues fixed</li> </ul>"},{"location":"releases-0.49/#0401-release-july-25-2022","title":"0.40.1 Release (July 25, 2022)","text":"<p>Rolling upgrades from versions before 0.40.0 are not supported!</p> <ul> <li>Fix key-lists issue resulting in server-errors (<code>ArrayIndexOutOfBoundsException</code>)</li> </ul>"},{"location":"releases-0.49/#0400-release-july-22-2022","title":"0.40.0 Release (July 22, 2022)","text":"<p>Rolling upgrades from earlier versions are not supported!</p> <ul> <li>Support Spark 3.1 (Scala 2.12), Spark 3.2 (Scala 2.12 + 2.13), Spark 3.3 (Scala 2.12 + 2.13)</li> <li>Support Iceberg 0.14.0</li> <li>Nessie Spark SQL extensions: handle timestamps w/ time-zones</li> <li>Nessie Spark SQL extensions: fix handling of <code>USE</code>d references for <code>CREATE/ASSIGN BRANCH/TAG</code></li> <li>Detailed merge/transplant result to allow inspection of conflicts</li> <li>Merge/transplant optionally allow \u201cforce-keep\u201d &amp; \u201cforce-merge\u201d of conflicting content-keys</li> <li>Iceberg table metadata stored in Nessie</li> <li>Improvements to REST error handling</li> <li>Performance improvements when there are many content-keys</li> <li>Hard limit on content-key length (max 20 elements, total 500 characters)</li> <li>Prevent (accidental) deletion of default branch</li> <li>Improved usage of automatic paging via <code>NessieApi</code></li> <li>Improvements to Nessie server health checks</li> <li>Add rolling-upgrade test suite in regular CI</li> <li>Daily testing against Java 17 + newer</li> <li>Switched to Java 17 in native images</li> <li>Build switched from Maven to Gradle</li> </ul>"},{"location":"releases-0.49/#0300-release-may-13-2022","title":"0.30.0 Release (May 13, 2022)","text":"<ul> <li>Add commit-ID to KeyListEntry when writing new key-lists</li> <li>Do not process old key-lists when retrieving values</li> <li>Helm: Fix k8s version detection in ingress template</li> <li>Database-adapter: commit optimizations</li> <li>Remove the configurable default for the configurable values for getDefaultMaxKeyListSize</li> <li>Dynamo/Mongo/TX: use bulk/batch requests where possible</li> </ul>"},{"location":"releases-0.49/#0290-release-may-5-2022","title":"0.29.0 Release (May 5, 2022)","text":"<ul> <li>Spark SQL: Configure ref.hash for NessieCatalog only when explicitly requested</li> <li>Escape all column names in SQL DML+DDL statements</li> <li>Use hashOnRef when fetching Namespaces</li> <li>Helm: Add ingress support for Kubernetes &gt;=1.22</li> <li>Fix CockroachDB transaction-retry behavior</li> </ul>"},{"location":"releases-0.49/#0280-release-april-26-2022","title":"0.28.0 Release (April 26, 2022)","text":"<ul> <li>Generate unique content IDs for explicitly created namespaces</li> <li>Fix patterns for metrics</li> <li>Various test improvements (CI + build)</li> <li>Various minor code fixes (fixes for errorprone warnings)</li> </ul>"},{"location":"releases-0.49/#0270-release-april-14-2022","title":"0.27.0 Release (April 14, 2022)","text":"<ul> <li>Support for Namespace properties</li> <li>Make NessieContentGenerator extensible</li> </ul>"},{"location":"releases-0.49/#0260-release-april-12-2022","title":"0.26.0 Release (April 12, 2022)","text":"<p>Rolling upgrades from an older Nessie release to 0.26.0 or newer are not supported.</p> <ul> <li>Remove global state for Iceberg tables + views</li> <li>Internal optimizations in database adapters, version store and API endpoints</li> <li>Change \u2018marker\u2019 character to indicate <code>.</code> in namespace/table identifiers from ASCII 0 to <code>\\u001D</code></li> <li>Opt-in to force-merge or not merge specific content keys (also for transplant)</li> <li>Squash merged and transplanted commits by default (with opt-out)</li> </ul>"},{"location":"releases-0.49/#0250-release-april-6-2022","title":"0.25.0 Release (April 6, 2022)","text":"<ul> <li>Nessie Quarkus Server can use Postgres as its backend database</li> <li>Explicitly define behavior of multiple commit-operations in a commit</li> <li>Load correct view metadata for a given ref</li> </ul>"},{"location":"releases-0.49/#0240-release-march-31-2022","title":"0.24.0 Release (March 31, 2022)","text":"<ul> <li>Prevent explicit creation of empty namespaces</li> <li>Add content-id to <code>BatchAccessChecker.canReadContentKey()</code></li> </ul>"},{"location":"releases-0.49/#0231-release-march-23-2022","title":"0.23.1 Release (March 23, 2022)","text":"<ul> <li>Support Namespaces</li> <li>CI \u201cperf tests\u201d improvements</li> <li>SQL Extension: Fix Create reference from a hash on non-default reference</li> <li>Enhance authorization checks</li> <li>Support custom annotations on Nessie Helm service</li> </ul>"},{"location":"releases-0.49/#0230-release-march-23-2022","title":"0.23.0 Release (March 23, 2022)","text":"<p>(not properly released)</p>"},{"location":"releases-0.49/#0220-release-march-11-2022","title":"0.22.0 Release (March 11, 2022)","text":"<ul> <li>Improve performance of <code>getValues</code></li> <li>Global-log compaction</li> <li>Store-level maintenance CLI</li> <li>Reduce number of tags for micrometer</li> <li>Grafana Dashboard for Nessie service</li> <li>Add new commands to generate-content tool</li> </ul>"},{"location":"releases-0.49/#0212-release-march-2-2022","title":"0.21.2 Release (March 2, 2022)","text":"<ul> <li>Fix serialization issue</li> </ul>"},{"location":"releases-0.49/#0211-release-march-2-2022","title":"0.21.1 Release (March 2, 2022)","text":"<ul> <li>(no user visible changes)</li> </ul>"},{"location":"releases-0.49/#0210-release-march-1-2022","title":"0.21.0 Release (March 1, 2022)","text":"<ul> <li>Add tracing to database-adapter internals</li> <li>Introduce compatibility and upgrade tests</li> <li>Refactor StreamingUtil class</li> <li>Support for Spark 3.1 + 3.2.1 in Nessie SQL extensions</li> <li>Proper usage of commit-id in Spark SQL extensions</li> <li>Add DELETE_DEFAULT_BRANCH access check</li> </ul>"},{"location":"releases-0.49/#0201-release-february-17-2022","title":"0.20.1 Release (February 17, 2022)","text":"<ul> <li>(no user visible changes)</li> </ul>"},{"location":"releases-0.49/#0200-release-february-16-2022","title":"0.20.0 Release (February 16, 2022)","text":"<ul> <li>Enable metrics for MongoDB by default</li> <li>Make try-loop-state configurable and add metrics</li> <li>Reorganize routes in UI</li> <li>Improve error reporting in Nessie Java client</li> <li>Various test improvements</li> </ul>"},{"location":"releases-0.49/#0190-release-february-7-2022","title":"0.19.0 Release (February 7, 2022)","text":"<ul> <li>Reads using \u201cdetached\u201d commit-ids w/o specifying a branch or tag name</li> <li>Bump Nessie client version in Nessie Spark-Extensions</li> <li>Support for Iceberg views (experimental)</li> <li>Diff endpoint supports named-references + commit-on-reference as well</li> <li>Add filtering for ref-log</li> <li>Rework and simplification of the Nessie UI code</li> </ul>"},{"location":"releases-0.49/#0180-release-january-13-2022","title":"0.18.0 Release (January 13, 2022)","text":"<ul> <li>Add reflog support</li> <li>Uses commit-timestamp \u201cnow\u201d for merged/transplanted commits</li> <li>Add new reflog command to the CLI</li> <li>Add support for Python 3.10</li> <li>Drop support for Python 3.6</li> </ul>"},{"location":"releases-0.49/#0170-release-december-08-2021","title":"0.17.0 Release (December 08, 2021)","text":"<ul> <li>Rename \u2018query_expression\u2019 query param to \u2018filter\u2019</li> <li>Rename \u2018max\u2019 query param to \u2018maxRecords\u2019</li> <li>Rename \u2018fetchAdditionalInfo\u2019 query param to \u2018fetch\u2019 for better extensibility</li> </ul>"},{"location":"releases-0.49/#0160-release-december-03-2021","title":"0.16.0 Release (December 03, 2021)","text":"<ul> <li>Mark optional fields as @Nullable / add validation for required fields in param classes</li> <li>Add CEL-filter to get-all-references</li> <li>Fix NPE for unchanged operation for fetching commit log with additional metadata</li> <li>Allow CEL-filtering on optional operations in get-commit-log</li> </ul>"},{"location":"releases-0.49/#0151-release-december-01-2021","title":"0.15.1 Release (December 01, 2021)","text":"<ul> <li>Fix wrongly placed validation annotation</li> </ul>"},{"location":"releases-0.49/#0150-release-december-01-2021","title":"0.15.0 Release (December 01, 2021)","text":"<ul> <li>Enhance commit log to optionally return original commit operations</li> <li>Optionally return commits ahead/behind, HEAD commit-meta, commit count,   common ancestor for named references</li> <li>Add missing REST endpoint to retrieve diff between two references</li> <li>Web UI improvements</li> </ul>"},{"location":"releases-0.49/#0140-release-november-12-2021","title":"0.14.0 Release (November 12, 2021)","text":"<ul> <li>Updated <code>IcebergTable</code> to track more information</li> <li>UI dependencies cleanup</li> <li>OpenAPI/REST API cleanup (breaking change)</li> </ul>"},{"location":"releases-0.49/#0121-release-november-3-2021","title":"0.12.1 Release (November 3, 2021)","text":"<ul> <li>Test code improvements</li> <li>Swagger examples fixes</li> <li>Web UI improvements</li> <li>Faster local builds w/ <code>./mvnw -Dquickly</code></li> </ul>"},{"location":"releases-0.49/#0120-release-october-25-2021","title":"0.12.0 Release (October 25, 2021)","text":"<ul> <li>Specialize and document Nessie exceptions</li> <li>Adopt Helm chart with new Nessie server settings</li> <li>Bump to GraalVM 21.3</li> </ul>"},{"location":"releases-0.49/#0110-release-october-20-2021","title":"0.11.0 Release (October 20, 2021)","text":"<ul> <li>Various doc + site improvements</li> <li>Fix Nessie\u2019s representation of global and on-reference state (Iceberg tables)</li> <li>Fix CLI log -n option</li> <li>Spark SQL extension improvements</li> </ul>"},{"location":"releases-0.49/#0101-release-october-8-2021","title":"0.10.1 Release (October 8, 2021)","text":"<ul> <li>Spark SQL extension changes</li> <li>Various (Open)API and client (Java, Python) implementation changes to prepare for better   backwards compatibility.</li> <li>JUnit extension based test support against different database/store types and configurations</li> <li>Unified version-store implementations into a part w/ the Nessie logic and a tier implementing   database access (MongoDB, DynamoDB, RocksDB, PostgreSQL).</li> <li>Remove JGit</li> </ul>"},{"location":"releases-0.49/#092-release-august-26-2021","title":"0.9.2 Release (August 26, 2021)","text":"<ul> <li>Cleanup &amp; fixes to OpenAPI examples, for Swagger UI</li> <li>Update Deltalake client to use version 1.0.0</li> <li>Drop Deltalake support for Spark 2</li> <li>Remove Hive-Metastore bridge</li> <li>Preparations for backwards-compatible Client-API</li> <li>Spark SQL Extensions: Introduce <code>IF NOT EXISTS</code> for <code>CREATE BRANCH</code>/<code>CREATE TAG</code></li> <li>Spark SQL Extensions: Updates to work with Iceberg 0.12.0</li> </ul>"},{"location":"releases-0.49/#090-release-august-9-2021","title":"0.9.0 Release (August 9, 2021)","text":"<ul> <li>Support for the upcoming Iceberg <code>0.12.0</code> release for both Spark 3.0 + 3.1</li> <li>Add docs for Nessie\u2019s metadata authorization</li> <li>Add SPI for Nessie authorization with Reference implementation</li> <li>Create Helm chart for Nessie</li> </ul>"},{"location":"releases-0.49/#083-release-july-19-2021","title":"0.8.3 Release (July 19, 2021)","text":"<ul> <li>Fix issue in spark sql extensions</li> <li>Python CLI: Fix ser/de of DeltaLakeTable when listing contents</li> </ul>"},{"location":"releases-0.49/#082-release-july-15-2021","title":"0.8.2 Release (July 15, 2021)","text":"<ul> <li>Add JAX-RS server implementation based on Glassfish/Jersey/Weld for integration testing   in Iceberg</li> <li>REST-API change: only accept named-references</li> <li>REST-API change: support time-travel on named-references</li> <li>REST-API change: Server-side commit range filtering</li> <li>OpenAPI: more explicit constraints on parameters</li> <li>OpenAPI: include OpenAPI yaml+json files in nessie-model artifact</li> <li>Remove already deprecated methods from ContentsApi</li> <li>Commit-log filtering on all fields of CommitMeta</li> <li>Use \u201cCommon Expression Language\u201d for commit-log and entries filtering</li> <li>Spark-extensions for Iceberg</li> <li>Prepare for multi-tenancy</li> <li>Gatling support + simulations</li> <li>Python CLI: Fix ser/de of DeltaLakeTable when listing contents</li> </ul>"},{"location":"releases-0.49/#070-release-june-15-2021","title":"0.7.0 Release (June 15, 2021)","text":"<ul> <li>Server-side filtering improvements for entries-listing and log-listing</li> <li>Distinguish between author &amp; committer in the Python CLI</li> <li>Allow setting author when committing via Python CLI</li> <li>Loosen pins for client install on Python cli</li> <li>Fix edge case when merging using in memory + jgit stores</li> <li>Gradle plugin improvements</li> <li>(Development) change to Google Code Style, add spotless plugin</li> <li>(CI) Add OWASP Dependency Check</li> </ul>"},{"location":"releases-0.49/#061-release-may-25-2021","title":"0.6.1 Release (May 25, 2021)","text":"<ul> <li>Gradle plugin improvements</li> </ul>"},{"location":"releases-0.49/#060-release-may-12-2021","title":"0.6.0 Release (May 12, 2021)","text":"<ul> <li>TreeApi.createReference() + commitMultipleOperations() return commit information</li> <li>Iceberg GC actions and a process to execute GC algorithm</li> </ul>"},{"location":"releases-0.49/#051-release-april-9-2021","title":"0.5.1 Release (April 9, 2021)","text":"<ul> <li>Fix Gradle plugin (non-deterministic order of dependencies causing failures)</li> <li>Fix Web-UI</li> </ul>"},{"location":"releases-0.49/#050-release-april-8-2021","title":"0.5.0 Release (April 8, 2021)","text":"<ul> <li>Iceberg table GC support</li> <li>Consistency fixes under high load</li> <li>Breaking changes to the backend to support richer commit metadata and data types</li> <li>Performance, metrics and tracing improvements</li> <li>Gradle plugin improvement for incremental builds</li> </ul>"},{"location":"releases-0.49/#040-release-march-8-2020","title":"0.4.0 Release (March 8, 2020)","text":"<ul> <li>rename base package to org.projectnessie</li> <li>NessieClient is now an interface and some easier builders</li> <li>initial implementation of GC algorithm</li> <li>major refactor of tiered classes for better modularity and extensibility</li> <li>observability improvements including better DynamoDB metrics and opentracing support for the client</li> </ul>"},{"location":"releases-0.49/#030-release-december-30-2020","title":"0.3.0 Release (December 30, 2020)","text":"<ul> <li>118 commits since 0.2.1</li> <li>Replace jax-rs client with one based on HttpURLConnection</li> <li>Update Quarkus to 1.10.5</li> <li>Improvements to Server including better UI routing, validation checks on inputs etc</li> <li>Various improvements to python client and cli. Including python3.9 support</li> </ul>"},{"location":"releases-0.49/#021-release-october-30-2020","title":"0.2.1 Release (October 30, 2020)","text":"<ul> <li>Fix missing dateutil requirement for pynessie install</li> <li>Address path discovery in Gradle plugin (for testing in external integrations)</li> </ul>"},{"location":"releases-0.49/#020-release-october-29-2020","title":"0.2.0 Release (October 29, 2020)","text":"<ul> <li>Update Nessie CLI commands to better match <code>git</code> syntax</li> <li>Update REST Apis to be more consistent and better</li> <li>Add support for merge &amp; cherry-pick in DynamoDB storage backend</li> <li>Add WebUI</li> <li>Introduce new DynamoDB optimizations to support faster log and entry retrieval</li> <li>Update to Quarkus 1.9.1</li> <li>Expose the new Store interface for low level storage implementations</li> <li>Introduce Quarkus Gradle runner plugin for easier third-party testing (e.g. Iceberg)</li> <li>Enable swagger-ui by default in Nessie service</li> </ul>"},{"location":"releases-0.49/#010-release-october-1-2020","title":"0.1.0 Release (October 1, 2020)","text":"<ul> <li>Initial release</li> </ul>"},{"location":"releases-0.69/","title":"Release Notes 0.50.0 to 0.69.2","text":"<p>See Nessie Server upgrade notes for supported upgrade paths.</p>"},{"location":"releases-0.69/#recent-releases","title":"Recent releases","text":"<p>See Main Release Notes page for the most recent releases.</p>"},{"location":"releases-0.69/#0692-release-august-29-2023","title":"0.69.2 Release (August 29, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases-0.69/#fixes","title":"Fixes","text":"<ul> <li>Nessie CLI: check-content command was incorrectly reporting deleted keys as missing content, when   using new storage model.</li> <li>GC Tool handles JDBC config via environment correctly</li> </ul>"},{"location":"releases-0.69/#commits","title":"Commits","text":"<ul> <li>Process outer test instances in clientFactoryForContext (#7444)</li> </ul>"},{"location":"releases-0.69/#0691-release-august-28-2023","title":"0.69.1 Release (August 28, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases-0.69/#fixes_1","title":"Fixes","text":"<ul> <li>Nessie CLI: check-content command was incorrectly reporting deleted keys as missing content, when   using new storage model.</li> <li>GC Tool handles JDBC config via environment correctly</li> </ul>"},{"location":"releases-0.69/#commits_1","title":"Commits","text":"<ul> <li>Refactor <code>ContentMapping.fetchContents</code> (#7434)</li> <li>Ninja: fix badged in README</li> </ul>"},{"location":"releases-0.69/#0690-release-august-25-2023","title":"0.69.0 Release (August 25, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases-0.69/#fixes_2","title":"Fixes","text":"<ul> <li>Nessie CLI: check-content command was incorrectly reporting deleted keys as missing content, when   using new storage model.</li> <li>GC Tool handles JDBC config via environment correctly</li> </ul>"},{"location":"releases-0.69/#commits_2","title":"Commits","text":"<ul> <li>Ninja: changelog</li> <li>GC: Finally fix JDBC values specified via environment (#7425)</li> <li>Make check-content not report deleted keys (#7427)</li> <li>Remove unused org.openapitools:openapi-generator-cli dependency (#7426)</li> <li>Move OTel wrappers to the modules defining corresponding interfaces (#7423)</li> <li>Pushdown filters in TreeApiImpl.getEntries (#7415)</li> </ul>"},{"location":"releases-0.69/#0680-release-august-24-2023","title":"0.68.0 Release (August 24, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases-0.69/#upgrade-notes","title":"Upgrade notes","text":"<ul> <li>If a repository has been imported using Nessie CLI 0.68.0 or higher, then this repo cannot be   later served by a Nessie server whose version is lower than 0.68.0. This is due to a change in the   internal repository description format.</li> </ul>"},{"location":"releases-0.69/#new-features","title":"New Features","text":"<ul> <li>Support BigTable in Helm charts</li> <li>NessieCLI check-content command is now compatible with Nessie\u2019s new storage model</li> </ul>"},{"location":"releases-0.69/#changes","title":"Changes","text":"<ul> <li>Java client API to assign/delete reference operations without specifying a concrete reference type   (no change to REST API).</li> <li>Creating and assigning references now requires a target hash to be specified.</li> </ul>"},{"location":"releases-0.69/#fixes_3","title":"Fixes","text":"<ul> <li>Secondary commit parents are now properly exported and imported</li> <li>Fix volume declarations for RocksDB in Helm</li> <li>Remove unnecessary repository-deletion when importing a legacy Nessie repo</li> <li>GC Tool uber-jar now includes AWS STS classes</li> <li>GC Tool now logs at <code>INFO</code> instead of <code>DEBUG</code></li> <li>GC Tool now correctly works against PostgreSQL</li> </ul>"},{"location":"releases-0.69/#commits_3","title":"Commits","text":"<ul> <li>Add Value.Check to Entry + KeyEntry (#7420)</li> <li>BigTable: use Batcher consistently for bulk mutations (#7416)</li> <li>CI: Disable tests in Windows + macOS jobs (#7403)</li> <li>Support reading arguments from a file in the <code>delete</code> CLI command (#7382)</li> <li>Make check-content command compatible with new model (#7411)</li> <li>Ninja: changelog</li> <li>Gradle: Replace deprecated <code>Project.buildDir</code> (#7412)</li> <li>Build/Gradle/Jandex/Quarkus: Attempt to fix the occasional <code>ConcurrentModificiationException</code> (#7413)</li> <li>GC: Allow configuring gc-tool JDBC via env only (#7410)</li> <li>GC: Fix duplicate-key error w/ PostgreSQL (#7409)</li> <li>Enforce expected hash presence when creating and assigning references (#7396)</li> <li>GC: Fix log level property interpolation and default to INFO (#7407)</li> <li>CI: Free disk space for NesQuEIT (#7402)</li> <li>Include Hibernate Validator in nessie-jaxrs-testextension (#7395)</li> <li>Mark repository as imported (#7380)</li> <li>Bump undertow from 2.2.24 to 2.2.26 (#7388)</li> <li>Add runtime dependency to AWS STS in the GC tool (#7385)</li> <li>Ability to update the repository description (#7376)</li> <li>Fix compilation warning (#7381)</li> <li>Don\u2019t initialize the repo in Persist Import tests (#7378)</li> <li>Fix javadocs of Persist.storeObj (#7377)</li> <li>Protect against IOBE in IndexesLogicImpl.completeIndexesInCommitChain (#7371)</li> <li>Remove unused API v2 java client classes (#7370)</li> <li>Propagate content type from method parameter (#7369)</li> <li>Ninja: changelog</li> <li>Support assign/delete reference without type in Java client (#7348)</li> <li>Remove call to Persist.erase in ImportPersistV1.prepareRepository (#7363)</li> <li>Export secondary parents (#7356)</li> <li>Retry erase repo with non-admin path if DropRowRange fails (#7352)</li> <li>Add Quarkus config option to disable BigTable admin client (#7353)</li> <li>Add support for BigTable in Helm chart (#7345)</li> <li>Fix volume declaration for ROCKSDB storage (#7344)</li> <li>Minor javadoc clarification for ConflictType.UNEXPECTED_HASH (#7333)</li> <li>Remove workaround for Quarkus #35104 (#7315)</li> </ul>"},{"location":"releases-0.69/#0670-release-august-02-2023","title":"0.67.0 Release (August 02, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases-0.69/#upgrade-notes_1","title":"Upgrade notes","text":"<ul> <li>Tracing and metrics have been migrated to Quarkus \u201cnative\u201d. The options to en/disable metrics and tracing have been removed. Please remove the options <code>nessie.version.store.trace.enable</code>, <code>nessie.version.store.metrics.enable</code> from your Nessie settings.</li> </ul>"},{"location":"releases-0.69/#changes_1","title":"Changes","text":"<ul> <li>Nessie API spec upgraded to 2.1.1</li> <li>Support for relative hashes has been standardized and is now allowed in all v2 endpoints</li> <li>Migrate to Quarkus metrics and tracing</li> </ul>"},{"location":"releases-0.69/#commits_4","title":"Commits","text":"<ul> <li>Ninja: CHANGELOG</li> <li>Fix running Spark 3.3+ ITs on Java==17 (#7322)</li> <li>Nit: remove unused import (#7321)</li> <li>Fix NesQuEIT IntelliJ import (#7320)</li> <li>Remove metrics.enable and trace.enable properties in QuarkusEventConfig (#7319)</li> <li>Move to Quarkus provided observability (#6954)</li> <li>Extend relative hash support to whole API v2 (#7308)</li> <li>Add .python-version to .gitignore (#7313)</li> <li>Simplify BigTableBackendConfig.tablePrefix declaration (#7309)</li> </ul>"},{"location":"releases-0.69/#0660-release-july-31-2023","title":"0.66.0 Release (July 31, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases-0.69/#new-features_1","title":"New Features","text":"<ul> <li>New <code>entries</code> command in Content-Generator tool</li> <li>New <code>--all</code> option to the <code>content-refresh</code> Content-Generator tool command</li> <li>Helm chart: add <code>podLabels</code> for Nessie Pods</li> </ul>"},{"location":"releases-0.69/#changes_2","title":"Changes","text":"<ul> <li>Add/fix <code>info</code> section in OpenAPI spec, add templates to <code>servers</code> section</li> </ul>"},{"location":"releases-0.69/#fixes_4","title":"Fixes","text":"<ul> <li>Fix handling of not present and wrong reference-type for create/assign/delete-reference API calls</li> </ul>"},{"location":"releases-0.69/#commits_5","title":"Commits","text":"<ul> <li>Ninja: update CHANGELOG</li> <li>Add new <code>entries</code> command to <code>content-generator</code> (#7296)</li> <li>Make AbstractTestRestApiPersist extend BaseTestNessieRest (#7304)</li> <li>Move TestRelativeCommitSpec to right package (#7306)</li> <li>Add podLabels for Nessie Pods (#7287)</li> <li>Add <code>--all</code> option to the <code>content-refresh</code> command (#7292)</li> <li>Fix examples in openapi.yaml (#7291)</li> <li>Nit: update changelog (#7290)</li> <li>Correct handling of reference-type query parameter (#7282)</li> <li>Fix Quarkus Swagger UI (#7281)</li> <li>Move classes in quarkus-common, no change in functionality (#7275)</li> <li>Nit: Remove superfluous <code>allowDependencies</code> (#7273)</li> <li>Add OpenAPI info/description (#7270)</li> <li>Backport Keycloak and Nessie testcontainers (#7255)</li> </ul>"},{"location":"releases-0.69/#0651-release-july-19-2023","title":"0.65.1 Release (July 19, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases-0.69/#changes_3","title":"Changes","text":"<ul> <li>Add validation of cutoff-definitions in <code>GarbageCollectorConfig</code></li> <li>Fix self-reference in OpenAPI spec</li> <li>Add <code>servers</code> section to OpenAPI spec</li> </ul>"},{"location":"releases-0.69/#commits_6","title":"Commits","text":"<ul> <li>Ninja: fix test failure</li> <li>Nit: update changelog</li> <li>OpenAPI spec: add <code>servers</code> section (#7266)</li> <li>Fix openapi self-reference and type ambiguity (#7264)</li> <li>Validate default-cut-off-policy with gc config/repository APIs (#7265)</li> <li>Allow Java 17 for Spark 3.3+3.4 tests (#7262)</li> <li>Introduce <code>CHANGELOG.md</code>, include in release information (#7243)</li> <li>Introduce <code>StringLogic</code> for persisted strings (#7238)</li> <li>Bump Keycloak to 22.0.0 (#7254)</li> <li>Revert bot-changes in <code>server-upgrade.md</code> (#7244)</li> </ul>"},{"location":"releases-0.69/#0650-release-july-14-2023","title":"0.65.0 Release (July 14, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Revert Gradle 8.2.1 (#7239)</li> <li>Add Nessie as a Source announcement blog from Dremio website (#7236)</li> <li>Add <code>--author</code> option to <code>content-generator</code> commands (#7232)</li> <li>Add repository configuration objects (#7233)</li> <li>Fix retrieval of default branch (#7227)</li> <li>Allow re-adds in same commit (#7225)</li> <li>Allow snapshot versions in dependencies (#7224)</li> <li>IDE: Cleanup Idea excludes (#7223)</li> <li>Spark-tests: disable UI (#7222)</li> <li>Compatibility tests: move to new storage model (#6910)</li> <li>Use testcontainers-bom (#7216)</li> <li>Reference keycloak-admin-client-jakarta (#7215)</li> <li>Post Quarkus 3: Remove no longer needed dependency exclusion (#7214)</li> <li>Bump to Quarkus 3.2.0.Final (#6146)</li> <li>CI: Add some missing <code>--scan</code> Gradle flags (#7210)</li> <li>Update main README after UI sources moved (#7207)</li> <li>Forbid relative hashes in commits, merges and transplants (#7193)</li> <li>Remove misplaced license header (#7203)</li> <li>More diff-tests (#7192)</li> <li>removed extra tab (#7189)</li> <li>Tests: Make <code>ITCassandraBackendFactory</code> less flaky (#7186)</li> <li>IntelliJ: Exclude some more directories from indexing (#7181)</li> </ul>"},{"location":"releases-0.69/#0640-release-july-03-2023","title":"0.64.0 Release (July 03, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Ninja: really fix create-release WF</li> <li>Fix create-release workflow (#7177)</li> <li>Make gc-base, gc-iceberg classes Java 8 compatible (#7174)</li> <li>Nessie-Java-API: Add builder methods for extended merge behaviors (#7175)</li> <li>Spark-SQL-tests: Nessie client configuration + ignore committer (#7166)</li> <li>improve configure-on-demand (#7173)</li> <li>Fix Gradle task dependencies (#7172)</li> <li>Let Immutables discover the right Guava version (#7165)</li> <li>Unify Gradle daemon heap size (#7164)</li> <li>Revert Gradle daemon heap settings (#7160)</li> <li>Nessie Catalog related changes for Spark SQL extensions (#7159)</li> <li>Minor Gradle build scripts addition (noop for <code>main</code>) (#7158)</li> <li>Fully lazy <code>StoreIndexElement</code> deserializion (#7132)</li> <li>CI: attempt to fix snapshot publising (#7151)</li> <li>Revert \u201cUse WorkerThread if parallelism is one.\u201d (#7152)</li> <li>Use WorkerThread if parallelism is one. (#7150)</li> <li>Move <code>python/</code> to separate repo (#7147)</li> <li>Move <code>ui/</code> to separate repo (#7146)</li> <li>Simplify test profiles in AbstractOAuth2Authentication and AbstractBearerAuthentication (#7143)</li> <li>Remove another Deltalake left-over (#7145)</li> </ul>"},{"location":"releases-0.69/#0630-release-june-27-2023","title":"0.63.0 Release (June 27, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Fix inability to delete some references (#7141)</li> <li>Do not add Jandex index to shadow-jars (#7138)</li> <li>Allow table names prefixes for BigTable (#7140)</li> <li>Lazily deserialize <code>StoreIndexElement</code>s <code>content</code> (#7130)</li> <li>StoreIndexImpl: wrong estimated serialized size for empty index (#7128)</li> <li>Events/Quarkus: Do not cache huge artifacts in Gradle cache (#7118)</li> </ul>"},{"location":"releases-0.69/#0621-release-june-23-2023","title":"0.62.1 Release (June 23, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Emergency-fix of #7088 (#7117)</li> <li>Remove another occurence of Delta Lake (#7113)</li> </ul>"},{"location":"releases-0.69/#0620-release-june-23-2023","title":"0.62.0 Release (June 23, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Identify merge-base instead of common-ancestor (#7035)</li> <li>Remove code for and mentions of Delta Lake (#7108)</li> <li>Fix reference-recovery-logic (#7100)</li> <li>Fix Windows CI (#7107)</li> <li>CI: Fix newer-java CI (#7106)</li> <li>Enhance references with created-timestamp + extended-info-obj-pointer (#7088)</li> <li>BigTable follow-up (#7102)</li> <li>Refactor KeycloakTestResourceLifecycleManager (#7101)</li> <li>Tests: Only pass the surrounding test class to <code>AbstractReferenceLogicTests</code> (#7099)</li> <li>Ref-indexes: supply <code>int/refs</code> HEAD commit ID to check in reference-r\u2026 (#7095)</li> <li>Rocks: simplify <code>checkReference</code> (#7096)</li> <li>Inmemory: slightly change the update-ref function (#7094)</li> <li>Mongo: unify condition handling (#7093)</li> <li>Dynamo: simplify condition handling (#7092)</li> <li>Tests: Add logging to a bunch of modules (#7086)</li> <li>Add some convenience functionality to <code>Reference</code> (#7089)</li> <li>Nit: <code>ImportPersistV1/2</code> just a static import (#7091)</li> <li>Just test refactorings, no functional change (#7090)</li> <li>Site: Add database status section (#7085)</li> <li>Store indexes: some new tests, some test/bench optimizations (#7066)</li> <li>Testing: Remove unnecessary <code>ITCockroachDBCachingPersist</code> (#7087)</li> <li>Fix Newer-Java + Mac/Win CI (#7058)</li> <li>Micro-optimization of <code>LazyIndexImpl.get()</code> (#7059)</li> <li>Bump Scala 2.12/2.13 patch versions (#7060)</li> <li>CommitLogic: add <code>fetchCommits</code> for 2 IDs (#7055)</li> <li>Enable token exchange flow in authn docker-compose example (#7057)</li> <li>Make number of access checks configurable (#7056)</li> <li>Fix nit in <code>TypeMapping</code> (#7054)</li> <li>UI: Properly \u201cwire\u201d <code>compileAll</code> + <code>checkstyle</code> helper tasks (#7042)</li> <li>Add BigTable as a version-store type (#6846)</li> <li><code>nessie-rest-services</code> back to 8 (#7049)</li> <li>Fetch names references: replace <code>fetchReference</code>-per-ref w/ bulk-fetch (#7046)</li> <li>Extract <code>CommitMetaUpdater</code> class, fix \u201cset authors on merge\u201d (#7039)</li> <li>New storage/references logic: prevent one database read (#7045)</li> <li>More enhancements to docker-compose files (#7047)</li> <li>CI: Do not trigger on push to <code>feature/**</code> branches (#7044)</li> <li>CI: Split \u201cCode Checks\u201d job\u2019s main step (#7041)</li> <li>Minor fixes to Keycloak docs and docker-compose file (#7036)</li> <li><code>BatchAccessChecker</code>: distinguish <code>UPDATE</code> and <code>CREATE</code> for commited values (#7028)</li> <li>Build/nit: simplify version catalog usage (#7032)</li> <li>CI: Give \u201cNewer Java\u201d runs more memory (#7031)</li> <li>CI: Support <code>feature/**</code> branches (#7030)</li> </ul>"},{"location":"releases-0.69/#0610-release-june-13-2023","title":"0.61.0 Release (June 13, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Fix Backends.createPersist() (#7015)</li> <li>Make Docker image + tag for Minio configurable (#7023)</li> <li>Nit: Make <code>ITSparkIcebergNessieS3</code> safe against empty buckets (#7025)</li> <li>Ninja: make CI pass again, fix <code>ITSparkIcebergNessieS3</code></li> <li>Add four new Nessie animations for chats (#7008)</li> <li>Nit: remove unused method (#7007)</li> <li>Micro-opt: collections with expected size (#7006)</li> <li>Nit: remove unnecessary type arguments (#7005)</li> <li>Micro-opt <code>TypeMapping</code> (#7004)</li> <li>Fix <code>:nessie-services-bench</code> (#7003)</li> <li>Move <code>StreamingUtil</code> to its only call site (#6999)</li> <li>Remove AWS lambda mentions from README and docs (#6997)</li> <li>Database adapters - back to Java 8 (#6995)</li> <li>Docs: Update Trino version (#6991)</li> <li>DynamoDB: let Quarkus use the Apache HTTP client (#6994)</li> <li>Export zip tempfile (#6983)</li> <li>Build/nit: missing annotation dependency (#6992)</li> <li>Build: allow javac \u2018release\u2019 option, server modules built for Java 11 (#6730)</li> <li>Build/nit: properly prevent missing compiler annotation warnings (#6988)</li> <li>Do not initialize new model repository for CLI tool (#6985)</li> <li>Faster export w/ new data model (#6984)</li> <li>Make the TLS guide compatible with macOS (#6959)</li> <li>Fix soft-merge conflict of #6952 and #6975 (#6987)</li> <li>Add some version-store microbenchmarks (#6952)</li> <li>Serialize amount of entries of store-index (#6976)</li> <li>Micro-optimizations in ContentKey + TypeMapping (#6975)</li> <li>Emit access-checks for merge and transplant (#6949)</li> <li>Add <code>VersionStore.KeyRestrictions</code> parameter bag (#6951)</li> <li>Export: fix NPE when only the ZIP file name is supplied (#6982)</li> <li>Remove <code>ITScyllaDBBackendFactory</code> (#6969)</li> <li>Fix Spark 3.4 dependency nit (#6972)</li> <li>Throw when v2 requested but v1 provided (#6958)</li> <li>Quarkus tests OOM (again) (#6955)</li> <li>Let new-storage DynamoDb use Apache Http Client (#6950)</li> <li>Quarkus events tests - proper commits (#6953)</li> <li>Events notification system for Nessie - Quarkus (#6870)</li> <li>Identify relative commit + commit by timestamp (#6932)</li> <li>Enhancements to the Events API (#6945)</li> <li>Use parameter objects for <code>VersionStore.merge()</code> + <code>.transplant()</code> (#6944)</li> <li>Add SQL extension for Spark 3.4 (#6822)</li> <li>Add convenience <code>Content.withId(String)</code> (#6937)</li> <li>Add user guide for TLS ingress (#6861)</li> <li>Doc for <code>NessieConfiguration.specVersion</code> (#6938)</li> <li>Bump Spark to 3.2.4 + 3.3.2 (#6916)</li> <li>Fix possible IntelliJ dependency issue (#6918)</li> <li>Remove Gatling runs in CI (#6912)</li> <li>Build-tools-tests: switch tests to new storage model (#6913)</li> <li>Build: prevent duplicate checkstyle task runs (#6915)</li> <li>Content generator: test against new storage model (#6907)</li> <li>Iceberg-views: tests against new storage model (#6908)</li> <li>Events-SPI: doc update for new storage model config option (#6906)</li> <li>GC: Test against new storage model (#6905)</li> <li>jaxrs-testextension: test against new storage model (#6904)</li> <li>Nit: Quarkus-tests: use new storage model code (#6903)</li> <li>Quarkus-tests: Update test resources to use new storage test code (#6902)</li> <li>Mongo/nit: simplify <code>MongoClientProducer</code> (#6901)</li> <li>Remove Jackson support in Events API (#6899)</li> <li>Configure MeterRegistry and MeterFilter globally (#6900)</li> </ul>"},{"location":"releases-0.69/#0601-release-may-25-2023","title":"0.60.1 Release (May 25, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Release-publish: fix after removed native images (#6898)</li> </ul>"},{"location":"releases-0.69/#0600-release-may-25-2023","title":"0.60.0 Release (May 25, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Pass content-IDs of all <code>ContentKey</code> elements to <code>Check</code>s (#6859)</li> <li>Nit: Spec version updates (#6897)</li> <li>Fix <code>MergeKeyBehavior.DROP</code> in new storage model for merge (#6894)</li> <li>Defer API compatibility check until first request (#6893)</li> <li>Remove mentions of spark2 in Docs (#6762)</li> <li>Revert \u201cBump keykloak container image from 21.0.2 to 21.1 (#6887)\u201d (#6895)</li> <li>Table renames: allow delete-op after put-op (#6892)</li> <li>Bump keykloak container image from 21.0.2 to 21.1 (#6887)</li> <li>Update dependency ch.qos.logback:logback-classic to v1.2.12 (#6889)</li> <li>Bugfix: include ROCKSDB in selector to create PVC in both cases. (#6881)</li> <li>Build: cleanup <code>libs.versions.toml</code> (#6886)</li> <li>Don\u2019t use filters when checking API compatibility (#6877)</li> <li>Make nip.io usage resilient against lookup failures (#6885)</li> <li>CI/Nesqueit: Switch back to <code>iceberg-nesqueit</code> branch (#6883)</li> <li>Ability to disable the API compatibility check via system properties (#6875)</li> <li>Skip API compatibility check if /config endpoint fails (#6878)</li> <li>Fix snapshot publising after #6847 (#6882)</li> <li>Build: minor <code>baselibs.versions.toml</code> update (#6884)</li> <li>Improve NessieError message (#6874)</li> <li>Disable Scylla tests on macOS (#6871)</li> <li>Enable compatibility tests on macOS (#6857)</li> <li>Use nip.io domain in MinioExtension (#6856)</li> <li>Docker compose template for Nessie + OpenTelemetry (#6860)</li> <li>Release: Remove relocation-poms (#6810)</li> <li>Remove native image (#6847)</li> <li>Use API V2 by default in GC Tool (#6858)</li> <li>Enables the extended information in <code>NessieConfiguration</code> (#6640)</li> <li>Add missing jakarta annotations (#6850)</li> <li>Nit: Remove unneeded dev-profile hints (#6851)</li> <li>Quarkus ITs: Restrict Keycloak to tests using Keycloak (#6852)</li> </ul>"},{"location":"releases-0.69/#0590-release-may-18-2023","title":"0.59.0 Release (May 18, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Add an API compatibility check to Nessie clients (#6818)</li> <li>Remove Content type hierarchy from Events API (#6840)</li> <li>Mark REST API v2 as GA (#6749)</li> <li>REST V2: Key ranges for entries + diff (#6743)</li> <li>Add ObjId.objIdFromByteBuffer() (#6845)</li> <li>Wikis / REST API v2 changes (#6801)</li> <li>Allow exporting only the latest contents from a branch. (#6823)</li> <li>Events notification system for Nessie - Service module (#6760)</li> <li>Remove the lambda module (#6819)</li> <li>increase severity of multiple errorprone checks (#6800)</li> <li>Tests: Add utility check for new storage model (#6814)</li> <li>Nit: let export/new-model not use a deprecated function (#6813)</li> <li>Release: no relocation-pom for events (#6809)</li> <li>SQL Extensions: Fix handling of quoted reference names (#6804)</li> <li>Update site with new storage config (#6795)</li> <li>UI: Handle reference names with <code>/</code> (#6806)</li> <li>set ClassCanBeStatic severity to ERROR (#6797)</li> <li>Some errorprone mitigations (no functional change) (#6796)</li> <li>Make EventSubscriber.onSubscribe not default (#6777)</li> <li>Make Reference.getFullName return Optional (#6776) <li>Add new content type UDF (#6761)</li> <li>Add missing test for #6758 (#6768)</li> <li>Events: align field names to model (#6771)</li> <li>Blog: Fix rendering of namespace creation option (#6778)</li> <li>Blog: Update about namespace enforcement (#6753)</li> <li>Add Alex to dev list (#6775)</li> <li>New storage: allow deletion of multiple repositories (#6758)</li> <li>Fix left-over TODO from content-metadata PR (#6759)</li> <li>Events notification system for Nessie - VersionStore changes (#6647)</li> <li>Nessie error details: Add <code>ContentKey</code> to related errors (#6721)</li> <li>Minor optimization when retrieving named referencs (#6745)</li> <li>Cleanup V2 MergeResponse (#6747)</li> <li>GC: Trim long failure messages in JDBC repo (#6746)</li> <li>Nit: remove no longer valid TODO (#6744)</li> <li>Add extraEnv to helm chart (#6698)</li> <li>Events notification system for Nessie - SPI module (#6733)</li> <li>CI/main: Run CI on main sequentially (#6741)</li> <li>CI: Run \u201cforgotten\u201d java 8 tests (#6738)</li> <li>Bump json5 to 2.2.3 (#6739)</li> <li>Revert \u201cfix(deps): update mockito monorepo to v5 (major) (#6731)\u201d (#6737)</li> <li>Reserve usage of BatchingPersist to dry-run mode only (#6736)</li> <li>UI: Update a bunch of JS dependencies (#6734)</li> <li>Nit: Remove unnecessary <code>JdkSpecifics</code> (#6729)</li> <li>Events notification system for Nessie - API module (#6646)</li> <li>Build/CI: pass <code>test.log.level</code> via <code>CommandLineArgumentProvider</code> and populate Quarkus console log level (#6725)</li> <li>Remove top-level condition from the <code>CI Website</code> job (#6728)</li> <li>CI: add <code>build/quarkus.log</code> to failed quarkus jobs artifacts (#6727)</li> <li>Do not store intermediate commits during merge/transplant (#6677)</li> <li>build: <code>buildSrc</code> using Java toolchain (#6726)</li> <li>Cassandra: add timeouts for DDL + DML (#6716)</li> <li>Keycloak requires container-network (#6719)</li> <li>CI: update helm-chart-testing (#6720)</li> <li>Nit: remove unused version definition (#6718)</li> <li>CI: capture test reports (#6717)</li> <li>Let <code>:nessie-versioned-spi</code> use OpenTelemetry (#6687)</li> <li>Minor delta test fix (#6699)</li> <li>Add Nessie spec definition for 2.0.0-beta.1 (#6679)</li> <li>Change default message for (squash) merges (#6642)</li> <li>Add \u201cfallback cases\u201d for relevant enums in <code>:nessie-model</code> (#6634)</li> <li>Make dry run merge / transplant throw exceptions (#6685)</li> <li>Rename <code>ConflictResolution.IGNORE</code> -&gt; <code>ADD</code> (#6686)</li> <li>Add more commit-attributes to <code>Merge</code> (#6641)</li> <li>Implement \u201cexternal\u201d conflict resolution for merges (#6631)</li> <li>Ability to pass advanced config as nested YAML (#6684)</li> <li>Remove unnecessary annotations from <code>ITImplicitNamespaces</code> (#6678)</li> <li>Ensure custom content-types work (#6618)</li> <li>Nessie: Generic information for operations and content results (#6616)</li> <li>Events design doc - minor evolutions (#6672)</li> <li>bugfix: namespaceDepth filter loses entry content (#6648)</li> <li>Expose Nessie repository information (preparation) (#6635)</li> <li>Allow all commit attributes for namespace operations (#6643)</li> <li>Mark <code>namedRef</code> as <code>@Nullable</code> in <code>ContentService</code> (#6638)</li> <li>Allow Nessie clients to deserialize unknown <code>Content.Type</code>s (#6633)</li> <li>Prepare REST API for content-aware merges (#6619)</li>"},{"location":"releases-0.69/#0581-release-april-19-2023","title":"0.58.1 Release (April 19, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Null out references to java.net.http.HttpClient (#6630)</li> <li>Validate that the generated <code>nessie-gc</code> executable works (#6625)</li> <li>Cleanup content-type code (#6617)</li> <li>Add a test for getContent() on the default branch (API v1) (#6623)</li> <li>Tests for the OAuth2 authentication provider in nessie-quarkus (#6597)</li> <li>Add test for specVersion in API v2 (#6599)</li> <li>Disable IT-Auth on WIn/Mac CI (#6615)</li> <li>Docs for the new OAuth2 authentication provider (#6595)</li> </ul>"},{"location":"releases-0.69/#0580-release-april-15-2023","title":"0.58.0 Release (April 15, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Renovate: let more dependency trigger Quarkus/native CI (#6596)</li> <li>OAuth2 authentication provider for Nessie client (#6527)</li> <li>Design document for Nessie events notification system (#6482)</li> <li>JAX-RS: Properly \u201cpass through\u201d <code>WebApplicationException</code> (#6584)</li> <li>CI: Make Quarkus/native job not a matrix job (#6589)</li> <li>Quarkus: Fix HTTP compression parameters (#6581)</li> <li>Remove direct use of hamcrest (#6583)</li> <li>CI: Replace auto-labeler with changed-files checks (#6577)</li> <li>CI: Use Java 20 in \u201cnewer-java\u201d (#6582)</li> <li>Update Helm template for new storage implementation. (#6580)</li> <li>Gradle: eliminate some more <code>doFirst</code>/<code>doLast</code> script references (#6516)</li> <li>CI: Remove <code>CI Success</code> job (#6572)</li> <li>Align Quarkus dependencies (#6565)</li> <li>CI: Skip Helm CI, if version is not available (fix) (#6559)</li> <li>Revert \u201cUpdate smartbear/swaggerhub-cli action to v0.7.1 (#6519)\u201d (#6556)</li> <li>Release-WF: Disable the Gradle cache (#6555)</li> </ul>"},{"location":"releases-0.69/#0570-release-april-11-2023","title":"0.57.0 Release (April 11, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>CI: adjust Gradle cache storage parameters for non-<code>ci.yml</code> workflows (#6521)</li> <li>Build: use Smallrye-OpenAPI-Plugin from Gradle plugins (#6525)</li> <li>Perftest: Fix commit-to-branch simulation (#6552)</li> <li>Support <code>@NessieServerProperty</code> annotations in nested tests (#6548)</li> <li>Remove unused type parameter in Backends.createPersist (#6554)</li> <li>CI: Skip Helm CI, if version is not available (#6546)</li> <li>Fix wrong jackson-core coordinates (#6553)</li> <li>CI: Un-bump latest Java from 20 to 19 (#359) (#6551)</li> <li>CI-Workflow updates (#6481)</li> <li>CI: finally the correct artifacts (#6520)</li> <li>README: Fix deep-links into CI jobs (#6522)</li> <li>Bump nodejs to 18.15.0 LTS + npm to 9.5.0 LTS (#6542)</li> <li>Merge: Verify that there is at least one source commit (#6514)</li> <li>Gradle: minor non-changes for configuration-cache (#6513)</li> <li>CI: Cache everything that\u2019s needed (#6515)</li> <li>Fix <code>nessie-gc</code> publication (#6511)</li> <li>Fix specVersion to comply with semver syntax (#6506)</li> </ul>"},{"location":"releases-0.69/#0560-release-april-05-2023","title":"0.56.0 Release (April 05, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>CI: Save 15m in CI NesQuEIT (#6505)</li> <li>Populate reference conflict details (#6503)</li> <li>Enhance <code>NessieError</code> with error details (#6492)</li> <li>Revert slf4j2 bump (#6504)</li> <li>Revert \u201cUpdate smartbear/swaggerhub-cli action to v0.7.0 (#6478)\u201d (#6500)</li> <li>Test code cleanup / no functional change (#6502)</li> <li>Add <code>@JsonView</code> to HttpConfigApi v2 (#6498)</li> <li>Fix CI badge on README.md (#6499)</li> </ul>"},{"location":"releases-0.69/#0550-release-april-04-2023","title":"0.55.0 Release (April 04, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Add min-version + spec-version-bag to <code>NessieConfiguration</code> (#6480)</li> <li>IntelliJ: Exclude build directories in build-tools-integration-tests (#6489)</li> <li>Support running compatibility tests with the new data model. (#6484)</li> <li>Test code: Refactor <code>Json</code> to <code>JsonNode</code> in tests (#6487)</li> <li>Bump jaxb-impl from hadoop-common (#6485)</li> <li>Let errorprone not check code generated by APTs (#6486)</li> <li>Bring back logback logging (#6469)</li> <li>Remove concurrent test case execution per class (#6479)</li> <li>WF: No more need to schedule contaner-registry sync (#6454)</li> <li>Refactor CI workflow - bring back reasonable CI runtime (#6461)</li> <li>Do not let Weld register a shutdown hook (#6470)</li> <li>Skip empty commits during merge/transplant (#6468)</li> <li>Simplify BaseCommitHelper.mergeSquashFastForward() (#6467)</li> <li>Nit: remove unused version decl (#6466)</li> <li>Fix validation error messages in CommitImpl (#6450)</li> <li>Add a convenience <code>compileAll</code> Gradle task (#6456)</li> <li>Fix nessie-perftest-simulations standalone (#6453)</li> <li>IntelliJ: Let IntelliJ test-runner default to CHOOSE_PER_TEST again (#6452)</li> </ul>"},{"location":"releases-0.69/#0540-release-march-30-2023","title":"0.54.0 Release (March 30, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Scylla: 8-&gt;4G (#6446)</li> <li>Remove the need to pass <code>Put.expectedContent()</code> (#6438)</li> <li>Tests: Faster ScyllaDB container startup (#6444)</li> <li>IntelliJ: Always use Gradle to run tests (#6445)</li> <li>Testing: Faster C* container startup (#6443)</li> <li>Re-add Cassandra version store type to Nessie-Quarkus (#6440)</li> <li>Allow CI on Nessie forks (#6442)</li> <li>Cleanup/build: Remove no longer used Gradle configuration (#6437)</li> <li>Add missing \u201cproject only\u201d code-coverage-report tasks (#6436)</li> <li>Fix OOM when running tests <code>:nessie-cli</code> (#6435)</li> <li>Some minor Gradle build optimizations (#6428)</li> <li>Testing: add <code>-XX:+HeapDumpOnOutOfMemoryError</code> (#6433)</li> <li>Nit: fix IntelliJ import warning for nessie-perftest-simulations (#6431)</li> <li>Disable testcontainers startup checks (#6423)</li> <li>Remove old content-attachments approach (#6422)</li> <li>Expand iceberg-views version-ID to 64 bit (#6421)</li> <li>Update Nessie client version as per Iceberg 1.2.0 release (#6412)</li> <li>Site: Update Iceberg, Flink, Presto versions (#6419)</li> <li>Gradle CI tweaks (#6416)</li> <li>Bump maven to 3.9.1 + maven-resolver to 1.9.7 (#6413)</li> <li>Nit: suppress \u201cunclosed resource\u201d warning in some tests (#6408)</li> <li>UI-Build: Use openapi-generator jar directly (#6411)</li> <li>Nit: remove unnecessary <code>.collect()</code> (#6414)</li> <li>Nit: Suppress a bunch of unchecked and deprecated warnings (#6410)</li> <li>Fix nullability for fields of ContentValueObj (#6409)</li> <li>Remove deprecated and unused <code>@NessieUri</code> annotation (#6407)</li> <li>Tests: Update Quarkus\u2019 TestNessieError (#6399)</li> <li>Remove exception mapping for <code>java.security.AccessControlException</code> (#6405)</li> <li>Nit: remove unused code from #6384 (#6406)</li> </ul>"},{"location":"releases-0.69/#0531-release-march-24-2023","title":"0.53.1 Release (March 24, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>New storage: add \u201cbatch write\u201d facade (#6385)</li> <li>Persist: replace <code>updateObj()</code> with <code>upsertObj()</code> (#6384)</li> <li>Import: print total duration (#6383)</li> <li>IntelliJ: exclude more directories from indexing (#6374)</li> <li>Import: Eager prefetching of commits during finalization (#6378)</li> <li>New storage: <code>Persist.updateObj()</code> must respect index-size limits (#6377)</li> <li>Import: use updated commit as new parent (#6376)</li> <li>Nessie Import: Respect commit import batch size (new model) (#6375)</li> <li>Nessie Import: Print durations, double row size printed to console (#6370)</li> </ul>"},{"location":"releases-0.69/#0530-release-march-23-2023","title":"0.53.0 Release (March 23, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Import: paint dots also during the finalize phase (#6366)</li> <li>Use relocated protobuf runtime (#6355)</li> <li>Build/publishing: Proper Gradle module metadata for shadow-jars (#6361)</li> <li>Intellij: exclude <code>/.mvn</code> + <code>/ui/node_modules</code> (#6364)</li> <li>Support annotation-based configuration in \u201cold server\u201d tests (#6356)</li> <li>Mongo/storage: use <code>replaceOne</code> instead of <code>findAndReplaceOne</code> (#6363)</li> <li>Compatibility-test: allow adapter-configuration via system properties (#6353)</li> <li>Compatibility-tests: require Nessie &gt;= 0.42.0, cleanup code (#6352)</li> <li>CI: Bump newer-java-version WF from 19 to 20 (#6348)</li> <li>Properly return <code>ParamConverterProvider</code> errors as HTTP/400 (#6346)</li> <li>Remove InstantParamConverterProvider (#6341)</li> <li>Update release notes template, remove \u201cDocker Hub\u201d (#6339)</li> <li>Build: Fix Jackson-annotations configuration for build-plugins int-test (#6337)</li> <li>Nit: Remove unnecessary null-check (#6344)</li> </ul>"},{"location":"releases-0.69/#0523-release-march-20-2023","title":"0.52.3 Release (March 20, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Ninja: fix release WF</li> <li>Ninja: fix release WF</li> <li>Fix release WF (#6334)</li> <li>New storage: fix writetamp collision in C* integration tests (#6332)</li> <li>Use positive repo erase confirmation codes. (#6317)</li> <li>Forbid special ASCII chars in content keys (#6313)</li> <li>GH WF: Remove <code>concurrency</code> from PR jobs (#6314)</li> <li>Replace Docker Hub with ghcr.io (#6305)</li> <li>Fix param examples for getSeveralContents() (#6302)</li> <li>Fix labeler config (#6303)</li> <li>Workaround UI issues in ghcr.io + quay.io (#6299)</li> <li>Fix docker-sync WF typo (#6296)</li> <li>GH WF to sync container repositories (#6295)</li> <li>Content-tool: Add command to create missing namespaces (#6280)</li> <li>Ensure parent namespace(s) for commited content objects exist (#6246)</li> <li>Core reorg: use <code>api/</code> and <code>integrations/</code> directories (#6288)</li> <li>Add ACKNOWLEDGEMENTS.md (#6289)</li> <li>Allow some integration tests to be executed on macOS (#6266)</li> <li>Build/CI: Allow renovate to update the Maven wrapper (#6286)</li> <li>Update the Docker image section of main README.md (#6267)</li> <li>Ignore jEnv .java-version file (#6268)</li> <li>Use JvmTestSuite + JacocoReport (#6231)</li> <li>Content-generator tool changes for \u2018Ensure parent namespace(s) for commited content objects exist\u2019 PR (#6265)</li> <li>Test changes for \u2018Ensure parent namespace(s) for commited content objects exist\u2019 PR (#6263)</li> <li>CLI tool cheanges for \u2018Ensure parent namespace(s) for commited content objects exist\u2019 PR (#6264)</li> <li>compatiblity-tests: Extracted test changes for namespace-validation (#6261)</li> <li>Fix maven group id in docs (#6259)</li> <li>Add helper functions to <code>Namespace</code> and <code>ContentKey()</code> (#6256)</li> <li>Let ITs against Nessie/Quarkus send back stack traces (#6257)</li> <li>Replace <code>Key</code> with <code>ContentKey</code> (#6242)</li> <li>New DynamoDB storage - do not pull in the Apache HTTP client (#6243)</li> <li>Add missing Gradle build scan for last check (#6254)</li> <li>Fix <code>AbstractDatabaseAdapter.removeKeyCollisionsForNamespaces</code> (#6253)</li> <li>Fix wrong parameter validation override (#6241)</li> <li>Nit: remove mentions of dependabot (#6239)</li> <li>New Nessie storage model (#5641)</li> </ul>"},{"location":"releases-0.69/#0522-release-march-20-2023","title":"0.52.2 Release (March 20, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Ninja: fix release WF</li> <li>Fix release WF (#6334)</li> <li>New storage: fix writetamp collision in C* integration tests (#6332)</li> <li>Use positive repo erase confirmation codes. (#6317)</li> <li>Forbid special ASCII chars in content keys (#6313)</li> <li>GH WF: Remove <code>concurrency</code> from PR jobs (#6314)</li> <li>Replace Docker Hub with ghcr.io (#6305)</li> <li>Fix param examples for getSeveralContents() (#6302)</li> <li>Fix labeler config (#6303)</li> <li>Workaround UI issues in ghcr.io + quay.io (#6299)</li> <li>Fix docker-sync WF typo (#6296)</li> <li>GH WF to sync container repositories (#6295)</li> <li>Content-tool: Add command to create missing namespaces (#6280)</li> <li>Ensure parent namespace(s) for commited content objects exist (#6246)</li> <li>Core reorg: use <code>api/</code> and <code>integrations/</code> directories (#6288)</li> <li>Add ACKNOWLEDGEMENTS.md (#6289)</li> <li>Allow some integration tests to be executed on macOS (#6266)</li> <li>Build/CI: Allow renovate to update the Maven wrapper (#6286)</li> <li>Update the Docker image section of main README.md (#6267)</li> <li>Ignore jEnv .java-version file (#6268)</li> <li>Use JvmTestSuite + JacocoReport (#6231)</li> <li>Content-generator tool changes for \u2018Ensure parent namespace(s) for commited content objects exist\u2019 PR (#6265)</li> <li>Test changes for \u2018Ensure parent namespace(s) for commited content objects exist\u2019 PR (#6263)</li> <li>CLI tool cheanges for \u2018Ensure parent namespace(s) for commited content objects exist\u2019 PR (#6264)</li> <li>compatiblity-tests: Extracted test changes for namespace-validation (#6261)</li> <li>Fix maven group id in docs (#6259)</li> <li>Add helper functions to <code>Namespace</code> and <code>ContentKey()</code> (#6256)</li> <li>Let ITs against Nessie/Quarkus send back stack traces (#6257)</li> <li>Replace <code>Key</code> with <code>ContentKey</code> (#6242)</li> <li>New DynamoDB storage - do not pull in the Apache HTTP client (#6243)</li> <li>Add missing Gradle build scan for last check (#6254)</li> <li>Fix <code>AbstractDatabaseAdapter.removeKeyCollisionsForNamespaces</code> (#6253)</li> <li>Fix wrong parameter validation override (#6241)</li> <li>Nit: remove mentions of dependabot (#6239)</li> <li>New Nessie storage model (#5641)</li> </ul>"},{"location":"releases-0.69/#0521-release-march-20-2023","title":"0.52.1 Release (March 20, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Fix release WF (#6334)</li> <li>New storage: fix writetamp collision in C* integration tests (#6332)</li> <li>Use positive repo erase confirmation codes. (#6317)</li> <li>Forbid special ASCII chars in content keys (#6313)</li> <li>GH WF: Remove <code>concurrency</code> from PR jobs (#6314)</li> <li>Replace Docker Hub with ghcr.io (#6305)</li> <li>Fix param examples for getSeveralContents() (#6302)</li> <li>Fix labeler config (#6303)</li> <li>Workaround UI issues in ghcr.io + quay.io (#6299)</li> <li>Fix docker-sync WF typo (#6296)</li> <li>GH WF to sync container repositories (#6295)</li> <li>Content-tool: Add command to create missing namespaces (#6280)</li> <li>Ensure parent namespace(s) for commited content objects exist (#6246)</li> <li>Core reorg: use <code>api/</code> and <code>integrations/</code> directories (#6288)</li> <li>Add ACKNOWLEDGEMENTS.md (#6289)</li> <li>Allow some integration tests to be executed on macOS (#6266)</li> <li>Build/CI: Allow renovate to update the Maven wrapper (#6286)</li> <li>Update the Docker image section of main README.md (#6267)</li> <li>Ignore jEnv .java-version file (#6268)</li> <li>Use JvmTestSuite + JacocoReport (#6231)</li> <li>Content-generator tool changes for \u2018Ensure parent namespace(s) for commited content objects exist\u2019 PR (#6265)</li> <li>Test changes for \u2018Ensure parent namespace(s) for commited content objects exist\u2019 PR (#6263)</li> <li>CLI tool cheanges for \u2018Ensure parent namespace(s) for commited content objects exist\u2019 PR (#6264)</li> <li>compatiblity-tests: Extracted test changes for namespace-validation (#6261)</li> <li>Fix maven group id in docs (#6259)</li> <li>Add helper functions to <code>Namespace</code> and <code>ContentKey()</code> (#6256)</li> <li>Let ITs against Nessie/Quarkus send back stack traces (#6257)</li> <li>Replace <code>Key</code> with <code>ContentKey</code> (#6242)</li> <li>New DynamoDB storage - do not pull in the Apache HTTP client (#6243)</li> <li>Add missing Gradle build scan for last check (#6254)</li> <li>Fix <code>AbstractDatabaseAdapter.removeKeyCollisionsForNamespaces</code> (#6253)</li> <li>Fix wrong parameter validation override (#6241)</li> <li>Nit: remove mentions of dependabot (#6239)</li> <li>New Nessie storage model (#5641)</li> </ul>"},{"location":"releases-0.69/#0520-release-march-20-2023","title":"0.52.0 Release (March 20, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>New storage: fix writetamp collision in C* integration tests (#6332)</li> <li>Use positive repo erase confirmation codes. (#6317)</li> <li>Forbid special ASCII chars in content keys (#6313)</li> <li>GH WF: Remove <code>concurrency</code> from PR jobs (#6314)</li> <li>Replace Docker Hub with ghcr.io (#6305)</li> <li>Fix param examples for getSeveralContents() (#6302)</li> <li>Fix labeler config (#6303)</li> <li>Workaround UI issues in ghcr.io + quay.io (#6299)</li> <li>Fix docker-sync WF typo (#6296)</li> <li>GH WF to sync container repositories (#6295)</li> <li>Content-tool: Add command to create missing namespaces (#6280)</li> <li>Ensure parent namespace(s) for commited content objects exist (#6246)</li> <li>Core reorg: use <code>api/</code> and <code>integrations/</code> directories (#6288)</li> <li>Add ACKNOWLEDGEMENTS.md (#6289)</li> <li>Allow some integration tests to be executed on macOS (#6266)</li> <li>Build/CI: Allow renovate to update the Maven wrapper (#6286)</li> <li>Update the Docker image section of main README.md (#6267)</li> <li>Ignore jEnv .java-version file (#6268)</li> <li>Use JvmTestSuite + JacocoReport (#6231)</li> <li>Content-generator tool changes for \u2018Ensure parent namespace(s) for commited content objects exist\u2019 PR (#6265)</li> <li>Test changes for \u2018Ensure parent namespace(s) for commited content objects exist\u2019 PR (#6263)</li> <li>CLI tool cheanges for \u2018Ensure parent namespace(s) for commited content objects exist\u2019 PR (#6264)</li> <li>compatiblity-tests: Extracted test changes for namespace-validation (#6261)</li> <li>Fix maven group id in docs (#6259)</li> <li>Add helper functions to <code>Namespace</code> and <code>ContentKey()</code> (#6256)</li> <li>Let ITs against Nessie/Quarkus send back stack traces (#6257)</li> <li>Replace <code>Key</code> with <code>ContentKey</code> (#6242)</li> <li>New DynamoDB storage - do not pull in the Apache HTTP client (#6243)</li> <li>Add missing Gradle build scan for last check (#6254)</li> <li>Fix <code>AbstractDatabaseAdapter.removeKeyCollisionsForNamespaces</code> (#6253)</li> <li>Fix wrong parameter validation override (#6241)</li> <li>Nit: remove mentions of dependabot (#6239)</li> <li>New Nessie storage model (#5641)</li> </ul>"},{"location":"releases-0.69/#0511-release-march-07-2023","title":"0.51.1 Release (March 07, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>DOCS PR for Maven group-ID refactoring (#6207)</li> <li>Properly sign pom-relocations (#6236)</li> <li>Allow tests to customize Nessie REST API URI resolution. (#6234)</li> <li>build/nit: Remove superfluous settings (#6232)</li> <li>Add SwaggerHub badge to README (#6233)</li> <li>Make V2 ref path-param parsing independent of <code>Reference</code> (#6224)</li> <li>Add tests to validate that API v1 responses do not have v2 attributes. (#6222)</li> <li>Fix codecov-n-builds (#6230)</li> <li>V2 REST declares wrong response types in OpenAPI spec (#6211)</li> <li>Fix native images after recent rocksdb version bump (#6216)</li> <li>Symlink gradle-wrapper.jar, unignore gradle-wrapper.jar (#6219)</li> <li>upgrade GH WFs to ubuntu-22.04 (#6206)</li> <li>Maven group-ID refactoring (#6197)</li> <li>Fix: <code>Content.Type</code> is a string (#6202)</li> <li>Git ignore <code>__pycache__</code> (#6201)</li> <li>Compatibility tests code cleanup (#6198)</li> <li>Site: Update community page with updated chat information (#6192)</li> <li>Move Nessie speecific build code into this build (#6196)</li> <li>Nit: javadoc (#6193)</li> <li>Gradle 8 adoption for NesQuEIT (#6166)</li> <li>mac-CI: Workaround for brew-upgrade issue (#6184)</li> </ul>"},{"location":"releases-0.69/#0510-release-march-06-2023","title":"0.51.0 Release (March 06, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Allow tests to customize Nessie REST API URI resolution. (#6234)</li> <li>build/nit: Remove superfluous settings (#6232)</li> <li>Add SwaggerHub badge to README (#6233)</li> <li>Make V2 ref path-param parsing independent of <code>Reference</code> (#6224)</li> <li>Add tests to validate that API v1 responses do not have v2 attributes. (#6222)</li> <li>Fix codecov-n-builds (#6230)</li> <li>V2 REST declares wrong response types in OpenAPI spec (#6211)</li> <li>Fix native images after recent rocksdb version bump (#6216)</li> <li>Symlink gradle-wrapper.jar, unignore gradle-wrapper.jar (#6219)</li> <li>upgrade GH WFs to ubuntu-22.04 (#6206)</li> <li>Maven group-ID refactoring (#6197)</li> <li>Fix: <code>Content.Type</code> is a string (#6202)</li> <li>Git ignore <code>__pycache__</code> (#6201)</li> <li>Compatibility tests code cleanup (#6198)</li> <li>Site: Update community page with updated chat information (#6192)</li> <li>Move Nessie speecific build code into this build (#6196)</li> <li>Nit: javadoc (#6193)</li> <li>Gradle 8 adoption for NesQuEIT (#6166)</li> <li>mac-CI: Workaround for brew-upgrade issue (#6184)</li> </ul>"},{"location":"releases-0.69/#0500-release-february-24-2023","title":"0.50.0 Release (February 24, 2023)","text":"<p>See Release information on GitHub.</p> <ul> <li>Strip jakarta.* annotations for Java 8 (#6172)</li> <li>Fix Nessie GH URL in docs (#6167)</li> <li>Refactor advanced configuration (#6159)</li> <li>Site: Update community links CI and README.md CI (#6165)</li> <li>Redesign telemetry support in Helm chart (#6153)</li> <li>Disable versioned-transfer ITs in macOS CI (#6164)</li> <li>Remove rocksdb.dbPath from Helm chart values (#6149)</li> <li>Fix console log level setting in Helm charts (#6158)</li> <li>Minor fixes to the build-push-images.sh script (#6155)</li> <li>mac-os/win CI - add retry for steps that regularly timeout/hang (#6152)</li> <li>Add missing jakarta.* annotations (#6150)</li> <li>Disable intTest using containers/podman for macOS CI (#6148)</li> <li>Improve nessie-ui build a little bit (#6147)</li> <li>Bump npm from 7.24.2 to 9.4.2 and nodejs from 16.14.2 to 18.4.1 (#6145)</li> <li>Rename jobs for mac+win GH workflows (#6143)</li> <li>Allow all GH WFs to become \u201crequired checks\u201d (#6131)</li> <li>Label <code>model</code> changes with <code>pr-integrations</code> (#6132)</li> <li>GH workflow to smoke test Docker images (#6127)</li> <li>Fix typo in release notes (#6129)</li> <li>PR Auto labeler (#6128)</li> <li>Fix renovate configuration (#6124)</li> </ul>"},{"location":"releases-0.69/#older-releases","title":"Older releases","text":"<p>See this page for 0.49.0 and older</p>"},{"location":"releases/","title":"Release Notes","text":"<p>See Nessie Server upgrade notes for supported upgrade paths.</p>"},{"location":"releases/#01060-release-december-05-2025","title":"0.106.0 Release (December 05, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#fixes","title":"Fixes","text":"<ul> <li>Catalog/S3: Add a per-bucket configuration option <code>chunked-encoding-enabled</code> (defaults to <code>true</code>)   so deployments targeting Oracle Cloud Infrastructure (OCI) or other S3-compatible stores that   reject chunked payload signatures can disable AWS SDK chunked encoding without downgrading   Nessie. Fixes #11441.</li> </ul>"},{"location":"releases/#commits","title":"Commits","text":"<ul> <li>Ninja\u2026 one more</li> <li>ninja-fix previous PR</li> <li>Fix create-release workflow (#11691)</li> <li>Support configuration of S3 signing URL validity via chart (#11690)</li> <li>feat(helm): add priority class support (#11689)</li> <li>Add OCI friendly S3 chunked encoding toggle (#11676)</li> <li>Get back snapshot publications - take 2 (#11653)</li> <li>Attempt to get snapshot publishing back (#11629)</li> <li>Ninja (last one!): fix new Quarkus-CR workflow</li> <li>Ninja: fix run of Quarkus CR workflow</li> <li>Ninja: remove conditional</li> <li>Ninja: fix dangling whitespace in Quarkus CR workflow + align names</li> <li>CI: Add a workflow to verify that Quarkus CR versions pass Nessie CI (#11605)</li> <li>Fix a test issue in macOS CI (#11575)</li> </ul>"},{"location":"releases/#01057-release-november-06-2025","title":"0.105.7 Release (November 06, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#fixes_1","title":"Fixes","text":"<ul> <li>Fixes an issue when defining a nested schema field as an identifier field of an Iceberg schema.</li> </ul>"},{"location":"releases/#commits_1","title":"Commits","text":"<ul> <li>Ninja: changelog</li> <li>Fix: nested schema fields used as identifier field causes NPE (#11569)</li> <li>CI: Fix build scans (#11565)</li> <li>Build: improve caching (#11563)</li> </ul>"},{"location":"releases/#01056-release-october-24-2025","title":"0.105.6 Release (October 24, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#fixes_2","title":"Fixes","text":"<ul> <li>Fixes an issue where Iceberg batch deletions did not work with Nessie remote S3 signing.</li> </ul>"},{"location":"releases/#commits_2","title":"Commits","text":"<ul> <li>Build: fix some Java deprecation warnings (#11516)</li> <li>Gradle 9 prep - create iceberg/bom directory (#11515)</li> <li>Bugfix: S3 remote signing for batch deletions does not work (#11493)</li> <li>Prepare for Gradle 9 (#11491)</li> <li>Quarkus: Tackle some more deprecated/changed configs (#11492)</li> <li>CI/Test: Tackle deprecated otel config property (#11490)</li> </ul>"},{"location":"releases/#01055-release-october-16-2025","title":"0.105.5 Release (October 16, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#fixes_3","title":"Fixes","text":"<ul> <li>Iceberg REST: adapt the deprecation of <code>lastColumnId</code> in <code>AddSchema</code> table metadata update, fiel field is optional now</li> </ul>"},{"location":"releases/#commits_3","title":"Commits","text":"<ul> <li>Fix javadocs for checkstyle, add <code>--continue</code> to CI jobs, remove retries (#11464)</li> <li>Object-storage-mock: Fix off-by-1 for HTTP range requests (#11465)</li> <li>Support parsing compressed metadata files in registerTable (#11435)</li> </ul>"},{"location":"releases/#01054-release-october-08-2025","title":"0.105.4 Release (October 08, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#fixes_4","title":"Fixes","text":"<ul> <li>Iceberg REST: adapt the deprecation of <code>lastColumnId</code> in <code>AddSchema</code> table metadata update, fiel field is optional now</li> </ul>"},{"location":"releases/#commits_4","title":"Commits","text":"<ul> <li>Add KMS to GC tool (#11433)</li> </ul>"},{"location":"releases/#01053-release-september-24-2025","title":"0.105.3 Release (September 24, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#fixes_5","title":"Fixes","text":"<ul> <li>Iceberg REST: adapt the deprecation of <code>lastColumnId</code> in <code>AddSchema</code> table metadata update, fiel field is optional now</li> </ul>"},{"location":"releases/#commits_5","title":"Commits","text":"<ul> <li>IRC: Make AddSchema.lastColumnId optional (#11352)</li> <li>Add <code>nessie-object-storage-mock</code> to <code>nessie-bom</code> (#11343)</li> <li>Fix maxOS CI (#11329)</li> </ul>"},{"location":"releases/#01052-release-september-19-2025","title":"0.105.2 Release (September 19, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#highlights","title":"Highlights","text":"<ul> <li>Nessie UI: CSS + fonts are now fetched from Nessie instead of external sources (CDNs).</li> </ul>"},{"location":"releases/#commits_6","title":"Commits","text":"<ul> <li>Updates for Nessie-UI merge (#11323)</li> <li>CI/Caching: Fix Gradle cache retention (#11318)</li> </ul>"},{"location":"releases/#01051-release-september-16-2025","title":"0.105.1 Release (September 16, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#highlights_1","title":"Highlights","text":"<ul> <li>Bump Iceberg from version 1.9.2 to 1.10.0</li> </ul>"},{"location":"releases/#commits_7","title":"Commits","text":"<ul> <li>Ninja: changelog</li> <li>Iceberg 1.10: Adopt Nessie CLI for Iceberg 1.10 S3 remote signing reconfiguration (#11305)</li> <li>Site-build: do not exclude 1.10 (#11302)</li> <li>Iceberg 1.10: S3 remote signing can no longer be \u201creconfigured\u201d (#11303)</li> <li>Add AWSSDK kms dependency, required w/ Iceberg 1.10 (#11300)</li> <li>Iceberg 1.10: Adopt to oauth changes (#11304)</li> <li>Adopt Iceberg 1.10 GenericManifestFile restrictions (#11299)</li> <li>build: Nessie 0.10x is less than 0.50.* (#11301)</li> <li>Nit: fix deprecation warning in NessieServerAdminTestExtension (#11293)</li> <li>Nit: tackle deprecation warnings (#11294)</li> <li>Build/Shadow: handle duplicate files (no warnings) (#11292)</li> <li>Build/local-testing: Allow using custom Quarkus versions and not enforce Quarkus platform versions (#11276)</li> <li>Migrate from VLSI Jandex plugin to Kordamp Jandex plugin (#11268)</li> <li>Ninja: bump site-build timeout</li> </ul>"},{"location":"releases/#01050-release-september-03-2025","title":"0.105.0 Release (September 03, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#fixes_6","title":"Fixes","text":"<ul> <li>A bug has been resolved in the OAuth2 Authorization Code grant type. This fix addresses an issue   where the <code>extra-params</code> configuration option was not being properly included in the authorization   URI. Consequently, users, particularly Auth0 users passing the <code>audience</code> parameter, were receiving   opaque tokens instead of JWTs.</li> </ul>"},{"location":"releases/#commits_8","title":"Commits","text":"<ul> <li>OAuth2 (Authorization Code): Include extra parameters in authorization URI (#11239)</li> </ul>"},{"location":"releases/#010410-release-august-27-2025","title":"0.104.10 Release (August 27, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#changes","title":"Changes","text":"<ul> <li>JDBC: Previously, the JDBC backends and Nessie GC left the fetch-size to its default, which is to fetch   all rows when a <code>SELECT</code> statement is executed. This is changed to default to a fetch-size of 100 rows.   To revert to the old behavior, set the corresponding configuration option to <code>0</code> (not recommended).   A corresponding, new command line option for Nessie GC has been introduced as well.</li> </ul>"},{"location":"releases/#commits_9","title":"Commits","text":"<ul> <li>fix: typo in server-iam.assume-role (#11219)</li> </ul>"},{"location":"releases/#01049-release-august-25-2025","title":"0.104.9 Release (August 25, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#changes_1","title":"Changes","text":"<ul> <li>JDBC: Previously, the JDBC backends and Nessie GC left the fetch-size to its default, which is to fetch   all rows when a <code>SELECT</code> statement is executed. This is changed to default to a fetch-size of 100 rows.   To revert to the old behavior, set the corresponding configuration option to <code>0</code> (not recommended).   A corresponding, new command line option for Nessie GC has been introduced as well.</li> </ul>"},{"location":"releases/#commits_10","title":"Commits","text":"<ul> <li>Fix STDOUT redirect in create-gh-release-notes.sh (#11209)</li> </ul>"},{"location":"releases/#01047-release-august-25-2025","title":"0.104.7 Release (August 25, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#changes_2","title":"Changes","text":"<ul> <li>JDBC: Previously, the JDBC backends and Nessie GC left the fetch-size to its default, which is to fetch   all rows when a <code>SELECT</code> statement is executed. This is changed to default to a fetch-size of 100 rows.   To revert to the old behavior, set the corresponding configuration option to <code>0</code> (not recommended).   A corresponding, new command line option for Nessie GC has been introduced as well.</li> </ul>"},{"location":"releases/#commits_11","title":"Commits","text":"<ul> <li>Fix release workflow in case no non-renovate commits happened (#11198)</li> </ul>"},{"location":"releases/#01045-release-august-21-2025","title":"0.104.5 Release (August 21, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#changes_3","title":"Changes","text":"<ul> <li>JDBC: Previously, the JDBC backends and Nessie GC left the fetch-size to its default, which is to fetch   all rows when a <code>SELECT</code> statement is executed. This is changed to default to a fetch-size of 100 rows.   To revert to the old behavior, set the corresponding configuration option to <code>0</code> (not recommended).   A corresponding, new command line option for Nessie GC has been introduced as well.</li> </ul>"},{"location":"releases/#commits_12","title":"Commits","text":""},{"location":"releases/#01044-release-august-18-2025","title":"0.104.4 Release (August 18, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#changes_4","title":"Changes","text":"<ul> <li>JDBC: Previously, the JDBC backends and Nessie GC left the fetch-size to its default, which is to fetch   all rows when a <code>SELECT</code> statement is executed. This is changed to default to a fetch-size of 100 rows.   To revert to the old behavior, set the corresponding configuration option to <code>0</code> (not recommended).   A corresponding, new command line option for Nessie GC has been introduced as well.</li> </ul>"},{"location":"releases/#commits_13","title":"Commits","text":"<ul> <li>Add missing s3.path-style-access to GC Tool help (#11169)</li> <li>Helm chart: fix nodeport support (#11137)</li> </ul>"},{"location":"releases/#01043-release-july-11-2025","title":"0.104.3 Release (July 11, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#changes_5","title":"Changes","text":"<ul> <li>JDBC: Previously, the JDBC backends and Nessie GC left the fetch-size to its default, which is to fetch   all rows when a <code>SELECT</code> statement is executed. This is changed to default to a fetch-size of 100 rows.   To revert to the old behavior, set the corresponding configuration option to <code>0</code> (not recommended).   A corresponding, new command line option for Nessie GC has been introduced as well.</li> </ul>"},{"location":"releases/#commits_14","title":"Commits","text":"<ul> <li>Switch Keycloak container to standard token exchange (#10963)</li> <li>Respect <code>idea.active</code> in addition to <code>idea.sync.active</code> (#10980)</li> </ul>"},{"location":"releases/#01042-release-june-12-2025","title":"0.104.2 Release (June 12, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#changes_6","title":"Changes","text":"<ul> <li>JDBC: Previously, the JDBC backends and Nessie GC left the fetch-size to its default, which is to fetch   all rows when a <code>SELECT</code> statement is executed. This is changed to default to a fetch-size of 100 rows.   To revert to the old behavior, set the corresponding configuration option to <code>0</code> (not recommended).   A corresponding, new command line option for Nessie GC has been introduced as well.</li> </ul>"},{"location":"releases/#commits_15","title":"Commits","text":"<ul> <li>JDBC: Let JDBC fetch-size default to 100 (#10933)</li> <li>Testing/object-storage-mock: fix content-length for HTTP range requests (#10932)</li> <li>Fix error messages in <code>AssertRefSnapshotId</code> (#10860)</li> <li>feat(build): make archive builds reproducible (#10858)</li> <li>Java11-client: don\u2019t share the FJP, shutdown (if possible) (#10835)</li> <li>Add <code>public</code> workaround to some tests (#10836)</li> <li>Handle Iceberg NestedField.of() deprecation (#10829)</li> <li>Object-storage-mock: add start/stop log messages (#10832)</li> <li>Move secrets-manager <code>QuarkusTest</code>s to <code>src/test/</code> (#10839)</li> <li>Testing: give Quarkus 4g (#10837)</li> <li>Update <code>AddressResolver</code>, prepare for Vertx 5 (#10838)</li> <li>QUarkus: <code>RestAssured</code> may sometimes have the wrong port (#10840)</li> <li>Multi-env-test-engine cosmetics (#10828)</li> <li>Disable <code>ITOAuth2ClientAuthelia.testOAuth2AuthorizationCode()</code> (#10830)</li> </ul>"},{"location":"releases/#01041-release-may-07-2025","title":"0.104.1 Release (May 07, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#changes_7","title":"Changes","text":"<ul> <li>OpenAPI specs are no longer published to swaggerhub.</li> </ul>"},{"location":"releases/#commits_16","title":"Commits","text":"<ul> <li>Migrate to a different Maven publishing plugin (#10784)</li> </ul>"},{"location":"releases/#01040-release-may-06-2025","title":"0.104.0 Release (May 06, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#changes_8","title":"Changes","text":"<ul> <li>OpenAPI specs are no longer published to swaggerhub.</li> </ul>"},{"location":"releases/#commits_17","title":"Commits","text":"<ul> <li>Update Sonatype publishing URLs (#10758)</li> <li>Bump iceberg to 1.9 (#10773)</li> </ul>"},{"location":"releases/#01036-release-may-01-2025","title":"0.103.6 Release (May 01, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#changes_9","title":"Changes","text":"<ul> <li>OpenAPI specs are no longer published to swaggerhub.</li> </ul>"},{"location":"releases/#commits_18","title":"Commits","text":"<ul> <li>Remove links to swaggerhub (#10757)</li> </ul>"},{"location":"releases/#01035-release-april-26-2025","title":"0.103.5 Release (April 26, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features","title":"New Features","text":"<ul> <li>Add PDB support to helm chart</li> </ul>"},{"location":"releases/#commits_19","title":"Commits","text":"<ul> <li>Ninja: changelog</li> <li>Release: Fix Helm chart publication (#10730)</li> </ul>"},{"location":"releases/#01034-release-april-24-2025","title":"0.103.4 Release (April 24, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#changes_10","title":"Changes","text":"<ul> <li>Change default of <code>nessie.version.store.persist.cache-enable-soft-references</code> to <code>false</code></li> </ul>"},{"location":"releases/#commits_20","title":"Commits","text":"<ul> <li>Add PDB support to helm chart (#10709)</li> <li>Cache-config: fix default for \u201csoft references\u201d to <code>false</code> (#10668)</li> </ul>"},{"location":"releases/#01033-release-april-08-2025","title":"0.103.3 Release (April 08, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_1","title":"New Features","text":"<ul> <li>Introduces a hard objects-cache capacity limit to ensure that the cache does never consume more than   the configured cache-capacity plus a configurable \u201covershoot\u201d (defaults to 10%). New cache entries are   admitted as long as the current cache size is less than the \u201ccache-capacity + overshoot\u201d.</li> </ul>"},{"location":"releases/#changes_11","title":"Changes","text":"<ul> <li>Nessie\u2019s REST API endpoints now accept \u201ctruncated timestamps\u201d in relative-commit-specs, aka without the   second-fraction.</li> </ul>"},{"location":"releases/#commits_21","title":"Commits","text":"<ul> <li>Introduce a hard capacity limit for the objects cache (#10629)</li> <li>Maven publication: Produce correct <code>&lt;scm&gt;&lt;tag&gt;</code> in <code>pom.xml</code> (#10656)</li> <li>Relax Nessie REST API relative-commit-spec timestamp rules (#10623)</li> <li>Simplify Lowkey Vault configuration (#10591)</li> </ul>"},{"location":"releases/#01032-release-march-21-2025","title":"0.103.2 Release (March 21, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_2","title":"New Features","text":"<ul> <li>Catalog/S3/request-signing: Add a per-S3-bucket config option <code>url-signing-expire</code> to override the default   3-hour lifetime of S3-URL-signing URLs.</li> </ul>"},{"location":"releases/#commits_22","title":"Commits","text":"<ul> <li>Make S3 signing URL validity configurable (#10582)</li> <li>Adopt to next guava version (#10581)</li> <li>Disable NTP check for Authelia ITs (#10570)</li> <li>Adopt to JUnit deprecation (#10569)</li> </ul>"},{"location":"releases/#01031-release-march-18-2025","title":"0.103.1 Release (March 18, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#highlights_2","title":"Highlights","text":"<ul> <li>Configuration option <code>nessie.version.store.persist.cache-enable-soft-references</code> defaults to    <code>false</code> now. Some feedback suggests that using soft references in the Nessie cache may not be   optimal with respect to GC overhead in some environments, so defaulting to <code>false</code> is safer.</li> </ul>"},{"location":"releases/#new-features_3","title":"New Features","text":"<ul> <li>Helm: Allow annotations on the configmap</li> </ul>"},{"location":"releases/#fixes_7","title":"Fixes","text":"<ul> <li>Catalog: Return consistent metadata-location for Iceberg REST APIs</li> </ul>"},{"location":"releases/#commits_23","title":"Commits","text":"<ul> <li>Ninja: changelog</li> <li>Helm: Allow annotations on the configmap.  (#10510)</li> <li>Catalog: Return consistent metadata-location for Iceberg REST APIs (#10508)</li> <li>Make <code>cache-enable-soft-references</code> default to <code>false</code> (#10526)</li> <li>Add <code>copy</code> command to the ContentGenerator tool (#10443)</li> <li>Renovate: Quarkus Group (#10435)</li> <li>CI/NesQuEIT: Remove Spark 3.3 + revert workaround #10184 (#10436)</li> </ul>"},{"location":"releases/#01030-release-february-18-2025","title":"0.103.0 Release (February 18, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#highlights_3","title":"Highlights","text":"<ul> <li>If you are using Iceberg/Java 1.8.0 it is STRONGLY RECOMMENDED to upgrade to this or a newer Nessie release!</li> </ul>"},{"location":"releases/#upgrade-notes","title":"Upgrade notes","text":"<ul> <li>This Nessie version is compatible with Iceberg/Java version 1.8.0 via Iceberg REST.   Iceberg 1.8.0 contains changes that breaks compatibility with previous Nessie versions!</li> <li>Iceberg table spec v3 is not supported in Nessie, because it is still under active development.</li> </ul>"},{"location":"releases/#changes_12","title":"Changes","text":"<ul> <li>Dependencies that are only licensed using GPL+CE are no longer included in Nessie CLI, server and admin tool.</li> </ul>"},{"location":"releases/#commits_24","title":"Commits","text":"<ul> <li>Ban dependencies licensed only as GPL+CE (#10413)</li> <li>Update changelog (#10412)</li> <li>Iceberg 1.8: Adopt remaining changes in CatalogTests + ViewCatalogTests (#10411)</li> </ul>"},{"location":"releases/#01025-release-february-05-2025","title":"0.102.5 Release (February 05, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#note","title":"Note","text":"<ul> <li>This release has no code changes.</li> <li>NOTICE and LICENSE files clarifications, included in jars published to Maven Central.</li> </ul>"},{"location":"releases/#commits_25","title":"Commits","text":"<ul> <li>Include NOTICE+LICENSE in every jar (#10331)</li> </ul>"},{"location":"releases/#01024-release-january-31-2025","title":"0.102.4 Release (January 31, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#note_1","title":"Note","text":"<ul> <li>This release has no code changes except NOTICE file(s) clarifications.</li> </ul>"},{"location":"releases/#commits_26","title":"Commits","text":"<ul> <li>Ninja: CHANGELOG</li> <li>Add separate NOTICE-BINARY-DIST file (#10315)</li> <li>Site: Improve the <code>Nessie Spark SQL Extensions</code> page (#10304)</li> </ul>"},{"location":"releases/#01023-release-january-30-2025","title":"0.102.3 Release (January 30, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_4","title":"New Features","text":"<ul> <li>Catalog: Iceberg table configurations overrides are now available in storage configuration settings.   Example: <code>nessie.catalog.service.s3.default-options.table-config-overrides.py-io-impl=pyiceberg.io.pyarrow.PyArrowFileIO</code></li> </ul>"},{"location":"releases/#commits_27","title":"Commits","text":"<ul> <li>Ninja: add AL2 license for smallrye-certs</li> <li>Renovate: group quarkus-platform + quarkus-plugin together (#10300)</li> <li>Fix build scripts to distribute the right files (#10297)</li> <li>Catalog: Add table config overrides to bucket configuration (#10296)</li> <li>Fix a new failure in NesQuEIT, dependency issue w/ scala-collection-compat (#10298)</li> <li>Fix NesQuEIT for recent Iceberg changes (#10281)</li> <li>Adopt renovate config for #10275 (#10280)</li> <li>Update some Maven coordinates (#10275)</li> </ul>"},{"location":"releases/#01022-release-january-23-2025","title":"0.102.2 Release (January 23, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#fixes_8","title":"Fixes","text":"<ul> <li>Nessie re-assigns IDs for new schemas/partition-specs/sort-orders. The check that the provided ID for   those must be valid (&gt;= 0) is therefore superfluous, it can actually unnecessarily lead to problems. This   change also fixes an issue that the last-added schema/spec/sort ID is set to -1, if the schema/spec/sort   already existed. This lets the set-current-schema/set-default-partition-spec/set-default-sort-order   updates with <code>-1</code> for the last-added one fail, but it should return the ID of the schema/spec/sort ID that   already existed.</li> </ul>"},{"location":"releases/#commits_28","title":"Commits","text":"<ul> <li>Catalog: Allow passing -1 for new schema/partition-spec/sort-order (#10264)</li> </ul>"},{"location":"releases/#01021-release-january-22-2025","title":"0.102.1 Release (January 22, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#fixes_9","title":"Fixes","text":"<ul> <li>Catalog/ADLS: Fix an issue that prevented the usage of retry-options for ADLS.</li> </ul>"},{"location":"releases/#commits_29","title":"Commits","text":"<ul> <li>Catalog/ADLS: Only use <code>RequestRetryOptions</code> (#10255)</li> </ul>"},{"location":"releases/#01020-release-january-21-2025","title":"0.102.0 Release (January 21, 2025)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_5","title":"New Features","text":"<ul> <li>When using OAuth authentication, the Nessie client now supports including extra parameters in   requests to the token endpoint. This is useful for passing custom parameters that are not covered   by the standard OAuth 2.0 specification. See the Nessie   documentation for   details.</li> <li>Add a configuration option <code>nessie.version.store.persist.cache-enable-soft-references</code> (defaults to    <code>true</code>) to optionally disable the additional caching the constructed Java objects via soft references.   Having the already constructed Java object is faster when getting object from the cache, but a Java object   tree implies a rather unpredictable heap pressure, hence these object are referenced via Java soft   references. This optimization however can cause heap issues in rare scenarios, and disabling this   optimization can help there.</li> </ul>"},{"location":"releases/#fixes_10","title":"Fixes","text":"<ul> <li>Fix an issue that prevents the Nessie Server Admin tool to purge unreferenced data in the backend   database, for data being written before Nessie version 0.101.0.</li> <li>Fix an issue that prevents using nested fields in partition-spec and sort-order.   Given a schema having a <code>struct &lt; field_a, field_b &gt;</code>, it was not possible to reference   <code>field_a</code> or <code>field_b</code> in a partition-spec or sort-order. There was no issue however using fields   at the \u201ctop level\u201d (a schema like <code>field_a, field_b</code>).</li> </ul>"},{"location":"releases/#commits_30","title":"Commits","text":"<ul> <li>Fix using nested fields in partition-spec and sort-order (#10237)</li> <li>Fix backwards compatibility issues with Obj.referenced (#10218)</li> <li>Iceberg-update / set-statistics / snapshot-id deprecation (#10234)</li> <li>Cache: add option to disable soft references (#10217)</li> <li>Adopt Object Storage Mock to respect S3 chunked input trailing headers (#10231)</li> <li>Fix NesQuEIT to pass against recent Iceberg changes (#10184)</li> <li>Migrate to maintained shadow plugin (#10183)</li> <li>OAuth client: add support for custom request parameters (#10154)</li> <li>Remove duplicate entry in auth docs (#10157)</li> <li>Remove use of ScyllaDB in Nessie (#10144)</li> <li>Enable Azure Key Vault IT (#10142)</li> <li>Support metrics relabelings in service monitor (#10095)</li> </ul>"},{"location":"releases/#01013-release-december-18-2024","title":"0.101.3 Release (December 18, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_6","title":"New Features","text":"<ul> <li>Add the <code>cut-history</code> command to the Nessie server admin tool. This command allows advanced users to    detach certain commits from their predecessors (direct and merge parents).</li> </ul>"},{"location":"releases/#commits_31","title":"Commits","text":"<ul> <li>Add admin tool command to cut commit log at a certain point. (#10048)</li> <li>Revert \u201cWorkaround for CI failures because of missing <code>vectorized/redpanda</code> images (#10074)\u201d (#10087)</li> </ul>"},{"location":"releases/#01012-release-december-12-2024","title":"0.101.2 Release (December 12, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#fixes_11","title":"Fixes","text":"<ul> <li>Fix large index processing in the <code>cleanup-repository</code> admin command.</li> </ul>"},{"location":"releases/#commits_32","title":"Commits","text":"<ul> <li>Fix clean-up of reference index objects (#10083)</li> <li>Workaround for CI failures because of missing <code>vectorized/redpanda</code> images (#10074)</li> </ul>"},{"location":"releases/#01011-release-december-09-2024","title":"0.101.1 Release (December 09, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#fixes_12","title":"Fixes","text":"<ul> <li>Fix handling of Iceberg update-requirement \u201cno current snapshot\u201d</li> </ul>"},{"location":"releases/#commits_33","title":"Commits","text":"<ul> <li>Fix update-requirement check to handle \u201cno current snapshot\u201d requirement properly (#10064)</li> </ul>"},{"location":"releases/#01010-release-december-06-2024","title":"0.101.0 Release (December 06, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_7","title":"New Features","text":"<ul> <li>Helm: Add clusterIP and traffic policy to helm service config</li> <li>Add functionality to the Nessie server admin tool, the <code>cleanup-repository</code> command, to delete   unneeded objects from a Nessie repository (backend database).</li> </ul>"},{"location":"releases/#commits_34","title":"Commits","text":"<ul> <li>Server admin tool: add command to purge unreferenced <code>Obj</code>s (#9753)</li> <li>Persistence: purge unreferenced <code>Obj</code>s (#9688)</li> <li>Disable tests using containers on macOS in CI (#10038)</li> <li>Docs: update environment variables table and add section on Kubernetes memory settings (#10035)</li> <li>Ninja: changelog</li> <li>Add clusterIP and traffic policy to helm service config (#10011)</li> <li>Port some <code>CatalogTests</code> updates from Iceberg (#10036)</li> <li>Propagate CDI scopes to health checks (#10026)</li> </ul>"},{"location":"releases/#01003-release-december-02-2024","title":"0.100.3 Release (December 02, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_8","title":"New Features","text":"<ul> <li>Add <code>deploymentStrategy</code> to Helm deployment configs</li> </ul>"},{"location":"releases/#fixes_13","title":"Fixes","text":"<ul> <li>Allow multiple <code>SetProperties</code> updates via Iceberg REST.</li> </ul>"},{"location":"releases/#commits_35","title":"Commits","text":"<ul> <li>Ninja: changelog</li> <li>Add deployment strategy to helm deployment configs (#10012)</li> <li>Catalog: Allow multiple <code>SetProperties</code> updates (#10024)</li> <li>Build: Allow testing GC with Java 23 (#10023)</li> <li>Renovate: Update config for trino-client (#10004)</li> <li>Quarkus/Agoral: fix deprecation (#10000)</li> <li>Migrate to <code>org.apache.cassandra:java-driver-bom</code> and bump to latest version 4.18.1 (#9992)</li> <li>Support v2 iceberg views with gzip (#9982)</li> <li>Snapshot publishing: no build scan (#9976)</li> <li>Nit: Remove antlr-removal left-over (#9977)</li> <li>Spark integrations: enhance tests to use <code>merge-on-read</code> for compaction and rewrite-manifests (#9974)</li> <li>Build/release: Fix Git info after #9965 (#9975)</li> <li>Remove \u201cAWS Athena\u201d from the index page (#9972)</li> <li>Nit: fix an \u201cunchecked/unsafe case\u201d warning (#9966)</li> <li>CI: Use a non-rate-limiting docker.io mirror (#9911)</li> <li>Build: tackle Gradle deprecations marked as for-removal in Gradle 9 (#9965)</li> <li>CI: Explicitly configure Scala version (#9970)</li> </ul>"},{"location":"releases/#01002-release-november-20-2024","title":"0.100.2 Release (November 20, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#highlights_4","title":"Highlights","text":"<ul> <li>Export: ZIP file exports were broken in all Nessie versions from 0.92.1 until 0.100.0.   If you are using any of these versions, you must not use ZIP export mode, but use the   file (directory) based exporter (<code>--output-format=DIRECTORY</code>)!</li> </ul>"},{"location":"releases/#fixes_14","title":"Fixes","text":"<ul> <li>Export: ZIP file exports are fixed with this Nessie version 0.100.1.</li> </ul>"},{"location":"releases/#commits_36","title":"Commits","text":"<ul> <li>Force dnsjava downgrade to 3.5.3 (#9951)</li> </ul>"},{"location":"releases/#01001-release-november-19-2024","title":"0.100.1 Release (November 19, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#highlights_5","title":"Highlights","text":"<ul> <li>Export: ZIP file exports were broken in all Nessie versions from 0.92.1 until 0.100.0.   If you are using any of these versions, you must not use ZIP export mode, but use the   file (directory) based exporter (<code>--output-format=DIRECTORY</code>)!</li> </ul>"},{"location":"releases/#fixes_15","title":"Fixes","text":"<ul> <li>Export: ZIP file exports are fixed with this Nessie version 0.100.1.</li> </ul>"},{"location":"releases/#commits_37","title":"Commits","text":"<ul> <li>Export: fix ZIP output after #9034 (#9945)</li> <li>Helm chart: fix incorrect documentation (#9923)</li> <li>Testing: use age-based pull policy for postgres images (#9910)</li> </ul>"},{"location":"releases/#01000-release-november-12-2024","title":"0.100.0 Release (November 12, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#upgrade-notes_1","title":"Upgrade notes","text":"<ul> <li>Helm chart: the old <code>logLevel</code> field has been replaced with a new <code>log</code> section with many more   options to configure logging. You can now configure console- and file-based logging separately. It   is also possible to enable JSON logging instead of plain text (but this feature requires Nessie &gt;=   0.99.1). For file-based logging, it is also possible to configure rotation and retention policies,   and a persistent volume claim is now automatically created when file-based logging is enabled.   Furthermore, Sentry integration can also be enabled and configured. And finally, it is now   possible to configure the log level for specific loggers, not just the root logger. The old   <code>logLevel</code> field is still supported, but will be removed in a future release.</li> </ul>"},{"location":"releases/#changes_13","title":"Changes","text":"<ul> <li>The persistence cache tries to avoid deserialization overhead when getting an object from the   cache by using Java\u2019s <code>SoftReference</code>. There is no guarantee that cached objects keep their   Java object tree around, but it should eventually for the majority of accesses to frequently   accessed cached objects. The default cache capacity fraction has been reduced from 70% of the   heap size to 60% of the heap size. However, extreme heap pressure may let Java GC clear all   <code>SoftReference</code>s.</li> <li>Sends the following default options, which are convenient when using pyiceberg:</li> <li><code>py-io-impl=pyiceberg.io.fsspec.FsspecFileIO</code></li> <li><code>s3.signer=S3V4RestSigner</code> when S3 signing is being used</li> <li>Iceberg REST: No longer return <code>*FileIO</code> options from the Iceberg REST config endpoint</li> </ul>"},{"location":"releases/#fixes_16","title":"Fixes","text":"<ul> <li>GC: Consider referenced statistics (and partition statistics) files as \u2018live\u2019.</li> <li>JDBC: Perform JDBC commit when auto-creating tables to please transactional schema changes.</li> </ul>"},{"location":"releases/#commits_38","title":"Commits","text":"<ul> <li>Catalog/Iceberg: support new <code>remove-partition-specs</code> metadata-update (#9906)</li> <li>Testing/Docker: use exact version for C* images (#9908)</li> <li>Catalog: Fix \u201cload credendials\u201d model (#9907)</li> <li>Catalog: add new model and api (#9905)</li> <li>Refactor Nessie\u2019s HTTP authentication (Quarkus 3.16 prep) (#9863)</li> <li>Catalog/S3,GCS: Adopt IAM policies to new object-storage layout (Iceberg 1.7.0) (#9897)</li> <li>Catalog/config: add endpoints to config response (Iceberg 1.7.0) (#9895)</li> <li>Testing: disable looking up GCP credentials (#9900)</li> <li>JDBC: commit after DDL setup + more info (#9901)</li> <li>GC: consider statistics files (#9898)</li> <li>Catalog/S3: Adopt S3 signing to new object-storage layout (Iceberg 1.7.0) (#9896)</li> <li>Revert \u201cPrevent tracing initialization race (Quarkus 3.16 prep) (#9866)\u201d (#9899)</li> <li>[Catalog] Do not return <code>*FileIO</code> options from the Iceberg REST config endpoint (#9642)</li> <li>Send s3-signer only when signing is enabled (#9869)</li> <li>Prevent tracing initialization race (Quarkus 3.16 prep) (#9866)</li> <li>Remove <code>@Nested</code> from a Quarkus test (Quarkus 3.16 prep) (#9865)</li> <li>Adopt <code>AmazonSecretsManagerBuilder</code> (Quarkus 3.16 prep) (#9864)</li> <li>Convenience for pyiceberg (#9868)</li> <li>Build only: Prefer Maven Local if enabled (#9861)</li> <li>Build/internal/NesQuEIT: enforce no colon <code>:</code> for <code>nessieProject()</code> (#9842)</li> <li>Docs: update troubleshooting guide with recent UID/GID changes (#9783)</li> <li>Helm chart: add <code>extraInitContainers</code> value (#9773)</li> <li>fix/keycloak-v26-deprecated-vars (#9778)</li> <li>Helm chart: redesign logging options (#9775)</li> <li>Fix some IDE warnings, remove unused code (#9772)</li> <li>server-admin-tool intTest: Re-add <code>forkEvery</code> (#9762)</li> <li>Remove validatation annotations from static functions (#9761)</li> <li>Do not access Apache snapshots repository by default (#9754)</li> <li>Transfer/related: make <code>CoreTransferRelatedObjects</code> generally accessible (#9752)</li> <li>Persist: introduce <code>deleteWithReferenced(Obj)</code> (#9731)</li> <li>ReferenceLogic: parameterized purge of the commit log of a <code>Reference</code> (#9735)</li> <li>Add convenience functionality to get all storage locations defined in <code>LakehouseConfig</code> (#9742)</li> <li>More verbose \u201cUnauthorized signing request\u201d warnings (#9743)</li> <li>Move catalog-config types to separate module (#9741)</li> <li>Site: fix formatting in <code>Time travel with Iceberg REST</code> chapter (#9732)</li> <li>Docker compose: enhance all-in-one example with Spark SQL and Nessie CLI (#9719)</li> <li>Helm chart: explicitly include namespace in created resources (#9711)</li> <li>Let <code>Persist.scanAllObjects()</code> accept an empty set to return all object types (#9687)</li> <li>Make the composite <code>TransferRelatedObjects</code> accessible to other projects (#9689)</li> <li>Events SPI: load implementations via CDI (#9696)</li> <li>Events RI: use Quarkus Messaging extension (#9686)</li> <li>Fix <code>ObjId.longAt()</code> for non-256-bit object IDs (#9685)</li> <li>Fix deprecation of <code>o.t.containers.CassandraContainer</code> + <code>KafkaContainer</code> (#9680)</li> <li>Replace deprecated <code>Aws4Signer</code> with <code>AwsV4HttpSigner</code> (#9681)</li> <li>Cache: keep (deserialized) object around (#9648)</li> <li>Patch version bumps of Scala + Spark 3.5 (#9667)</li> <li>Fix running <code>nessie-quarkus</code> instructions (#9668)</li> <li>Update Docker Compose instructions in Getting Started guide (#9662)</li> <li>Helm chart: remove bogus default value for oidcAuthServerUrl (#9654)</li> <li>Events API: test JSON serde with views (#9645)</li> <li>Events RI: add example with JSON serialization (#9639)</li> </ul>"},{"location":"releases/#0990-release-september-26-2024","title":"0.99.0 Release (September 26, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#breaking-changes","title":"Breaking changes","text":"<ul> <li>The Events API has been redesigned to import the Nessie Model API directly, instead of using   specific DTO classes. This change is intended to simplify the API and facilitate consumption of   the events. The following classes from the <code>org.projectnessie.events.api</code> package have been   removed and replaced with their respective model classes from the <code>org.projectnessie.model</code>   package:<ul> <li><code>CommitMeta</code></li> <li><code>Content</code> and its subclasses</li> <li><code>ContentKey</code></li> <li><code>Reference</code> and its subclasses</li> </ul> </li> <li>Helm chart: the <code>service</code> section has been redesigned to allow for extra services to be defined.   If you have customized the <code>service.ports</code> field, beware that this field is now an array. Also,   the management port configuration has been moved to a new <code>managementService</code> section. And   finally, a new <code>extraServices</code> section has been added to allow for additional services to be   defined.</li> <li>ADLS: The way how storage URIs are resolved to ADLS \u201cbuckets\u201d (container @ storage-account) has been   changed (fixed). An ADLS \u201cbucket\u201d is technically identified by the storage-account, optionally further   identified by a container/file-system name. It is recommended to specify the newly added via the   <code>nessie.catalog.service.adls.file-systems.&lt;key&gt;.authority=container@storageAccount</code> option(s).   The <code>container@storageAccount</code> part is what is mentioned as <code>&lt;file_system&gt;@&lt;account_name&gt;</code> in the Azure   docs.</li> </ul>"},{"location":"releases/#new-features_9","title":"New Features","text":"<ul> <li>Access check SPI has been enhanced to provide richer information in the <code>Check</code> type about the receiving   API (Nessie REST or Iceberg REST) and about the individual changes, especially during a commit operation.</li> </ul>"},{"location":"releases/#changes_14","title":"Changes","text":"<ul> <li>S3/GCS/ADLS: Bucket settings</li> <li>The resolution of the specific bucket options has been enhanced to select the specific bucket options     using the longest matching option including an optional path-prefix.</li> <li>All bucket specific options (<code>nessie.catalog.service.adls.buckets.&lt;key&gt;.</code>,     <code>nessie.catalog.service.gcs.buckets.&lt;key&gt;.</code>, <code>nessie.catalog.service.adls.file-systems.&lt;key&gt;.</code>) got a     new option <code>path-prefix</code>, which is used to restrict settings to a specific object store path prefix.</li> <li>All bucket specific options (<code>nessie.catalog.service.adls.buckets.&lt;key&gt;.</code>,     <code>nessie.catalog.service.gcs.buckets.&lt;key&gt;.</code>, <code>nessie.catalog.service.adls.file-systems.&lt;key&gt;.</code>) got a     new option <code>authority</code>, which is recommended to specify the technical bucket name. If <code>authority</code> is     not specified, it will default to the value of the <code>name</code> option, then default to the <code>key</code> part of the     formerly mentioned maps.</li> <li>The base <code>location</code> of a new entity (e.g. tables) created via Iceberg REST is derived from the nearest   parent namespace that has an explicitly set <code>location</code> property. (Path separator character is <code>/</code>.)</li> <li>The <code>location</code> property on tables (and view) created via Iceberg REST may be explicitly configured, as   long as it can be resolved against the configured object storage locations. (Path separator character   is <code>/</code>.)</li> </ul>"},{"location":"releases/#fixes_17","title":"Fixes","text":"<ul> <li>CLI: Fix connecting to Nessie\u2019s Iceberg REST</li> </ul>"},{"location":"releases/#commits_39","title":"Commits","text":"<ul> <li>CLI: Pull in essential <code>*FileIO</code> dependencies for Iceberg REST (#9640)</li> <li>Events API: add support for direct JSON serialization (#9637)</li> <li>Remove unused <code>sourceHashes</code> from <code>TransplantResult</code> (#9628)</li> <li>Events API: use Nessie model API directly and remove DTOs (#9588)</li> <li>remove rocksdb dependency from nessie-compatibility-common (#9632)</li> <li>Helm chart: more flexible services configuration (#9625)</li> <li>Also initialize Iceberg-View <code>location</code> (#9629)</li> <li>[Catalog] More flexible named buckets (#9617)</li> <li>Nit: remove unintentional output (#9626)</li> <li>LakehouseConfigObj as transfer-related for export/import (#9623)</li> <li>Persistable <code>LakehouseConfig</code> (#9614)</li> <li>Derive <code>location</code> of new tables from parent namespaces, add some validations (#9612)</li> <li>HTTP client: Update Apache HTTP client impl to avoid deprecated classes (#9610)</li> <li>Richer access checks (#9553)</li> <li>Version Store Result API enhancements (#9592)</li> </ul>"},{"location":"releases/#0980-release-september-23-2024","title":"0.98.0 Release (September 23, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#highlights_6","title":"Highlights","text":"<ul> <li>Alert: If you are using MySQL or MariaDB, make sure to update <code>objs</code> table immediately:   <pre><code>ALTER TABLE objs MODIFY c_headers LONGBLOB;\nALTER TABLE objs MODIFY c_incremental_index LONGBLOB;\nALTER TABLE objs MODIFY c_reference_index_stripes LONGBLOB;\nALTER TABLE objs MODIFY i_index LONGBLOB;\nALTER TABLE objs MODIFY i_stripes LONGBLOB;\nALTER TABLE objs MODIFY s_text LONGBLOB;\nALTER TABLE objs MODIFY t_headers LONGBLOB;\nALTER TABLE objs MODIFY t_signature LONGBLOB;\nALTER TABLE objs MODIFY u_value LONGBLOB;\nALTER TABLE objs MODIFY v_data LONGBLOB;\nALTER TABLE objs MODIFY x_data LONGBLOB;\n</code></pre></li> </ul>"},{"location":"releases/#fixes_18","title":"Fixes","text":"<ul> <li>MySQL: Change type of binary columns from <code>BLOB</code> to <code>LONGBLOB</code>.</li> </ul>"},{"location":"releases/#commits_40","title":"Commits","text":"<ul> <li>Helm chart: allow setting config options when the value is a zero-value (#9587)</li> <li>Helm chart: fix Azure SAS token settings (#9585)</li> <li>Nit: move constant used in tests (#9579)</li> <li>Construct <code>*ApiImpl</code> instead of injecting the V1 rest instances (#9577)</li> <li>Simplify IcebergMetadataUpdate/trusted-location (#9576)</li> </ul>"},{"location":"releases/#0971-release-september-19-2024","title":"0.97.1 Release (September 19, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#highlights_7","title":"Highlights","text":"<ul> <li>Alert: If you are using MySQL or MariaDB, make sure to update <code>objs</code> table immediately:   <pre><code>ALTER TABLE objs MODIFY c_headers LONGBLOB;\nALTER TABLE objs MODIFY c_incremental_index LONGBLOB;\nALTER TABLE objs MODIFY c_reference_index_stripes LONGBLOB;\nALTER TABLE objs MODIFY i_index LONGBLOB;\nALTER TABLE objs MODIFY i_stripes LONGBLOB;\nALTER TABLE objs MODIFY s_text LONGBLOB;\nALTER TABLE objs MODIFY t_headers LONGBLOB;\nALTER TABLE objs MODIFY t_signature LONGBLOB;\nALTER TABLE objs MODIFY u_value LONGBLOB;\nALTER TABLE objs MODIFY v_data LONGBLOB;\nALTER TABLE objs MODIFY x_data LONGBLOB;\n</code></pre></li> </ul>"},{"location":"releases/#fixes_19","title":"Fixes","text":"<ul> <li>MySQL: Change type of binary columns from <code>BLOB</code> to <code>LONGBLOB</code>.</li> </ul>"},{"location":"releases/#commits_41","title":"Commits","text":"<ul> <li>MySQL/MariaDB: change from <code>BLOB</code> type to <code>LONGBLOB</code> (#9564)</li> </ul>"},{"location":"releases/#0970-release-september-18-2024","title":"0.97.0 Release (September 18, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_10","title":"New Features","text":"<ul> <li>Helm chart: support has been added for the <code>DYNAMODB2</code>, <code>MONGODB2</code>, <code>CASSANDRA2</code>, and <code>JDBC2</code>   version store types, introduced in Nessie 0.96.0. Also, support for legacy version store types   based on the old \u201cdatabase adapter\u201d code, which were removed in Nessie 0.75.0, has also been   removed from the Helm chart.</li> </ul>"},{"location":"releases/#fixes_20","title":"Fixes","text":"<ul> <li>Helm chart: fixed a regression where a datasource secret would result in a failure to deploy the   chart.</li> </ul>"},{"location":"releases/#commits_42","title":"Commits","text":"<ul> <li>Downgrade Jandex from 3.2.2 to 3.1.8 (#9561)</li> <li>Site: Add information about warehouse + object storages (#9560)</li> <li>Replace versioned-spi operation types with model operation types (#9551)</li> <li>Use <code>Tree</code>/<code>ContentService</code> in Catalog (#9547)</li> <li>Nit: fix JSON alias typo in IcebergMetadataUpdate interface (#9550)</li> <li>Quarkus: fail on unknown config properties (#9542)</li> <li>Add secrets-validation option to docs (#9541)</li> <li>Docs: add missing link in TOC of index.md (#9529)</li> <li>Events notification system for Nessie - Reference Implementation (#6943)</li> </ul>"},{"location":"releases/#0961-release-september-12-2024","title":"0.96.1 Release (September 12, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_11","title":"New Features","text":"<ul> <li>Helm chart: support has been added for the <code>DYNAMODB2</code>, <code>MONGODB2</code>, <code>CASSANDRA2</code>, and <code>JDBC2</code>   version store types, introduced in Nessie 0.96.0. Also, support for legacy version store types   based on the old \u201cdatabase adapter\u201d code, which were removed in Nessie 0.75.0, has also been   removed from the Helm chart.</li> </ul>"},{"location":"releases/#fixes_21","title":"Fixes","text":"<ul> <li>Helm chart: fixed a regression where a datasource secret would result in a failure to deploy the   chart.</li> </ul>"},{"location":"releases/#commits_43","title":"Commits","text":"<ul> <li>Helm chart: add support for DYNAMODB2, MONGODB2, CASSANDRA2, JDBC2 (#9520)</li> <li>Postgres tests: use alpine-based Docker images (#9521)</li> <li>Helm chart: redesign CI fixtures + minor fixes to docs (#9519)</li> <li>[Bug]: Helm Chart does not render in version 0.96 (#9517)</li> </ul>"},{"location":"releases/#0960-release-september-11-2024","title":"0.96.0 Release (September 11, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#upgrade-notes_2","title":"Upgrade notes","text":"<ul> <li>Support for Java 8 has been removed, even for Nessie clients. Minimum runtime requirement for clients   is Java 11.</li> <li>Nessie Docker images now all execute as user <code>nessie</code> (UID 10000 and GID 10001). They would   previously execute as user <code>default</code> (UID 185 and GID 0). This is a security improvement, as the   Nessie images no longer run with a UID within the privileged range, and the GID is no longer 0   (root). If you have any custom configurations, especially Kubernetes manifests containing security   contexts, that rely on the previous user <code>default</code> (UID 185 and GID 0), you will need to adjust   them to reference the new user <code>nessie</code> (UID 10000 and GID 10001) from now on.</li> <li>Helm chart: the chart now comes with sane defaults for both pod and container security contexts.   If you have customized these settings, you don\u2019t need to do anything. If you have not customized these   settings, you may need to check if the new defaults are compatible with your environment.</li> </ul>"},{"location":"releases/#breaking-changes_1","title":"Breaking changes","text":"<ul> <li>The deprecated JDBC configuration properties for <code>catalog</code> and <code>schema</code> have been removed.</li> <li>Catalog/Object store secrets: Secrets are now referenced via a URN as requirement to introduce support   for secret managers like Vault or those offered by cloud vendors. All secret reference URNs use the   pattern <code>urn:nessie-secret:&lt;provider&gt;:&lt;secret-name&gt;</code>.   The currently supported provider is <code>quarkus</code>, the <code>&lt;secret-name&gt;</code> is the name of the Quarkus   configuration entry, which can also be an environment variable name.   Make sure to use the new helm chart.   See Nessie Docs.</li> <li>Catalog/Object store secrets: secrets are now handled as immutable composites, which is important   to support secrets rotation with external secrets managers.   See Nessie Docs.</li> </ul>"},{"location":"releases/#new-features_12","title":"New Features","text":"<ul> <li>Catalog/ADLS: Added experimental support for short-lived SAS tokens passed down to clients. Those   tokens still have read/write access to the whole file system and are not scoped down.</li> <li>Catalog/GCS: Added experimental support for short-lived and scoped down access tokens passed down   to clients, providing a similar functionality as vended-credentials for S3, including object-storage   file layout.</li> <li>Client-configs: Commit authors, signed-off-by, message can be customized per REST/HTTP request. Those   can be configured for both the Nessie client API   and for Iceberg REST catalog clients.</li> <li>Support for Servlet Spec v6 w/ strict URI path validation has been added and will be transparently   used by Nessie REST API v2 clients since this version. This steps is a preparation for when Quarkus   introduces that Servlet Spec. Content keys in URL paths may look different than before. More information   here.</li> <li>The Swagger UI and OpenAPI generation by Quarkus has been disabled, because the contents/results were   wrong. Instead, refer to SwaggerHub. You can   also fetch the Nessie REST OpenAPI yaml from Nessie <code>/nessie-openapi/openapi.yaml</code> (for example via   <code>curl http://127.0.0.1:19120//nessie-openapi/openapi.yaml</code>)</li> <li>Nessie commit author(s) and \u201csigned off by\u201d can now be configured for both Nessie clients and Iceberg   REST clients. More info on   projectnessie.org. </li> <li>Enable authentication for the Nessie Web UI</li> <li>Introduce new <code>JDBC2</code> version store type, which is has the same functionality as the <code>JDBC</code> version   store type, but uses way less columns, which reduces storage overhead for example in PostgreSQL a lot.</li> <li>Introduce new <code>CASSANDRA2</code> version store type, which is has the same functionality as the <code>CASSANDRA</code> version   store type, but uses way less attributes, which reduces storage overhead.</li> <li>Introduce new <code>DYNAMODB2</code> version store type, which is has the same functionality as the <code>DYNAMODB</code> version   store type, but uses way less attributes, which reduces storage overhead.</li> <li>Introduce new <code>MONGODB2</code> version store type, which is has the same functionality as the <code>MONGODB</code> version   store type, but uses way less attributes, which reduces storage overhead.</li> <li>Added functionality to optionally validate that referenced secrets can be resolved, opt-in.</li> </ul>"},{"location":"releases/#deprecations","title":"Deprecations","text":"<ul> <li>The current version store type <code>JDBC</code> is deprecated, please migrate to the new <code>JDBC2</code> version store   type. Please use the Nessie Server Admin Tool   to migrate from the <code>JDBC</code> version store type to <code>JDBC2</code>.</li> <li>The current version store type <code>CASSANDRA</code> is deprecated, please migrate to the new <code>CASSANDRA2</code> version store   type. Please use the Nessie Server Admin Tool   to migrate from the <code>CASSANDRA</code> version store type to <code>CASSANDRA2</code>.</li> <li>The current version store type <code>MONGODB</code> is deprecated, please migrate to the new <code>MONGODB2</code> version store   type. Please use the Nessie Server Admin Tool   to migrate from the <code>MONGODB</code> version store type to <code>MONGODB2</code>.</li> </ul>"},{"location":"releases/#fixes_22","title":"Fixes","text":"<ul> <li>CLI: fixed a bug that was preventing the tool from running properly when history is disabled.</li> </ul>"},{"location":"releases/#commits_44","title":"Commits","text":"<ul> <li>Secrets validation (#9509)</li> <li>Introduce functional <code>LakehouseConfig</code> (#9353)</li> <li>External secrets managers follow-up (#9497)</li> <li>Wire <code>SecretsProvider</code>s up to Quarkus (#8708)</li> <li>Add <code>SecretsProvider</code> implementations for AWS, GCP, Vault (#8707)</li> <li>Move object-store configuration types to <code>:nessie-catalog-files-api</code> (#9350)</li> <li>Reference secrets by name, do not inject (#9345)</li> <li>\u201cThread per test class\u201d via <code>MultiEnvTestEngine</code> (#9453)</li> <li>public test methods in <code>BaseTestNessieApi</code> (#9472)</li> <li>Introduce <code>MONGODB2</code> version store type, deprecate <code>MONGODB</code> version store type (#9367)</li> <li>Helm chart: strengthen default security context (#9448)</li> <li>Add <code>referenced</code> attribute to persisted <code>Obj</code>s (#9401)</li> <li>Helm chart: add license headers and LICENSE file (#9466)</li> <li>Switch Nessie Docker images to use UID 10000 (#9456)</li> <li>Add ability to change pathType of ingress (#9462)</li> <li>CLI: make tool runnable without history (#9449)</li> <li>Use non-blocking random (#9445)</li> <li>Switch to new s3-sign endpoint using an opaque path parameter (#9447)</li> <li>Docs: fix broken link (#9446)</li> <li>Choose the Nessie client by name, case-insensitive (#9439)</li> <li>Cleanup resources held by Iceberg that accumulate JVM resources (#9440)</li> <li>Introduce <code>DYNAMODB2</code> version store type, deprecate <code>DYNAMODB</code> version store type (#9418)</li> <li>Adopt tests for C*2 (#9426)</li> <li>Introduce <code>CASSANDRA2</code> version store type, deprecate <code>CASSANDRA</code> version store type (#9368)</li> <li>Minor cleanups in JDBC(2) (#9422)</li> <li>Introduce <code>JDBC2</code> version store type, deprecate <code>JDBC</code> version store type (#9366)</li> <li>Allow Nessie Web UI to authenticate (#9398)</li> <li>[DocTool] Allow nested config sections (#9370)</li> <li>Catalog: use <code>prefixKey</code> in \u201clist\u201d operations (#9383)</li> <li>OAuth client: avoid recomputing HTTP headers for static secrets (#9411)</li> <li>Remove no longer used \u201cglobal state\u201d code (#9365)</li> <li>GC: always look into all keys of the last commit (#9400)</li> <li>Add license element to every pom (#9407)</li> <li>Minor cleanup in <code>:nessie-versioned-storage-store</code> + commit-logic-impl (#9399)</li> <li>Ninja: format comment</li> <li>Mitigate OOM during <code>:nessie-server-admin-tool:intTest</code> (#9371)</li> <li>Build: remove unneeded references to jacoco (#9384)</li> <li>Drop support for Java 8 (#9253)</li> <li>Require Java 21 for Nessie build (#9382)</li> <li>Remove a couple unused dependency declaration (#9376)</li> <li>Nit: remove unused field (#9359)</li> <li>Fix use of deprecated API (#9357)</li> <li>Add notes about running <code>ct lint</code> and <code>helm lint</code> locally (#9354)</li> <li>Ninja: changelog</li> <li>Eliminate more Quarkus test warnings (#9341)</li> <li>Use <code>@RepositoryId</code> qualifier instead of named bean (#9348)</li> <li>Allow tweaking <code>CommitMeta</code> via HTTP headers (#9335)</li> <li>Move object-storage specific code from <code>IcebergConfigurer</code> into <code>ObjectIO</code> implementations (#9327)</li> <li>Nit: suppress warning in <code>S3*Iam</code> types (#9349)</li> <li>Adopt Nessie REST API + client (#9289)</li> <li>Disable Swagger-UI + eliminate \u201cduplicate operation ID\u201d warnings in Quarkus tests (#9340)</li> <li>Explicitly prevent empty content keys (#9338)</li> <li>Fix typos in docs (#9339)</li> <li>Catalog: Implementation for down-scoped GCP/GCS access tokens (#9302)</li> <li>Add java client-based test for <code>getEntries</code> with a UDF (#9336)</li> <li>Add some \u201cparanoid\u201d tests for new URL encoding algorithm + forbid 0x7f (#9333)</li> <li>Fix javadoc after #9282 (#9332)</li> <li>Catalog: Implementation for short-lived user-delegation SAS tokens (#9301)</li> <li>BasicAuthenticationProvider: ability to dynamically provide a password (#9319)</li> <li>Prepare for Jakarta Servlet Spec 6 (#9282)</li> <li>OAuth2 client: introduce secret suppliers (#9315)</li> <li>Prepare downscoped credentials for ADLS + GCS (#9299)</li> <li>Doc generator: fix incorrect prefix when property has custom section (#9303)</li> <li>Site: <code>migration.md</code> overhaul + add server-admin-tool command references (#9291)</li> <li>Catalog: more producers (#9300)</li> <li>Release: Fix openapi publishing idempotency (#9294)</li> </ul>"},{"location":"releases/#0950-release-august-07-2024","title":"0.95.0 Release (August 07, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#catalog-s3-bucket-configuration-changes-breaking","title":"Catalog S3 bucket configuration changes / breaking","text":"<ul> <li>The S3 bucket configuration option <code>client-authentication-mode</code> has been removed (defaulted to <code>REQUEST_SIGNING</code>).</li> <li>A new S3 bucket configuration option <code>request-signing-enabled</code> has been added (defaults to <code>true</code>).</li> </ul>"},{"location":"releases/#breaking-changes_2","title":"Breaking changes","text":"<ul> <li>See above for breaking changes to S3 bucket configurations. </li> </ul>"},{"location":"releases/#new-features_13","title":"New Features","text":"<ul> <li>Catalog/Trino: Add convenience REST endpoint to provide a starter Trino catalog configuration.   Use <code>/iceberg-ext/v1/client-template/trino?format=static</code> for Trino \u2018static\u2019 catalog configurations,   <code>/iceberg-ext/v1/client-template/trino?format=dynamic</code> for Trino \u2018dynamic\u2019 catalog configurations.   Please take a look at the Trino page for known   limitations in Trino.</li> <li>Catalog: The Iceberg REST header <code>X-Iceberg-Access-Delegation</code> is now respected. The functionality   depends on the S3 bucket configuration options <code>request-signing-enabled</code> and <code>assume-role-enabled</code>.</li> </ul>"},{"location":"releases/#changes_15","title":"Changes","text":"<ul> <li>Catalog: Only general object store configurations are returned via the <code>/iceberg/v1/config</code> endpoint.</li> <li>Catalog: Table specific options are returned for each individual table, including scoped-down S3   credentials, if applicable.</li> <li>The Nessie Spark SQL extensions are now based on the same syntax and options that are provided by the   Nessie CLI. A reference docs page for the Nessie Spark SQL command syntax was added to the web site.</li> </ul>"},{"location":"releases/#fixes_23","title":"Fixes","text":"<ul> <li>Declare the <code>contentType</code> variable for CEL Authorization rules.</li> <li>Catalog: Make Nessie time-travel functionality available to all use cases, including DDL.</li> </ul>"},{"location":"releases/#commits_45","title":"Commits","text":"<ul> <li>Quarkus 3.13.1 (#9236)</li> <li>Helm Chart: mention imagePullSecrets in values.yaml (#9292)</li> <li>Trino config-helper-endpoint &amp; web site updates (#9270)</li> <li>Catalog: split S3SessionsManager into several components (#9279)</li> <li>Catalog: Enable time-travel and branch/tag selection for Iceberg REST in all cases (#9219)</li> <li>Replace antlr w/ congocc grammar in SQL Extensions (#9256)</li> <li>Build / IntelliJ: include the project root dir in the IDE window name (#9281)</li> <li>Remove Windows CI (#9260)</li> <li>Fix illegal access (#9280)</li> <li>Catalog: expose <code>location</code> for namespaces, iam-policy per location (#9170)</li> <li>Nit: remove references to unused <code>pr-native</code> label (#9272)</li> <li>Release: make <code>publish-openapi</code> job idempotent (#9264)</li> <li>Add signer-keys service (#9239)</li> <li>Scala Compiler and how it disrespects things (#9261)</li> <li>Adopt to Iceberg dropping support for Java 8 (#9259)</li> <li>Blog: Polaris (#9257)</li> <li>Fix Gradle/Kotlin deprecation warning (#9255)</li> <li>Fix a Gradle deprecation (#9254)</li> <li>Fix <code>NOTICE</code> vs <code>LICENSE</code> confusion (#9250)</li> <li>Declare the <code>contentType</code> variable for CEL AuthZ rules. (#9251)</li> <li>Add ability to generate IAM policies (#9244)</li> <li>Protect Iceberg REST config endpoint (#9247)</li> <li>Nit: make some test profiles non-<code>final</code> (#9243)</li> <li>Rename <code>S3Clients.awsCredentialsProvider()</code> to make its usage clearer (#9242)</li> <li>Add <code>StorageUri.pathWithoutLeadingTrailingSlash()</code> (#9241)</li> <li>Add utility method to escape strings in IAMs (#9240)</li> </ul>"},{"location":"releases/#0944-release-august-01-2024","title":"0.94.4 Release (August 01, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_14","title":"New Features","text":"<ul> <li>Helm chart: liveness and readiness probes are now configurable via the <code>livenessProbe</code> and    <code>readinessProbe</code> Helm values.</li> </ul>"},{"location":"releases/#commits_46","title":"Commits","text":"<ul> <li>Replace deprecated quarkus properies (#9235)</li> <li>Fix NPE, will NPE w/ Quarkus 3.13.0 (#9233)</li> <li>Replace <code>quarkus-resteasy-reactive</code> with <code>quarkus-rest</code> (#9234)</li> <li>Helm chart: better document the purpose of the logLevel option (#9223)</li> <li>Gradle wrapper - download and verify (#9221)</li> <li>Fix legit scale=0 (#9214)</li> </ul>"},{"location":"releases/#0943-release-july-29-2024","title":"0.94.3 Release (July 29, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_15","title":"New Features","text":"<ul> <li>Helm chart: liveness and readiness probes are now configurable via the <code>livenessProbe</code> and    <code>readinessProbe</code> Helm values.</li> </ul>"},{"location":"releases/#commits_47","title":"Commits","text":"<ul> <li>Helm chart: configurable liveness and readiness probes (#9203)</li> <li>Dependency cleanup, remove dependency on <code>nessie-quarkus-common</code> (#9206)</li> <li>Ninja: fix release notes</li> </ul>"},{"location":"releases/#0942-release-july-26-2024","title":"0.94.2 Release (July 26, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#highlights_8","title":"Highlights","text":"<ul> <li>Helm chart: it is now possible to use Helm templating in all values; any built-in   object can be specified. This is   particularly useful for dynamically passing the namespace to the Helm chart, but cross-referencing   values from different sections is also possible, e.g.:</li> </ul> <pre><code>mongodb:\n  name: nessie\n  connectionString: mongodb+srv://mongodb.{{ .Release.Namespace }}.svc.cluster.local:27017/{{ .Values.mongodb.name }}\n</code></pre> <p>The above would result in the following properties when deploying to namespace <code>nessie-ns</code>:</p> <pre><code>quarkus.mongodb.database=nessie\nquarkus.mongodb.connection-string=mongodb://mongodb.nessie-ns.svc.cluster.local:27017/nessie\n</code></pre>"},{"location":"releases/#commits_48","title":"Commits","text":"<ul> <li>Renovate: let Trino pass (#9199)</li> <li>Helm chart: allow templating in chart values (#9202)</li> <li>Helm chart: configurable mount point for configmap (#9201)</li> <li>Docs: add warning about printing configuration (#9193)</li> </ul>"},{"location":"releases/#0941-release-july-25-2024","title":"0.94.1 Release (July 25, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#upgrade-notes_3","title":"Upgrade notes","text":"<ul> <li>Helm chart: the <code>logLevel</code> configuration option now only sets the log level for the console and   file appenders, but does not change the <code>io.quarkus</code> logger level anymore. To actually modify a   logger level, use the <code>advancedConfig</code> section and set the   <code>quarkus.log.category.\"&lt;category&gt;\".level</code> configuration option, e.g.   <code>quarkus.log.category.\"io.quarkus\".level=DEBUG</code> would set the log level for the <code>io.quarkus</code>   logger to <code>DEBUG</code>, effectively achieving the same as setting <code>logLevel</code> to <code>DEBUG</code> in previous   versions.</li> </ul>"},{"location":"releases/#commits_49","title":"Commits","text":"<ul> <li>Quarkus: split out code from <code>:nessie-quarkus</code> into separate modules (#9189)</li> <li>Include LoggingConfigSourceInterceptor to help diagnose configuration issues (#9190)</li> <li>Helm chart: don\u2019t set quarkus.log.level (#9191)</li> </ul>"},{"location":"releases/#0940-release-july-25-2024","title":"0.94.0 Release (July 25, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#upgrade-notes_4","title":"Upgrade notes","text":"<ul> <li>Helm chart: the <code>service.port</code> configuration option has been deprecated by <code>service.ports</code>, which   is a map of port names to port numbers to expose on the service. The old <code>service.port</code> is still   supported, but will be removed in a future release.</li> </ul>"},{"location":"releases/#breaking-changes_3","title":"Breaking changes","text":"<ul> <li>Catalog/ADLS: The authorization type for ADLS file systems must be explicitly specified using the new   <code>auth-type</code> configuration option. Valid values are: <code>STORAGE_SHARED_KEY</code>, <code>SAS_TOKEN</code> or <code>APPLICATION_DEFAULT</code>   (new, container/pod credentials) or <code>NONE</code> (new, default, anonymous access).</li> <li>Helm chart/ADLS: The authorization type for ADLS file systems must be explicitly specified using the new   <code>catalog.storage.adls.defaultOptions.authType</code> configuration option (overridable on a per-filesystem basis).   Valid values are the same as above, the default is <code>NONE</code>.</li> </ul>"},{"location":"releases/#new-features_16","title":"New Features","text":"<ul> <li>Catalog/ADLS: Add mandatory <code>auth-type</code> configuration option for ADLS file systems, see above.</li> <li>GC: Iceberg view metadata is now expired as well.</li> </ul>"},{"location":"releases/#fixes_24","title":"Fixes","text":"<ul> <li>All application-level errors like \u201ccontent not found\u201d were logged at <code>INFO</code> level with a stack trace.</li> <li>Catalog: Fixed a potential fallback to the default auth-mode for S3 and GCS buckets.</li> <li>JDBC backend: fix a potential infinite recursion when creating/checking the required tables.</li> </ul>"},{"location":"releases/#commits_50","title":"Commits","text":"<ul> <li>Helm chart: ability to change or add container ports (#9183)</li> <li>Ninja: changelog</li> <li>Add java client-based test for committing a UDF (#9182)</li> <li>Initial Trino integration testing (#9160)</li> <li>CLI: Add s3/adls/gcs file-io impls to CLI (#9177)</li> <li>Nit: simplify <code>normalizeBuckets()</code> (#9179)</li> <li>Catalog/data-access: propagate <code>X-Iceberg-Access-Delegation</code> header (#9178)</li> <li>Add utility to create an OAuth2 token (#9176)</li> <li>Minio test resource: way to expose S3 endpoint (#9175)</li> <li>Fix missing OAuth token for GCS IT (#9174)</li> <li>Load reference info before processing commits during export (#9168)</li> <li>Remove OIDC from dependencies of nessie-catalog-service-rest (#9171)</li> <li>Bump timeout in release workflows (#9172)</li> <li>Catalog: Don\u2019t log every client error using <code>INFO</code> log level (#9162)</li> <li>Doc-gen-tool: support HTML entities in Javadoc (#9169)</li> <li>Remove <code>@WithDefault</code> annotations (#9165)</li> <li>Limit recursion when creating tables in <code>JdbcBackend</code> (#9163)</li> <li>ninja: changelog</li> <li>GC: Support Iceberg Views (#9074)</li> <li>Helm chart: add support for ADLS auth type (#9158)</li> <li>Ninja: changelog</li> <li>Add <code>APPLICATION_DEFAULT</code> authorization type for ADLS Gen2 (#9086)</li> <li>Site: libraries note typo (#9152)</li> <li>Remove superfluous badge from README (#9150)</li> <li>Site-gen: remove backup files generated by <code>sed</code> (#9151)</li> </ul>"},{"location":"releases/#0931-release-july-19-2024","title":"0.93.1 Release (July 19, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#breaking-changes_4","title":"Breaking changes","text":"<ul> <li>The <code>throttled-retry-after</code> advanced configuration property was renamed from   <code>nessie.catalog.service.s3.throttled-retry-after</code> to   <code>nessie.catalog.error-handling.throttled-retry-after</code>. The old property name is ignored.</li> <li>Helm chart: a few ADLS-specific options under <code>catalog.storage.adls</code> were incorrectly placed and   therefore effectively ignored by Nessie; if you are using ADLS, please re-check your configuration   and adjust it accordingly.</li> </ul>"},{"location":"releases/#new-features_17","title":"New Features","text":"<ul> <li>CLI: New <code>REVERT CONTENT</code> command to update one or more tables or views to a previous state.</li> </ul>"},{"location":"releases/#changes_16","title":"Changes","text":"<ul> <li>Catalog: ADLS + GCS credentials are no longer sent to the client. It is considered insecure to expose the   server\u2019s credentials to clients, even if this is likely very convenient. Unless we have a secure mechanism   to provide per-client/table credentials, users have to configure object store credentials when using GCS or   ADLS via the local Iceberg configuration(s).</li> </ul>"},{"location":"releases/#fixes_25","title":"Fixes","text":"<ul> <li>GC: Fix behavior of cutoff policy \u201cnum commits\u201d, it was \u2018off by one\u2019 and considered the n-th commit as non-live   vs the n-th commit as the last live one.</li> <li>GC: Record failed \u201csweep\u201d/\u201dexpire\u201d runs in the repository. Before this fix, failures were reported on the console.</li> <li>GC: Fix handling of broken manifest files written by pyiceberg up to 0.6.1</li> <li>Catalog/ADLS: Don\u2019t let endpoint default to warehouse/object-store URI</li> <li>Catalog/ADLS: More informative error message if mandatory <code>endpoint</code> is missing.</li> <li>Catalog/ADLS: Use a less restrictive endpoint in the \u2018ObjectIO.ping\u2019 function used for health checks.</li> </ul>"},{"location":"releases/#commits_51","title":"Commits","text":"<ul> <li>Revert \u201c[release] release nessie-0.93.0\u201d, but keep version</li> <li>Build: fix annotation types warnings (#9148)</li> <li>CI: Use Java 21 for CI (#9140)</li> <li>Release workflow: Let helm chart publishing depend on images publishing (#9142)</li> <li>Bump undertow from 2.2.28 to 2.2.33 (#9143)</li> <li>Bump Spark 3.4 from 3.4.2 to 3.4.3 (#9144)</li> <li>Bump Scala 2.13 from 2.13.13 to 2.13.14 (#9145)</li> <li>Revert \u201c[release] release nessie-0.93.0\u201d</li> <li>Add an Operating System check (#9139)</li> <li>Cache invalidations: move code to <code>:nessie-quarkus</code> (#9137)</li> <li>Catalog: decouple bucket name from bucket config key (#9116)</li> <li>Catalog/ADLS: change \u2018ping\u2019 endpoint (#9134)</li> <li>Add exception mappers to convert storage failures to Iceberg REST client exceptions (#8558)</li> <li>CLI: Nicer syntax rendering (#9119)</li> <li>CLI: Add <code>REVERT CONTENT</code> command (#9120)</li> <li>Catalog: update <code>IcebergManifestFileReader</code> to handle broken manifest files (#9132)</li> <li>Reference caching: update default for negative, update comments/docs (#9126)</li> <li>Catalog/ADLS: More information if manadory ADLS endpoint is missing (#9128)</li> <li>ninja: changelog</li> <li>GC: Manifest file reading with <code>specById</code> (#9131)</li> <li>Site: notes on Nessie server sizing + tips (#9127)</li> <li>Catalog / GCS: minor enhancements (#9107)</li> <li>Site: dynamo db note (#9113)</li> <li>GC: Record expiry exception in repository + record stack trace as well (#9114)</li> <li>Catalog: Don\u2019t expose ADLS + GCS credentials (#9100)</li> <li>Renovate: automerge action updates (#9106)</li> <li>Catalog: Accept object-store locations w/o trailing <code>/</code> (#9098)</li> <li>Catalog/ADLS: Don\u2019t let endpoint default to warehouse/object-store URI (#9102)</li> <li>Add <code>message</code> argument to <code>Objects.requireNonNull()</code> (#9099)</li> <li>GC: Fix behavior of cutoff policy \u201cnum commits\u201d, \u2018off by one\u2019 (#9096)</li> <li>Site: Fix links to nessie-bom (#9088)</li> </ul>"},{"location":"releases/#0921-release-july-13-2024","title":"0.92.1 Release (July 13, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#fixes_26","title":"Fixes","text":"<ul> <li>Catalog: fix field-ID reassignment and last-column-id calculation</li> </ul>"},{"location":"releases/#commits_52","title":"Commits","text":"<ul> <li>Catalog: Fix assignment of nested fields in a table\u2019s initial schema (#9085)</li> <li>Update docs for CEL Authorization rules (#9078)</li> <li>HTTP client: consistently close streams and responses (#9082)</li> <li>HTTP client: remove hard-coded behavior for status &gt;= 400 (#9071)</li> <li>Gradle 8.9 post updates (#9075)</li> <li>Use released Nessie image in docker-compose examples (#9070)</li> </ul>"},{"location":"releases/#0920-release-july-11-2024","title":"0.92.0 Release (July 11, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#breaking-changes_5","title":"Breaking changes","text":"<ul> <li>Catalog: The <code>nessie.catalog.s3.default-options.auth-mode</code> configuration property has been renamed   to <code>nessie.catalog.s3.default-options.client-auth-mode</code> to better reflect its purpose. The old   property name is not supported anymore and must be updated in customized Helm values and/or    Quarkus configurations.</li> </ul>"},{"location":"releases/#new-features_18","title":"New Features","text":"<ul> <li>Catalog: Exported Nessie repositories now include the contents for Nessie Catalog</li> <li>Catalog: Improve indicated health check errors</li> <li>Catalog/GCS: Support using the default application credentials</li> <li>Catalog/S3: Allow custom key+trust stores</li> <li>Catalog: Check privileges earlier</li> <li>Catalog: cleanup non-committed metadata objects</li> </ul>"},{"location":"releases/#changes_17","title":"Changes","text":"<ul> <li>Helm chart improvements</li> </ul>"},{"location":"releases/#fixes_27","title":"Fixes","text":"<ul> <li>Fix potential class-loader deadlock via <code>Namespace.EMPTY</code></li> <li>Catalog: Fix double write of metadata objects to S3</li> <li>GC/ADLS: Handle <code>BlobNotFound</code> as well</li> <li>Fix behavior of metadata-update/set-statistics + set-partition-statistics</li> <li>Fix duplicate OAuth interactive flows when the Nessie API compatibility filter is enabled</li> </ul>"},{"location":"releases/#commits_53","title":"Commits","text":"<ul> <li>Fix Iceberg REST \u201cgetting started\u201d examples (#9065)</li> <li>HTTP client: refactor BaseHttpRequest (#9055)</li> <li>Persist: make GenericObj implement Obj (#9059)</li> <li>Remove superfluous <code>\\</code> in CLI welcome message (#9066)</li> <li>Make <code>o.p.client.http.Status</code> safe against unknown HTTP status codes (#9062)</li> <li>Export related objects (#9034)</li> <li>Catalog S3: use DefaultCredentialsProvider if no access key configured (#8987)</li> <li>nit: fix typo in changelog (#9051)</li> <li>Compatibility filter: bypass filters when contacting the config endpoint (#9050)</li> <li>Add ability to (de)serialize generic custom <code>ObjType</code>s (#9032)</li> <li>Refactor Nessie HTTP compatibility filter (#9047)</li> <li>Fix Docker Compose ports (#9046)</li> <li>Fix behavior of metadata-update/set-statistics + set-partition-statistics (#9045)</li> <li>SQL Extension SHOW LOG with AT (#8294)</li> <li>Committing operations: cleanup metadata after failures (#8889)</li> <li>Ability to identify related objects for Nessie export (#9033)</li> <li>Docs: Update REST catalog config for Trino (#9016)</li> <li>ninja: changelog</li> <li>Fix concurrency issue in flaky test <code>TestIceberg*Files.iceberg()</code> (#9039)</li> <li>Allow Smallrye config sections to have no properties (#9023)</li> <li>Catalog: Improve warehouse health/readiness checks (#9036)</li> <li>Add at least some reason to <code>Optional.orElseThrow()</code> (#9037)</li> <li>Nit: move EntityObj ID calculation from impl class (#9030)</li> <li>Unify obj-type namespaces (#9031)</li> <li>Fix older-client/server dependency resolution errors (#9029)</li> <li>Implement early access-checks for Iceberg REST operations (#8768)</li> <li>Export: remove ability to export a new repo using the legacy export version (#9017)</li> <li>GC/ADLS: Handle BlobNotFound (#9015)</li> <li>ninja: changelog</li> <li>Catalog/S3: Allow custom trust and key stores (#9012)</li> <li>Ninja: changelog</li> <li>Ninja: changelog updates</li> <li>Catalog/Bug: Fix double-write for S3 (#9008)</li> <li>Support using the default application credentials for google client (#9004)</li> <li>Docs: improve tabbed display of code blocks (#9006)</li> <li>Helm chart: fix advanced config examples (#9005)</li> <li>Refactor <code>MultiTableUpdate</code> (#9001)</li> <li>Exclude DNS test on macOS (#9003)</li> <li>Add <code>for-write</code> query parameter to Nessie API v2 (#8993)</li> <li>Move <code>AddressResolve</code> to a separate module, so it\u2019s reusable (#8992)</li> <li>Add initial unit tests for <code>CatalogServiceImpl</code> (#8998)</li> <li>Make address resolution more resilient (#8984)</li> <li>Add <code>returnNotFound</code> for <code>VersionStore.getValue(s)</code> (#8983)</li> <li>Adopt to change from Gradle 8.0 to 8.1 (#8985)</li> <li>Helm chart: don\u2019t call template if feature is disabled (#8982)</li> <li>Add <code>ObjectIO.deleteObjects()</code> (#8981)</li> <li>Site: add docs on Kubernetes troubleshooting (#8966)</li> <li>Fix potential class-loader deadlock via <code>Namespace.EMPTY</code> (#8963)</li> <li>Clean site directories and built-tools-IT directories during <code>clean</code> (#8969)</li> <li>Fix more IJ inspections (#8962)</li> </ul>"},{"location":"releases/#0913-release-june-28-2024","title":"0.91.3 Release (June 28, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#breaking-changes_6","title":"Breaking changes","text":"<ul> <li>The config properties for the object storage defaults for S3, GCS and ADLS have been moved under the   <code>default-options</code> composite object. Inconsistent property names between the old defaults and the per-bucket   property names have been resolved.</li> </ul>"},{"location":"releases/#new-features_19","title":"New Features","text":"<ul> <li>Nessie CLI now has its own Docker images. Running Nessie CLI is now as simple as: <code>docker run -it   ghcr.io/projectnessie/nessie-cli</code>. Read more about it   here.</li> </ul>"},{"location":"releases/#fixes_28","title":"Fixes","text":"<ul> <li>Fix console output during <code>CONNECT</code> in CLI/REPL (the bug was introduced in 0.91.1)</li> </ul>"},{"location":"releases/#commits_54","title":"Commits","text":"<ul> <li>Fix a bunch of IntelliJ inspections, rather nits + cosmetic (#8955)</li> <li>nit: fix typo in NessieConfigConstants (#8959)</li> <li>Site: display download options for each artifact in tabs (#8946)</li> <li>Just add comments when adding new images. (#8958)</li> <li>Nessie CLI: publish Docker images (#8935)</li> <li>Fix STDOUT in <code>ConnectCommand</code> (#8953)</li> <li>Remove unused <code>Cloud</code> enum (#8951)</li> <li>Fix flaky <code>TestCacheInvalidationSender.regularServiceNameLookups</code> (#8950)</li> <li>Helm chart: remove build-time property quarkus.log.min-level (#8948)</li> <li>Helm chart: fix catalogStorageEnv template (#8944)</li> <li>Clarify OpenAPI doc for updateRepositoryConfig (#8945)</li> <li>Migrate default bucket options to <code>default-options</code> composite property (#8933)</li> <li>Site: new placeholders for nightly version, tag and unstable suffix (#8939)</li> <li>Replace usage of <code>Project.extra</code> for <code>PublishingHelperPlugin</code> + explicitly disable Gradle config cache (#8942)</li> <li>create-gh-release-notes.sh: don\u2019t include h1 title in release notes (#8938)</li> <li>Site: nicer word-break for config references (#8936)</li> <li>Site / downloads fixes + reorg (#8934)</li> <li>Follow-up of stream-to-list refactor (#8931)</li> <li>OAuth2: nit: clarify trust relationship required for impersonation (#8928)</li> <li>GH WFs: Fix comment + job parameter description (#8924)</li> </ul>"},{"location":"releases/#0912-release-june-24-2024","title":"0.91.2 Release (June 24, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#breaking-changes_7","title":"Breaking changes","text":"<ul> <li>We have improved Nessie client\u2019s support for impersonation scenarios using the token exchange   grant type. A few options starting with <code>nessie.authentication.oauth2.token-exchange.*</code> were   renamed to <code>nessie.authentication.oauth2.impersonation.*</code>. Check the [Nessie authentication   settings] for details. Please note that token exchange and impersonation are both considered in   beta state. Their APIs and configuration options are subject to change at any time.</li> </ul>"},{"location":"releases/#commits_55","title":"Commits","text":"<ul> <li>OAuth2Client: differentiate token exchange from impersonation (#8847)</li> <li>Migrate off deprecated <code>AwsS3V4Signer</code> to <code>AwsV4HttpSigner</code> (#8923)</li> <li>OAuthClient: improve next token refresh computation (#8922)</li> <li>Docker compose: expose Nessie management port (#8888)</li> <li>Site: update headers to appear in ToC (#8916)</li> </ul>"},{"location":"releases/#0911-release-june-22-2024","title":"0.91.1 Release (June 22, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_20","title":"New Features","text":"<ul> <li>Nessie\u2019s metrics now support custom, user-defined tags (dimensional labels). To    define a custom tag, set the <code>nessie.metrics.tags.&lt;tag-name&gt;=&lt;tag-value&gt;</code> configuration property.   Such tags are added to all metrics published by Nessie.</li> <li>Readiness/health check testing the general availability of the object stores configured for the warehouses.</li> <li>Helm chart with support for Iceberg REST</li> </ul>"},{"location":"releases/#fixes_29","title":"Fixes","text":"<ul> <li>S3 request signing, when using Iceberg REST, did not work with Iceberg (Java) before 1.5.0. Iceberg   S3 request signing before 1.5.0 works now. </li> <li>Fix service-name resolution in k8s, spamming the Nessie log.</li> </ul>"},{"location":"releases/#commits_56","title":"Commits","text":"<ul> <li>Fix flaky tests in <code>:nessie-gc-iceberg-files</code> (#8913)</li> <li>Fix patch version</li> <li>Ninja: bump sonatype timeout + bump snapshot-version for next release</li> <li>Revert release 0.91.0</li> <li>CI: split Quarkus intTest jobs (#8898)</li> <li>Site: exchange \u201cblack\u201d Nessie (#8908)</li> <li>Fix Helm chart CI (#8900)</li> <li>NInja: changelog</li> <li>Possibly fix Java stream issuue w/ name resolution (#8899)</li> <li>Expose object store(s) availability as a readiness check (#8893)</li> <li>ADLS: advice against shared account name/key (#8895)</li> <li>Use correct IcebergView.of (#8896)</li> <li>Update nessie-IDs in returned metadata properties, simplify some code (#8879)</li> <li>Catalog/ADLS: Expose the endpoint and sas-token using the correct Iceberg properties (#8894)</li> <li>Add admin command: delete-catalog-tasks (#8869)</li> <li>TestOAuth2Authentication: fix flaky test (#8892)</li> <li>Metrics: support user-defined tags (#8890)</li> <li>Prevent console spam when connecting to a non-Iceberg-REST endpoint (#8885)</li> <li>Add the nice Nessie images (#8880)</li> <li>Add docs and docker-compose illustrating the use of a reverse proxy (#8864)</li> <li>Helm chart: support for REST Catalog (#8831)</li> <li>Docker compose: add examples with Prometheus and Grafana (#8883)</li> <li>Fix heading in index-release.md (#8881)</li> <li>Bump regsync from 0.4.7 to 0.6.1 (#8873)</li> <li>Fix test failure in <code>TestResolvConf</code> in macOS CI job (#8874)</li> <li>Make S3 signing work with Iceberg before 1.5.0 (#8871)</li> <li>Relax image registry sync (#8872)</li> <li>Disable Iceberg GC by default (#8870)</li> <li>AdlsConfig: remove unused option (#8868)</li> <li>Support v2 iceberg tables with gzip (#8848)</li> <li>OAuth2 client: improve error reporting (#8861)</li> <li>Use <code>search</code> in <code>resolv.conf</code> to resolve service names (#8849)</li> <li>REST Catalog: disable metrics by default (#8853)</li> <li>Proper <code>N commits since x.y.z</code> in release notes (#8823)</li> <li>Fix release-publish (#8822)</li> <li>Attempt to fix flaky test #8819 (#8821)</li> <li>Increase test worker heap size (#8820)</li> </ul>"},{"location":"releases/#0904-release-june-13-2024","title":"0.90.4 Release (June 13, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_21","title":"New Features","text":"<ul> <li>Support for token exchange in the Nessie client has been completely redesigned. The new API and   configuration options are described in the [Nessie authentication settings]. If this feature is   enabled, each time a new access token is obtained, the client will exchange it for another one by   performing a token exchange with the authorization server. We hope that this new feature will   unlock many advanced use cases for Nessie users, such as impersonation and delegation. Please note   that token exchange is considered in beta state and both the API and configuration options are   subject to change at any time; we appreciate early feedback, comments and suggestions.</li> </ul>"},{"location":"releases/#commits_57","title":"Commits","text":"<ul> <li>Skip 0.90.3</li> <li>Fix compilation failure (#8817)</li> <li>Revert commits for 0.90.3 release</li> <li>JDBC: add proper support for H2 (#8751)</li> <li>Fix cache-deserialization for <code>UpdateableObj</code> (#8815)</li> <li>OAuth2Client: refactor scopes (#8814)</li> <li>Token exchange: documentation enhancements (#8813)</li> <li>Token exchange: more flexible subject / actor configuration (#8812)</li> <li>OAuth2Client: integration tests for token exchange (#8810)</li> <li>Fix docker-compose files after Keycloak upgrade to 25.0 (#8804)</li> <li>HTTP client: introduce specialized HttpClientResponseException (#8806)</li> <li>OAuth2Client: internal code cleanup (#8807)</li> <li>OAuthClient: configurable subject and actor token types (#8805)</li> <li>OAuth2Client: introduce proper support for token exchange (#8803)</li> <li>OAuth2Client: don\u2019t refresh tokens using token exchange (#8790)</li> <li>Disable a test on macOS (#8799)</li> <li>Helm chart: minor improvement to JDBC section in docs (#8717)</li> <li>Nessie w/ Iceberg REST / post-release site changes (#8510)</li> <li>Ninja: fix release-create workflow issues in \u201ccreate github release\u201d job</li> </ul>"},{"location":"releases/#0902-release-june-11-2024","title":"0.90.2 Release (June 11, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#highlights_9","title":"Highlights","text":"<ul> <li>Nessie has got support for Iceberg REST.</li> <li>MySQL users can now configure the JDBC connection using the <code>quarkus.datasource.mysql.*</code>   properties. Also, JDBC URLs can now use the <code>mysql</code> prefix, e.g.   <code>jdbc:mysql://example.com:3306/my_db</code>.</li> </ul>"},{"location":"releases/#upgrade-notes_5","title":"Upgrade notes","text":"<ul> <li>Helm chart: the <code>cassandra</code> section now has support for pulling credentials from a secret. The   old <code>cassandra.auth.username</code> and <code>cassandra.auth.password</code> properties still work, but are now   deprecated and will be removed in a future release. Use the <code>cassandra.secret</code> property instead.</li> </ul>"},{"location":"releases/#new-features_22","title":"New Features","text":"<ul> <li>Support for Iceberg REST is in \u201cbeta\u201d state. We appreciate early feedback, comments and suggestions.   Take a look at the Guides and Docs   on our web site projectnessie.org for more information.</li> <li>CEL access check scripts now receive the variable <code>roles</code> that can be used to check whether the current   principal has a role assigned using a CEL expression like <code>'rolename' in roles</code>.</li> </ul>"},{"location":"releases/#deprecations_1","title":"Deprecations","text":"<ul> <li>Support for Java 8 is officially deprecated and users are encouraged to upgrade all clients to   at least Java 11, better Java 17 or 21, if possible. Current Spark versions 3.3, 3.4 and 3.5    work with Java 11 and 17. Support for Java 8 will eventually be removed.</li> <li>For JDBC version stores, the following settings, which never worked as expected, are now    deprecated and will be removed in a future release. The catalog and the schema must always be   specified explicitly in the JDBC URL.</li> <li><code>nessie.version.store.persist.jdbc.catalog</code></li> <li><code>nessie.version.store.persist.jdbc.schema</code></li> </ul>"},{"location":"releases/#fixes_30","title":"Fixes","text":"<ul> <li>A bug in the API compatibility filter has been discovered and fixed: when OAuth2 authentication is   being used, the filter causes the OAuth2 client to close prematurely, thus triggering unauthorized   errors. A workaround is to simply disable the filter (set <code>nessie.enable-api-compatibility-check</code>    to <code>false</code>), but this is no longer necessary.</li> </ul>"},{"location":"releases/#commits_58","title":"Commits","text":"<ul> <li>Manually bump to 0.90.1-SNAPSHOT for next release</li> <li>Revert \u201c[release] release nessie-0.90.1\u201d</li> <li>Another release-publish bug</li> <li>Manually bump to 0.90.1</li> <li>Revert \u201c[release] release nessie-0.90.0\u201d</li> <li>Revert 0.90.0 commits</li> <li>Ninja: fix</li> <li>Revert 0.90.0 release commits</li> <li>Ninja: Fix Swaggerhub</li> <li>Prepare bump to v0.90.0 (#8792)</li> <li>Persist: introduce type-safe bulk-fetch functions (#8763)</li> <li>Persist-cache: add negative obj cache (#8762)</li> <li>Remove NessieDockerTestResourceLifecycleManager (#8788)</li> <li>Catalog tests: disable token refresh (#8786)</li> <li>OAuth2Client: fix race condition when scheduling refreshes (#8787)</li> <li>OAuth2 client: add support for RFC 8414 metadata discovery (#8789)</li> <li>Support distributed invalidation for <code>Persist</code> cache (#8463)</li> <li>API compatibility filter: don\u2019t share authentication with main client (#8769)</li> <li>[Catalog] Restrict S3 signing to URIs within table base prefix (#8654)</li> <li>Persist: add <code>fetchObjsIfExist()</code> (#8761)</li> <li>Refactor Backend.setupSchema to return schema info (#8753)</li> <li>Add workaround for nip.io for macOS, optimize for Linux (#8759)</li> <li>Cleanup nessie-quarkus build file (#8757)</li> <li>[Catalog] Recognize <code>s3a</code> and <code>s3n</code>, behave like <code>s3</code> (#8755)</li> <li>Expose \u2018roles\u2019 in CEL access check scripts (#8752)</li> <li>Site: Add RSS+JSON feeds (#8754)</li> <li>Docs-gen: more documented types, don\u2019t expose \u201cJava types\u201d (#8705)</li> <li>Allow secrets in a Keystore (#8710)</li> <li>[Catalog] Secrets suppliers/managers API (#8706)</li> <li>JDBC: remove options \u2018catalog\u2019 and \u2018schema\u2019 and revisit schema creation (#8723)</li> <li>Helm chart: add support for OIDC client secret (#8750)</li> <li>Site: Clarify \u201cAuthorization\u201d and remove confusing paragraph (#8749)</li> <li>Helm chart: fix a few property names (#8743)</li> <li>Add explicit authorization-type = CEL (#8746)</li> <li>Add <code>isAnonymous()</code> to <code>AccessContext</code> (#8745)</li> <li>Add a JSON view to <code>SmileSerialization</code> (#8744)</li> <li>Helm chart: reload pods when configmap changes (#8725)</li> <li>Replace <code>:latest</code> with versions in docker-compose files (#8724)</li> <li>Nit: fix docker-compose paths (#8722)</li> <li>Avoid invalid test names due to non-printable characters (#8721)</li> <li>Helm chart / Cassandra: add support for pulling credentials from secrets (#8716)</li> <li>Helm chart: fix wrong env var + nits (#8718)</li> <li>Helm chart: mention removal of legacy store types in 0.75.0 (#8719)</li> <li>Fix API examples for merge conflict responses (#8712)</li> <li>Prevent Quarkus warning about unindexed dependency (#8711)</li> <li>Helm chart: create ConfigMap for application.properties file (#8709)</li> <li>[Catalog] S3CredentialsResolver: remove one second from session end (#8703)</li> <li>Update URLs in ConfigChecks (#8704)</li> <li>Do not pin digests for Dockerfiles and docker-compose files (#8689)</li> <li>Strip leading <code>/</code> (only for S3) (#8681)</li> <li>[Catalog] Implement paging for Iceberg REST (#8633)</li> <li>Adjust all <code>logback*.xml</code> files to new syntax (#8678)</li> <li>Testing: less console log during BigTable tests (#8677)</li> <li>Helm chart: add support for JDBC catalog and schema (#8672)</li> <li>GC: fix incorrect paths when listing prefixes with ADLS (#8661)</li> <li>[Catalog] return metadata-location in object store (at least for now) (#8669)</li> <li>[Catalog] Handle <code>CompletionStage</code> from snapshot store (#8667)</li> <li>[Catalog] Remove unused parameters/code (#8666)</li> <li>Deprecate support for Java 8 (#8650)</li> <li>[Catalog] Simplify tokens-uri resolution (#8649)</li> <li>[Catalog] update CHANGELOG (#8651)</li> <li>README: remove link to demos repository (#8652)</li> <li>Un-ignore SetLocation directives (#8646)</li> <li>[Catalog] remove Google projectId for S3 (#8635)</li> <li>Ninja: fix renovate config</li> <li>Renovate: more automatic merges (#8640)</li> <li>Ninja: rename snapshot-publish workflow</li> <li>[FEATURE] Nessie integration of Iceberg REST (#7043)</li> <li>Authelia tests: remove nip.io (#8632)</li> <li>Split Release workflow job into multiple, retryable jobs (#8596)</li> <li>Custom keycloak container: add function to create a bearer token (#8622)</li> <li>OAuth2 client: add integration tests with Authelia (#8618)</li> <li>Docker compose: add example with Authelia (#8619)</li> <li>Fix jaegertracing image spec (#8615)</li> <li>Fix downloads on site + publish CLI uber jar (#8595)</li> <li>Fix incremental generation of site/docs (#8591)</li> <li>Separate MariaDB and MySQL datasources (#8585)</li> <li>Clean up failed 0.83.0 + 0.83.1 releases (#8590)</li> <li>Snapshot-Publishing: job splitting for snapshot releases (#8589)</li> <li>Renovate: there\u2019s more than just <code>dockerfile</code> (#8586)</li> <li>Remove deprecated <code>HttpClientBuilder</code> (#7803)</li> </ul>"},{"location":"releases/#0832-release-may-23-2024","title":"0.83.2 Release (May 23, 2024)","text":"<p>See Release information on GitHub.</p> <p>(Note: the 0.83.1 and 0.83.0 versions failed to fully release all artifacts for technical reasons.)</p>"},{"location":"releases/#highlights_10","title":"Highlights","text":"<ul> <li>New Nessie CLI tool + REPL, replacing the old Python based CLI, based on Java.   SQL-ish syntax, built-in online <code>HELP</code> command, auto-completion of commands, keywords   and reference names, syntax highlighting, paging of long results, command history.</li> <li>Nessie now includes built-in support for MariaDB, with full compatibility with MySQL servers. New   users wishing to try MariaDB (or MySQL) should:   1. Specify the new configuration property: <code>nessie.version.store.persist.jdbc.datasource=mariadb</code>;   2. Provide all the MariaDB (or MySQL) connection details using <code>quarkus.datasource.mariadb.*</code>      configuration properties.</li> <li>The Nessie GC tool is now also compatible with MariaDB and MySQL (using the MariaDB connector).</li> <li>The Nessie Server Admin tool is now also compatible with MariaDB and MySQL (using the MariaDB   connector).</li> </ul>"},{"location":"releases/#upgrade-notes_6","title":"Upgrade notes","text":"<ul> <li>Due to the newly-introduced support for MariaDB, existing PostgreSQL users can continue to use   their current JDBC configuration, but are encouraged to update it as follows:   1. Specify the new configuration property:       <code>nessie.version.store.persist.jdbc.datasource=postgresql</code>;   2. Migrate any property under <code>quarkus.datasource.*</code> to <code>quarkus.datasource.postgresql.*</code>. Support      for the old <code>quarkus.datasource.*</code> properties will be removed in a future release.</li> <li>For the same reason, the Nessie Helm chart has been updated. The old <code>postgres</code> section is now   called <code>jdbc</code>. Existing Helm chart configurations should be updated accordingly, e.g.   <code>postgres.jdbcUrl</code> now becomes <code>jdbc.jdbcUrl</code>. Although the old <code>postgres</code> section is still   honored, it won\u2019t be supported in future releases. The right datasource will be chosen based on   the <code>jdbcUrl</code> contents.</li> </ul>"},{"location":"releases/#breaking-changes_8","title":"Breaking changes","text":"<ul> <li><code>nessie-quarkus-cli</code>, the low-level tool to for example export/import Nessie repositories, has been renamed   to <code>nessie-server-admin-tool</code>.</li> </ul>"},{"location":"releases/#new-features_23","title":"New Features","text":"<ul> <li>More verbose exceptions from Nessie GC.</li> </ul>"},{"location":"releases/#commits_59","title":"Commits","text":"<p>(Note: the 0.83.1 and 0.83.0 versions failed to fully release all artifacts for technical reasons, this list of commits contains all commits for 0.83.0, 0.83.1 and 0.83.2.)</p> <ul> <li>Ninja: fix GH release-create</li> <li>New CLI: Post-release site updates (#8468)</li> <li>Blog post: support for MariaDB and MySQL backends (#8577)</li> <li>Post-release: Rename nessie-quarkus-cli to nessie-server-admin-tool in docs (#8484)</li> <li>Nessie Server Admin Tool: add support for MariaDB and MySQL backends (#8548)</li> <li>Helm chart: add support for MariaDB and MySQL backends (#8554)</li> <li>Persistence: properly handle timeout-ish exceptions (#8533)</li> <li>Renovate: merge \u201cdigest\u201d updates automatically (#8576)</li> <li>Nessie GC: add support for MariaDB and MySQL backends (#8545)</li> <li>Nessie server: add support for MariaDB and MySQL backends (#8544)</li> <li>UDF type: additional changes (#8560)</li> <li>Tests/Scylla: Cap SMP to \u2153 of num-CPUs (#8559)</li> <li>Testing: Centralize image resolution (#8546)</li> <li>Always close BigTable clients (#8549)</li> <li>Cassandra: explicitly specify statement idempotence (#8557)</li> <li>Refactor BackendTestFactory (#8553)</li> <li>JDBC persist: properly handle SUCCESS_NO_INFO and EXECUTE_FAILED (#8551)</li> <li>Persistence layer: add support for MariaDB and MySQL backends (#8483)</li> <li>Renovate: add some recommended extensions (#8534)</li> <li>Add congocc license to <code>NOTICE</code> (#8540)</li> <li>Hide <code>namespace-validation</code> setting in docs (#8535)</li> <li>Nit: fix Util.isHexChar() (#8528)</li> <li>Experimental ability to cache <code>Reference</code>s (#8111)</li> <li>Changelog / Server-Admin-Tool (#8515)</li> <li>Nessie client: optionally disable certificate verifications (#8506)</li> <li>Blog: Nessie Catalog announcement</li> <li>Publish docker images of the server admin tool (#8507)</li> <li>Verify BSD+MIT+Go+UPL+ISC license mentions in <code>NOTICE</code> file + expose in Nessie server, GC-tool, Admin-Tool and CLI/REPL (#8498)</li> <li>Use <code>ubi9/openjdk-21-runtime</code> instead of <code>ubi9/openjdk-21</code> as the base image (#8503)</li> <li>Rename nessie-quarkus-cli to nessie-server-admin-tool (#8482)</li> <li>Add license reports and checks (#8497)</li> <li>Bump slf4j to 1.7.36/2.0.12 + logback to 1.3.14/1.5.6 (#6536)</li> <li>Add GC tool help to site and enhance GC tool help (#8447)</li> <li>Update <code>UDF</code> + <code>IcebergView</code>content types (#8478)</li> <li>Let renovate merge all google-cloud-cli Docker tag bumps (#8472)</li> <li>New Java based Nessie CLI tool + REPL (#8348)</li> <li>GC: more verbose error messages (#8467)</li> <li>build-push-images.sh: remove unused \u201cartifacts\u201d parameter (#8464)</li> </ul>"},{"location":"releases/#0820-release-may-06-2024","title":"0.82.0 Release (May 06, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#breaking-changes_9","title":"Breaking changes","text":"<ul> <li>The readiness, liveness, metrics and Swagger-UI endpoints starting with <code>/q/</code> have been moved from the   HTTP port exposing the REST endpoints to a different HTTP port (9000),   see Quarkus management interface reference docs.   Any path starting with <code>/q/</code> can be safely removed from a possibly customized configuration   <code>nessie.server.authentication.anonymous-paths</code>.</li> <li>The move of the above endpoints to the management port requires using the Nessie Helm chart for this   release or a newer release. Also, the Helm Chart for this release will not work with older Nessie   releases.</li> </ul>"},{"location":"releases/#commits_60","title":"Commits","text":"<ul> <li>Doc-gen: trim-right before style changes (#8459)</li> <li>Site: Fix release-workflow version bump mistakes (#8454)</li> <li>Site: split release notes pages (#8453)</li> <li>Move management endpoints to separate HTTP port (#8207)</li> <li>Site: let site use wider horizontal space (#8452)</li> <li>OAuthClient: fix AbstractFlow.isAboutToExpire (#8423)</li> </ul>"},{"location":"releases/#0811-release-may-03-2024","title":"0.81.1 Release (May 03, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#fixes_31","title":"Fixes","text":"<ul> <li>GC: Fix handling of quoted column names in Iceberg</li> </ul>"},{"location":"releases/#commits_61","title":"Commits","text":"<ul> <li>Ninja: changelog</li> <li>Use custom class for handling storage URIs (#8420)</li> <li>Site doc generator: respect Javadoc on type for smallrye-config (#8445)</li> <li>Doc-gen: traverse really all supertypes (#8446)</li> <li>Build: fix a bunch of deprecation warnings and no non-consumed apt arg (#8436)</li> <li>Build: tackle Gradle deprecations (#8440)</li> <li>Handle deprecation of Maven <code>DefaultServiceLocator</code> (#8431)</li> <li>Nit: Eliminate deprecation warnings (#8430)</li> <li>Build: Disable warnings about javac command line options for Java 21+ (#8432)</li> <li>Build: Prevent log4j2 to register its MBean (#8433)</li> <li>Build: Prevent unnecessary scala compile warning (#8437)</li> <li>Build/Scala: replace deprecated <code>JavaConverters</code> with <code>CollectionConverters</code> (#8439)</li> <li>Build: Remove no longer necessary Java toolchain \u201cconstraint\u201d (#8438)</li> </ul>"},{"location":"releases/#0810-release-may-01-2024","title":"0.81.0 Release (May 01, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#highlights_11","title":"Highlights","text":"<ul> <li>The Nessie client now supports public clients when using OAuth 2 authentication. Public clients    are clients that do not have a client secret; they are compatible with the <code>password</code>,    <code>authorization_code</code>, and <code>device_code</code> grant types. See the   Nessie documentation    for details.</li> </ul>"},{"location":"releases/#new-features_24","title":"New Features","text":"<ul> <li>The Nessie Helm chart now supports AWS profiles. There are now two ways to configure AWS    credentials in the Helm chart:</li> <li>Using a secret. The secret name can be set in the <code>dynamodb.secret</code> value.</li> <li>Using an AWS profile (new). The profile name can be set in the <code>dynamodb.profile</code> value.</li> </ul>"},{"location":"releases/#commits_62","title":"Commits","text":"<ul> <li>GC: Fix NPE when fetching/parsing table-metadata JSON (#8428)</li> <li>Docs: Trino Nessie configurations (#8385)</li> <li>OAuth2Client: refactor ResourceOwnerEmulator (#8425)</li> <li>OAuthClient: don\u2019t call Instant.now() in serde layer (#8419)</li> <li>ITOAuth2Client: reduce device code poll interval (#8424)</li> <li>Update GH workflows / Gradle run steps (#8386)</li> <li>OAuthClient: consider zero as null for refresh_expires_in field (#8415)</li> <li>OAuthClient: minor tweaks to the token exchange flow (#8414)</li> <li>ResourceOwnerEmulator: properly shutdown executor (#8407)</li> <li>Simpler workaround for #8390 (#8397)</li> <li>Fix Quarkus uber-jar (#8394)</li> <li>Helm charts: add support for AWS profiles (#8382)</li> <li>Let nessie-quarkus use resteasy-reactive (#8383)</li> <li>Resolve some minor build warnings (#8380)</li> <li>Add Amazon STS to dependencies of Nessie Core server (#8377)</li> <li>Nessie client: fix javadoc and compiler warnings (#8376)</li> <li>OAuth2Client: fully support public clients (#8372)</li> <li>Site: exclude generated .md files from search index (#8374)</li> <li>Site: exlude old releases from search index, fix released-version title and page-dir (#8368)</li> <li>Nit: replace non-ASCII double-quote (#8369)</li> </ul>"},{"location":"releases/#0800-release-april-21-2024","title":"0.80.0 Release (April 21, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#upgrade-notes_7","title":"Upgrade notes","text":"<ul> <li>GC Tool: the Nessie GC tool is not published anymore as a Unix executable file, but as a regular    jar named <code>nessie-gc.jar</code>. Instead of running <code>nessie-gc</code>, you now run <code>java -jar nessie-gc.jar</code>.</li> </ul>"},{"location":"releases/#new-features_25","title":"New Features","text":"<ul> <li>Nessie clients can now use the Apache HTTP client, if it\u2019s available on the classpath</li> <li>Nessie clients now return more readable error messages</li> </ul>"},{"location":"releases/#fixes_32","title":"Fixes","text":"<ul> <li>Helm chart: Fix incorrect OpenTelemetry variable names</li> <li>SQL extensions: Respect comments in SQL</li> <li>GC tool: perform commit after schema creation</li> </ul>"},{"location":"releases/#commits_63","title":"Commits","text":"<ul> <li>Ninja: reorg steps in release-create GH workflow</li> <li>Ninja: update CHANGELOG</li> <li>Make bulk read timeout configurable in BigTablePersist (#8361)</li> <li>Fix generates smallrye-config site docs, always add map-key (#8356)</li> <li>Ability to cancel interactive client authentication (#8347)</li> <li>OAuth2Client: defer initial token fetch when user interaction is required (#8354)</li> <li>Generate server and client docs from source (#8334)</li> <li>Update <code>ResponseCheckFilter</code> to return more human friendly error message (#8321)</li> <li>Fix site docs generation on macOS (#8350)</li> <li>OAuth2Client: refactor HTTP exchanges into separate flows (#8338)</li> <li>Add nessie version placeholder as docker image tags (#8339)</li> <li>Docs: align titles with metadata (#8341)</li> <li>OAuth2Client: improve sleep/close detection (#8337)</li> <li>Cleanup ResourceOwnerEmulator and AuthorizationCodeFlow (#8333)</li> <li>Add Nessie GC as Docker Image section to downloads page (#8332)</li> <li>Site: Fix link in site footer (#8327)</li> <li>Make <code>nessie.authentication.oauth2.client-secret</code> optional (#8323)</li> <li>Site rearrangement (#8318)</li> <li>OAuth2 client config: \u201cbark\u201d with all check-errors (#8322)</li> <li>Cleanup: Remove the outdated WORKFLOWS.md file (#8320)</li> <li>OAuth2/client: doc-nit: add note about running a client in a conainer (#8324)</li> <li>Renovate: Update package pattern (#8311)</li> <li>Testing: add test method timeout + more verbose reporting (#8310)</li> <li>Remove left-over micrometer-pattern (#8307)</li> <li>CI: Mitigate still unfixed Gradle bug leading to a <code>ConcurrentModificationException</code> (#8297)</li> <li>Testing/obj-storage-mock: add dummy assume-role endpoint (#8288)</li> <li>Use new plugin ID <code>com.gradle.develocity</code> instead of <code>com.gradle.enterprise</code> (#8293)</li> <li>Commit after schema creation for GC tool (#8290)</li> <li>Content Generator: reorganize options using argument groups (#8284)</li> <li>Update README.md to point to correct url (#8282)</li> <li>Add \u2013recursive to BulkCommittingCommand (#8271)</li> <li>Configurable scopes in KeycloakTestResourceLifecycleManager (#8275)</li> <li>Enforce S3 path-style access for ObjectStorageMock (#8272)</li> <li>SQL extensions: fix whitespace not appearing in antlr exceptions (#8273)</li> <li>Testing: Move <code>Bucket.createHeapStorageBucket()</code> to separate class, allow clearing it (#8268)</li> <li>[Docs]: Fix iceberg-links on docs with latest url (#8260)</li> <li>Object-store test containers improvements (#8258)</li> <li>Testing/object-storage-mock: support different ADLS client upload behavior (#8256)</li> <li>Add support for ADLS Gen2 and GCS to Object Storage Mock (#8244)</li> <li>Remove effectively unnecessary <code>:nessie-jaxrs</code> module (#8247)</li> <li>Respect comments in SQL + move code to extensions-base (#8248)</li> <li>S3-Mock: add local heap storage (#8236)</li> <li>Fix incorrect OpenTelemetry variable names (#8237)</li> <li>Add Apache HTTP client, choose HTTP client by name (#8224)</li> <li>HTTP-Client: capture beginning of unparseable error response (#8226)</li> <li>Allow concurrent instances of GCSContainer (#8235)</li> <li>Add GCS testcontainer + IT for Nessie-GC (#8233)</li> <li>Bump <code>com.google.cloud.bigdataoss:gcs-connector</code> from <code>hadoop3-2.2.18</code> to <code>.21</code> (#8232)</li> <li>CI: Remove duplicate run of <code>:nessie-client:test</code> (#8229)</li> <li>Minor enhancements to MinioContainer (#8225)</li> <li>Prevent duplicate ResponseCheckFilter (#8223)</li> <li>CI: Unify Docker + Helm jobs, validate helm chart against local image (#8215)</li> <li>Ninja: Fix renovate config</li> <li>Renovate/versioning schema for Minio (#8210)</li> <li>Add <code>NessieClientConfigSource</code> backed by Iceberg REST catalog properties (#8206)</li> <li>Fix nessie-gc executable (#8205)</li> <li>CI: Add timeouts to all jobs (#8203)</li> <li>Bump Minio testing container (#8194)</li> <li>Build: Allow \u201cbig\u201d shadow-jars (#8192)</li> </ul>"},{"location":"releases/#0790-release-march-12-2024","title":"0.79.0 Release (March 12, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_26","title":"New Features","text":"<ul> <li>Iceberg bumped to 1.5.0</li> </ul>"},{"location":"releases/#changes_18","title":"Changes","text":"<ul> <li>SQL extensions for Spark 3.2 have been removed</li> <li>Experimental iceberg-views module has been removed</li> </ul>"},{"location":"releases/#commits_64","title":"Commits","text":"<ul> <li>Remove SQL Extensions for Spark 3.2 for Iceberg 1.5+ (#7940)</li> <li>Remove <code>iceberg-views</code> module for Iceberg 1.5+ (#8058)</li> <li>Nit: minor javadoc updates in <code>MergeKeyBehavior</code> (#8173)</li> <li>Simplify ConfigChecks.configCheck (#8177)</li> </ul>"},{"location":"releases/#0780-release-march-07-2024","title":"0.78.0 Release (March 07, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_27","title":"New Features","text":"<ul> <li>GC Tool: ability to skip creating existing tables (IF NOT EXISTS)</li> <li>Make <code>Authorizer</code> pluggable</li> <li>Helm chart: add option to set sessionAffinity on Service</li> </ul>"},{"location":"releases/#fixes_33","title":"Fixes","text":"<ul> <li>Handle re-added keys when creating squash commits</li> <li>JDBC backend: infer catalog and schema if not specified</li> </ul>"},{"location":"releases/#commits_65","title":"Commits","text":"<ul> <li>Ninja; update CHANGELOG.md</li> <li>Add convenience <code>Namespace.getParentOrEmpty()</code> (#8170)</li> <li>Enforce covariant return types in NessieClientBuilder hierarchy (#8164)</li> <li>Revert meaning of <code>@WithInitializedRepository</code> (#8162)</li> <li>Handle re-added keys when creating squash commits (#8160)</li> <li>HTTP client: add request and response filters programmatically (#8154)</li> <li>Reduce flakiness in CustomKeycloakContainer startup (#8153)</li> <li>Docker image tool: implement local image builds (#8141)</li> <li>JDBC backend: infer catalog and schema if not specified (#8131)</li> <li>Bump Scala to 2.12.19 + 2.13.13 (#8125)</li> <li>Bump Spark 3.5 to 3.5.1 (#8124)</li> <li>APIv2: Properly throw an exception when using <code>GetEntriesBuilder.namespaceDepth</code> (#7563)</li> <li>Update Iceberg version in mkdocs.yml (#8122)</li> <li>Testing: Allow test instance <code>@NessiePersist.configMethod</code> (#8120)</li> <li>Update Iceberg version in README.md (#8121)</li> <li>Keycloak containers: close admin clients (#8118)</li> <li>Remove unnecessary <code>ServerAccessContext</code> (#8114)</li> <li>Add set of roles to <code>AccessContext</code>, inject AC instead of <code>Principal</code> (#8091)</li> <li>Add test case for re-create table and merge (#8113)</li> <li>Add S3 user guide section for Minio (#8102)</li> <li>GC Tool: ability to skip creating existing tables (IF NOT EXISTS) (#8081)</li> <li>Persist-cache: add config object (#8112)</li> <li>Nit/noop: Remove use of anonymous <code>StoreConfig</code> impl (#8110)</li> <li>Nit: correct outdated javadoc (#8107)</li> <li>Make <code>Authorizer</code> pluggable (#8090)</li> <li>Helm chart: add option to set sessionAffinity on Service (#8095)</li> <li>Fix logging class in <code>ConfigChecks</code> (#8089)</li> <li>Remove unused function in CachingPersistImpl (#8086)</li> </ul>"},{"location":"releases/#0771-release-february-14-2024","title":"0.77.1 Release (February 14, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#highlights_12","title":"Highlights","text":"<ul> <li>The Nessie GC tool is now published as a Docker image. See the GC Tool documentation   page for more.</li> <li>Remove synchronizing to docker.io container registry, only publish to ghcr.io and quay.io.</li> </ul>"},{"location":"releases/#upgrade-notes_8","title":"Upgrade notes","text":"<ul> <li>Projectnessie no longer publishes container images to docker.io/Docker Hub. Container images are   available from ghcr.io and quay.io.</li> </ul>"},{"location":"releases/#new-features_28","title":"New Features","text":"<ul> <li>Add some configuration checks to highlight probably production issues</li> <li>Publish Docker images for the GC tool</li> </ul>"},{"location":"releases/#changes_19","title":"Changes","text":"<ul> <li>Disable default OIDC tenant when authentication is disabled</li> <li>Disable OpenTelemetry SDK when no endpoint is defined</li> </ul>"},{"location":"releases/#fixes_34","title":"Fixes","text":"<ul> <li>Fix VersionStore panels of the Grafana dashboard</li> </ul>"},{"location":"releases/#commits_66","title":"Commits","text":"<ul> <li>Fix release-publishing (#8069)</li> </ul>"},{"location":"releases/#0770-release-february-14-2024","title":"0.77.0 Release (February 14, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#highlights_13","title":"Highlights","text":"<ul> <li>The Nessie GC tool is now published as a Docker image. See the GC Tool documentation   page for more.</li> <li>Remove synchronizing to docker.io container registry, only publish to ghcr.io and quay.io.</li> </ul>"},{"location":"releases/#upgrade-notes_9","title":"Upgrade notes","text":"<ul> <li>Projectnessie no longer publishes container images to docker.io/Docker Hub. Container images are   available from ghcr.io and quay.io.</li> </ul>"},{"location":"releases/#new-features_29","title":"New Features","text":"<ul> <li>Add some configuration checks to highlight probably production issues</li> <li>Publish Docker images for the GC tool</li> </ul>"},{"location":"releases/#changes_20","title":"Changes","text":"<ul> <li>Disable default OIDC tenant when authentication is disabled</li> <li>Disable OpenTelemetry SDK when no endpoint is defined</li> </ul>"},{"location":"releases/#fixes_35","title":"Fixes","text":"<ul> <li>Fix VersionStore panels of the Grafana dashboard</li> </ul>"},{"location":"releases/#commits_67","title":"Commits","text":"<ul> <li>CI: Don\u2019t fail CI job when list of changed files is empty (#8066)</li> <li>No longer publish to docker.io, only ghcr.io and sync to quay.io (#8065)</li> <li>Ninja: update CHANGELOG</li> <li>Version catalog: split versions for Quarkus/Platform and Quakus Gradle Plugin (#8062)</li> <li>Mark ContentResponse.getEffectiveReference as not null (#8043)</li> <li>Simplify ClientSideGetMultipleNamespaces (#8044)</li> <li>Separate production from test code in storage modules (#8046)</li> <li>Events: don\u2019t log startup messages if there aren\u2019t any subscribers (#8061)</li> <li>OpenTelemetry: only log startup messages once (#8059)</li> <li>Publish Docker images for the GC tool (#8055)</li> <li>Renovate: don\u2019t add any labels (#8053)</li> <li>Tweak CassandraClientProducer for tests (#8052)</li> <li>Fix <code>invalid @Startup method</code> again (#8047)</li> <li>Remove redundant logback-test.xml in classpath (#8048)</li> <li>Use Google Cloud CLI image that supports linux/arm (#8045)</li> <li>Add some configuration checks to highlight probably production issues (#8027)</li> <li>Fix VersionStore panels of the Grafana dashboard (#8033)</li> <li>Custom object types: Use gz as default compression (#8041)</li> <li>Fix typo (#8029)</li> <li>Update Jackson version test matrix in <code>:nessie-client</code> (#8030)</li> <li>Re-add <code>@Startup</code> annotation to eagerly setup the version store backend (#8028)</li> <li>Gradle - simpler assignments (#8016)</li> <li>CI/Testing: Let Renovate manage Docker image references (#8015)</li> <li>Testing: set dynamic Quarkus test-http port everywhere (#8006)</li> <li>Disable default OIDC tenant when authentication is disabled (#8000)</li> <li>Improve documentation for Nessie on Kubernetes (#7997)</li> <li>Disable OpenTelemetry SDK when no endpoint is defined (#8004)</li> </ul>"},{"location":"releases/#0766-release-january-26-2024","title":"0.76.6 Release (January 26, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#changes_21","title":"Changes","text":"<ul> <li>Nessie Docker images now contain Java 21</li> <li>Helm: Make ingressClassName configurable</li> <li>Helm: Use auto scaling</li> <li>Improve error message when JDBC/C* columns are missing</li> </ul>"},{"location":"releases/#commits_68","title":"Commits","text":"<ul> <li>Bump Spark 3.3 from 3.3.3 to 3.3.4 (#7990)</li> <li>Ninja: update CHANGELOG</li> <li>Helm chart: use autoscaling/v2 (#7982)</li> <li>Make ingressClassName configurable (#7979)</li> <li>Coordinated Tasks: make service-impl CDI-friendly (#7974)</li> <li>Use Java 21 in published Docker images (#7973)</li> <li>Coordinated Tasks (#7947)</li> <li>Build: add some more javac linting (#7965)</li> <li>nessie-events-quarkus: remove deprecation warnings (#7964)</li> <li>Bump Spark 3.4 from 3.4.1 to 3.4.2 (#7963)</li> <li>Improve error message when JDBC/C* columns are missing (#7961)</li> <li>Bump logback-classic from 1.2.12 to 1.2.13 (#7962)</li> </ul>"},{"location":"releases/#0765-release-january-26-2024","title":"0.76.5 Release (January 26, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#changes_22","title":"Changes","text":"<ul> <li>Nessie Docker images now contain Java 21</li> <li>Helm: Make ingressClassName configurable</li> <li>Helm: Use auto scaling</li> <li>Improve error message when JDBC/C* columns are missing</li> </ul>"},{"location":"releases/#commits_69","title":"Commits","text":""},{"location":"releases/#0764-release-january-26-2024","title":"0.76.4 Release (January 26, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#changes_23","title":"Changes","text":"<ul> <li>Nessie Docker images now contain Java 21</li> <li>Helm: Make ingressClassName configurable</li> <li>Helm: Use auto scaling</li> <li>Improve error message when JDBC/C* columns are missing</li> </ul>"},{"location":"releases/#commits_70","title":"Commits","text":"<ul> <li>Bump Spark 3.3 from 3.3.3 to 3.3.4 (#7990)</li> <li>Ninja: update CHANGELOG</li> <li>Helm chart: use autoscaling/v2 (#7982)</li> <li>Make ingressClassName configurable (#7979)</li> <li>Coordinated Tasks: make service-impl CDI-friendly (#7974)</li> <li>Use Java 21 in published Docker images (#7973)</li> <li>Coordinated Tasks (#7947)</li> <li>Build: add some more javac linting (#7965)</li> <li>nessie-events-quarkus: remove deprecation warnings (#7964)</li> <li>Bump Spark 3.4 from 3.4.1 to 3.4.2 (#7963)</li> <li>Improve error message when JDBC/C* columns are missing (#7961)</li> <li>Bump logback-classic from 1.2.12 to 1.2.13 (#7962)</li> </ul>"},{"location":"releases/#0763-release-january-16-2024","title":"0.76.3 Release (January 16, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#highlights_14","title":"Highlights","text":"<ul> <li>The Nessie client supports two new authentication flows when using OAuth 2 authentication:   the Authorization Code flow and the Device Code flow. These flows are well suited for use within    a command line program, such as a Spark SQL shell, where a user is interacting with Nessie using a   terminal. In these flows, the user must use their web browser to authenticate with the identity   provider. See the    Nessie documentation    for details. The two new flows are enabled by the following new grant types:</li> <li><code>authorization_code</code>: enables the Authorization Code flow; this flow can only be used with     a local shell session running on the user\u2019s machine.</li> <li><code>device_code</code>: enables the Device Code flow; this flow can be used with either a local or a      remote shell session. </li> <li>The Nessie client now supports endpoint discovery when using OAuth 2 authentication. If an    identity provider supports the OpenID Connect Discovery mechanism, the Nessie client can be    configured to use it to discover the OAuth 2 endpoints. See the    Nessie documentation    for details.</li> </ul>"},{"location":"releases/#new-features_30","title":"New Features","text":"<ul> <li>Nessie client: the OAUTH2 authentication provider now supports programmatic configuration. See the    Nessie documentation for details.</li> </ul>"},{"location":"releases/#fixes_36","title":"Fixes","text":"<ul> <li>Fix potential NPE when fetching commit log with fetch option <code>ALL</code> and access checks enabled.</li> </ul>"},{"location":"releases/#commits_71","title":"Commits","text":"<ul> <li>Fix URL endpoint for injected <code>NessieApi</code> from <code>OldNessieServer</code> (#7958)</li> <li>Improve test coverage in ProtoSerialization (#7943)</li> <li>Use fluent <code>ObjIdHasher</code> (#7957)</li> <li>Mention JAVA_TOOL_OPTIONS in the docs (#7952)</li> <li>Fix \u201cunlimited\u201d behavior flexible cache (#7949)</li> <li>remove unused jersey-test-framework-provider dependencies (#7946)</li> <li>Persist: Conditional deletes + updates (#7932)</li> <li>Fix RockDB config properties (#7942)</li> <li>Shrink heap footprint of <code>ObjIdGeneric</code> (#7934)</li> <li>Caching behavior per object type (#7931)</li> <li>HttpClient: ability to define per-request authentication (#7928)</li> </ul>"},{"location":"releases/#0762-release-january-11-2024","title":"0.76.2 Release (January 11, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#highlights_15","title":"Highlights","text":"<ul> <li>The Nessie client supports two new authentication flows when using OAuth 2 authentication:   the Authorization Code flow and the Device Code flow. These flows are well suited for use within    a command line program, such as a Spark SQL shell, where a user is interacting with Nessie using a   terminal. In these flows, the user must use their web browser to authenticate with the identity   provider. See the    Nessie documentation    for details. The two new flows are enabled by the following new grant types:</li> <li><code>authorization_code</code>: enables the Authorization Code flow; this flow can only be used with     a local shell session running on the user\u2019s machine.</li> <li><code>device_code</code>: enables the Device Code flow; this flow can be used with either a local or a      remote shell session. </li> <li>The Nessie client now supports endpoint discovery when using OAuth 2 authentication. If an    identity provider supports the OpenID Connect Discovery mechanism, the Nessie client can be    configured to use it to discover the OAuth 2 endpoints. See the    Nessie documentation    for details.</li> </ul>"},{"location":"releases/#new-features_31","title":"New Features","text":"<ul> <li>Nessie client: the OAUTH2 authentication provider now supports programmatic configuration. See the    Nessie documentation for details.</li> </ul>"},{"location":"releases/#fixes_37","title":"Fixes","text":"<ul> <li>Fix potential NPE when fetching commit log with fetch option <code>ALL</code> and access checks enabled.</li> </ul>"},{"location":"releases/#commits_72","title":"Commits","text":"<ul> <li>fix free-disk-space action (#7937)</li> <li>Nit: let OAuth2Client use switch for enum (#7933)</li> <li>Adapt OAuth2Client to per-request base URIs (#7925)</li> </ul>"},{"location":"releases/#0761-release-january-09-2024","title":"0.76.1 Release (January 09, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#highlights_16","title":"Highlights","text":"<ul> <li>The Nessie client supports two new authentication flows when using OAuth 2 authentication:   the Authorization Code flow and the Device Code flow. These flows are well suited for use within    a command line program, such as a Spark SQL shell, where a user is interacting with Nessie using a   terminal. In these flows, the user must use their web browser to authenticate with the identity   provider. See the    Nessie documentation    for details. The two new flows are enabled by the following new grant types:</li> <li><code>authorization_code</code>: enables the Authorization Code flow; this flow can only be used with     a local shell session running on the user\u2019s machine.</li> <li><code>device_code</code>: enables the Device Code flow; this flow can be used with either a local or a      remote shell session. </li> <li>The Nessie client now supports endpoint discovery when using OAuth 2 authentication. If an    identity provider supports the OpenID Connect Discovery mechanism, the Nessie client can be    configured to use it to discover the OAuth 2 endpoints. See the    Nessie documentation    for details.</li> </ul>"},{"location":"releases/#new-features_32","title":"New Features","text":"<ul> <li>Nessie client: the OAUTH2 authentication provider now supports programmatic configuration. See the    Nessie documentation for details.</li> </ul>"},{"location":"releases/#fixes_38","title":"Fixes","text":"<ul> <li>Fix potential NPE when fetching commit log with fetch option <code>ALL</code> and access checks enabled.</li> </ul>"},{"location":"releases/#commits_73","title":"Commits","text":"<ul> <li>Allow per-request base URIs in HttpClient (#7922)</li> <li>Compile services and SPI for java 8 (#7924)</li> <li>Fix Device Code flow tests with Java 21 (#7920)</li> <li>Custom obj types should not persist class name (#7915)</li> <li>Site: Fix TOC in <code>client_config.md</code> (#7906)</li> <li>Nit: remove mention of \u201cHive\u201d on project web site (#7905)</li> </ul>"},{"location":"releases/#0760-release-january-02-2024","title":"0.76.0 Release (January 02, 2024)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#highlights_17","title":"Highlights","text":"<ul> <li>The Nessie client supports two new authentication flows when using OAuth 2 authentication:   the Authorization Code flow and the Device Code flow. These flows are well suited for use within    a command line program, such as a Spark SQL shell, where a user is interacting with Nessie using a   terminal. In these flows, the user must use their web browser to authenticate with the identity   provider. See the    Nessie documentation    for details. The two new flows are enabled by the following new grant types:</li> <li><code>authorization_code</code>: enables the Authorization Code flow; this flow can only be used with     a local shell session running on the user\u2019s machine.</li> <li><code>device_code</code>: enables the Device Code flow; this flow can be used with either a local or a      remote shell session. </li> <li>The Nessie client now supports endpoint discovery when using OAuth 2 authentication. If an    identity provider supports the OpenID Connect Discovery mechanism, the Nessie client can be    configured to use it to discover the OAuth 2 endpoints. See the    Nessie documentation    for details.</li> </ul>"},{"location":"releases/#new-features_33","title":"New Features","text":"<ul> <li>Nessie client: the OAUTH2 authentication provider now supports programmatic configuration. See the    Nessie documentation for details.</li> </ul>"},{"location":"releases/#fixes_39","title":"Fixes","text":"<ul> <li>Fix potential NPE when fetching commit log with fetch option <code>ALL</code> and access checks enabled.</li> </ul>"},{"location":"releases/#commits_74","title":"Commits","text":"<ul> <li>Revert \u201cAdd <code>detach-history</code> command to the ContentGenerator tool (#7867)\u201d (#7907)</li> <li>Add Quarkus tests for the OAuth2 Device Code flow (#7900)</li> <li>OAuth2 client: support for Device Code Flow (#7899)</li> <li>OAuth2 client: programmatic creation of OAuth2Authenticator (#7894)</li> <li>OAuth2 client: support endpoint discovery (#7884)</li> <li>HttpClient: properly close resources (#7898)</li> <li>Persist: simplify JsonObj (#7866)</li> <li>Add documentation page for repository migration (#7895)</li> <li>Quarkus CLI: minor cleanup after #6890 (#7896)</li> <li>Fix Helm CI tests (#7893)</li> <li>Nessie Client: support for Authorization Code grant (#7872)</li> <li>Fix NPE when fetching commit log with access checks enabled. (#7886)</li> <li>Add <code>detach-history</code> command to the ContentGenerator tool (#7867)</li> </ul>"},{"location":"releases/#0750-release-december-15-2023","title":"0.75.0 Release (December 15, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#upgrade-notes_10","title":"Upgrade notes","text":"<ul> <li>Nessie Quarkus parts are now built against Java 17 and Java 17 is required to run Nessie Quarkus Server directly.   If you use the Docker image, nothing needs to be done, because the image already contains a compatible Java runtime.</li> <li>Due to the introduction of new object types in the storage layer, some storage backends   will require a schema upgrade:</li> <li>JDBC: the following SQL statement must be executed on the Nessie database (please adapt the     statement to the actual database SQL dialect):     <pre><code>ALTER TABLE objs \n  ADD COLUMN x_class VARCHAR,\n  ADD COLUMN x_data BYTEA,\n  ADD COLUMN x_compress VARCHAR,\n  ADD COLUMN u_space VARCHAR,\n  ADD COLUMN u_value BYTEA;\n</code></pre></li> <li>Cassandra: the following CQL statement must be executed on the Nessie database and keyspace:     <pre><code>ALTER TABLE &lt;keyspace&gt;.objs \n  ADD x_class text, \n  ADD x_data blob,\n  ADD x_compress text,\n  ADD u_space text,\n  ADD u_value blob;\n</code></pre></li> <li>When using one of the legacy and deprecated version-store implementations based on \u201cdatabase adapter\u201d,   make sure to migrate to the new storage model before upgrading to this version or newer Nessie   versions.</li> </ul>"},{"location":"releases/#breaking-changes_10","title":"Breaking changes","text":"<ul> <li>The deprecated version-store implementations based on \u201cdatabase datapter\u201d have been removed from the   code base.</li> </ul>"},{"location":"releases/#commits_75","title":"Commits","text":"<ul> <li>Quarkus ITs: Remove LoggerFinder error log message (#7862)</li> <li>Quarkus-tests: do not log OIDC connection + tracing warnings (#7860)</li> <li>Let OAuth2 errors not lot stack traces (#7859)</li> <li>stop testing spark-extensions 3.2 on iceberg main (#7863)</li> <li>Fix Quarkus warning <code>@Inject</code> on private field (#7857)</li> <li>Use <code>/</code> as the resteasy base path in Quarkus (#7854)</li> <li>Extract reusable REST related functionality (#7838)</li> <li>Move authN/Z code to separate module (#7851)</li> <li>Make <code>DiffParams</code> work with resteasy-reactive (#7846)</li> <li>Make server-side components use only Jakarta EE (#7837)</li> <li>Remove dependency-resolution workaround for guava/listenablefuture (#7841)</li> <li>renovate: reduce awssdk update frequency (#7840)</li> <li>Helm chart: remove mentions of legacy storage types (#7830)</li> <li>Persist: simplify serialization of custom objects (#7832)</li> <li>Ignore Obj.type() when using Smile serialization (#7828)</li> <li>Fix \u201colder Jackson versions\u201d tests in <code>:nessie-client</code> (#7820)</li> <li>Expose <code>HttpClient</code> from <code>NessieApi</code> when available (#7808)</li> <li>Expose request-URI in <code>HttpResponse</code> (#7807)</li> <li>Build: remove no longer needed reflection-config-plugin (#7800)</li> <li>Nit: Remove unsed Quarkus config options (#7799)</li> <li>Remove unused <code>@RegisterForReflection</code> annotations (#7797)</li> <li>Remove invalid <code>@Startup</code> annotation (#7796)</li> <li>Ensure that content IDs are unique in a Nessie repository (#7757)</li> <li>Remove database adapter code (#6890)</li> <li>Persist/custom objects: allow compression (#7795)</li> <li>CassandraPersist: minor code cleanup (#7793)</li> <li>Extensible object types (#7771)</li> <li>Require Java 17 for the build, prepare for Quarkus 3.7 (#7783)</li> <li>Update issue templates (#7787)</li> <li>CI: Add <code>concurrency</code> to CI-Mac/Win + newer-Java workflows (#7785)</li> <li>GH WF: Remove no-longer existing images to remove (#7784)</li> <li>Move ObjIdSerializer.java to test scope (#7782)</li> <li>Add <code>LABEL</code>s to <code>Dockerfile-jvm</code> (#7775)</li> <li>ClientSideGetMultipleNamespaces: push some predicates down to the server (#7758)</li> </ul>"},{"location":"releases/#0740-release-november-21-2023","title":"0.74.0 Release (November 21, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_34","title":"New Features","text":"<ul> <li>Nessie-GC: Support Google Cloud Storage (GCS) (experimental)</li> <li>Nessie-GC: Support Azure Blob Storage (experimental)</li> </ul>"},{"location":"releases/#fixes_40","title":"Fixes","text":"<ul> <li>Add namespace validation for rename operation.</li> <li>Namespace validation now correctly reports only one conflict when deleting a namespace that has   children, whereas previously it reported one conflict for each child.</li> </ul>"},{"location":"releases/#commits_76","title":"Commits","text":"<ul> <li>Fix wrong cache capacity unit in <code>NessiePersistCache</code> (#7746)</li> <li>Persist: enforce that null array elements are legal (#7741)</li> <li>BigTable: set QUALIFIER_OBJ_TYPE cell when upserting (#7731)</li> <li>Fix Nesqueit after #7715 (#7722)</li> <li>Renovate: only add one label (#7720)</li> <li>Nessie-GC: Support Azure Blob Storage (experimental) (#7715)</li> <li>Minor doc enhancements around authentication providers (#7710)</li> <li>Nessie-GC: Support Google Cloud Storage (GCS) (experimental) (#7709)</li> <li>Report a single conflict when deleting a namespace with many children (#7704)</li> <li>Add nessie-client-testextension to nessie-bom (#7703)</li> <li>Fix Maven-Central search URL in release notes (#7696)</li> <li>Align opentelemetry-alpha version (#7694)</li> <li>Fix snapshot publishing (#7692)</li> <li>Add namespace validation for rename operation (#7650)</li> <li>fix JUnit Jupiter references (#7690)</li> <li>DynamoDB: fix wrong error message when key schema invalid (#7685)</li> <li>OAuth2Client: improve stack traces and reduce log frequency (#7678)</li> <li>ITOAuth2Client: improve shutdown sequence (#7676)</li> <li>ITOAuth2Client: use less aggressive token lifespans (#7675)</li> </ul>"},{"location":"releases/#0730-release-october-27-2023","title":"0.73.0 Release (October 27, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#highlights_18","title":"Highlights","text":"<ul> <li>Nessie API spec was upgraded to 2.1.3. The only change is that when a commit attempts to create a content   inside a non-existing namespace, the server will not only return a <code>NAMESPACE_ABSENT</code> conflict for the   non-existing namespace itself, but will also return additional <code>NAMESPACE_ABSENT</code> conflicts for all the   non-existing ancestor namespaces.</li> </ul>"},{"location":"releases/#new-features_35","title":"New Features","text":"<ul> <li>Nessie client: the OAuth2 authentication provider is now able to recover from transient failures when   refreshing the access token.</li> </ul>"},{"location":"releases/#commits_77","title":"Commits","text":"<ul> <li>Update CHANGELOG.md (#7674)</li> <li>Expose full set of Bigtable retry settings in BigTableClientsConfig (#7672)</li> <li>Report all missing namespaces at once (#7671)</li> <li>OAuth2Client: implement fault tolerance (#7669)</li> <li>Make Conflict.conflictType non nullable (#7667)</li> <li>OAuth2Client: forbid recursive calls to renewTokens() (#7663)</li> </ul>"},{"location":"releases/#0724-release-october-24-2023","title":"0.72.4 Release (October 24, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#fixes_41","title":"Fixes","text":"<ul> <li>Docker images again honor environment variables such as <code>JAVA_OPTS_APPEND</code> that are used to pass   additional JVM options to the Nessie server. See the    ubi8/openjdk-17   base image documentation for the list of all supported environment variables.</li> </ul>"},{"location":"releases/#commits_78","title":"Commits","text":"<ul> <li>Docker: re-enable JAVA_OPTS_APPEND env var (#7659)</li> <li>Relax CacheSizing validation rules. (#7660)</li> <li>OAuth2Client: propagate scheduling failures when calling authenticate() (#7656)</li> <li>Remove commit-meta property containing the merge-parent commit-ID (#7646)</li> <li>Enhance released jars (#7651)</li> </ul>"},{"location":"releases/#0722-release-october-19-2023","title":"0.72.2 Release (October 19, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_36","title":"New Features","text":"<ul> <li>Exposes object cache metrics, meters use the tags <code>application=\"Nessie\",cache=\"nessie-objects\"</code></li> </ul>"},{"location":"releases/#fixes_42","title":"Fixes","text":"<ul> <li>Fix incorrectly calculated object cache maximum weight.</li> </ul>"},{"location":"releases/#commits_79","title":"Commits","text":"<ul> <li>Expose object cache metrics (#7643)</li> <li>Cleanup: remove dependency on immutables-value-fixtures (#7644)</li> </ul>"},{"location":"releases/#0721-release-october-18-2023","title":"0.72.1 Release (October 18, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_37","title":"New Features","text":"<ul> <li>Spark SQL extensions now support the <code>DROP ... IF EXISTS</code> syntax for branches and tags.</li> <li><code>table-prefix</code> configuration option added to DynamoDB version store.</li> <li>Ability to export repositories in V1 format. This is useful for migrating repositories to older    Nessie servers that do not support the new storage model.</li> <li>Added support for Spark 3.5, removed support for Spark 3.1 - along with the version bump of Apache Iceberg to 1.4.0.</li> <li>Functionality that records current-HEAD changes of named references and APIs to expose the information.   This is useful to recover from a scenario when a \u201cprimary data center/region/zone\u201d has been lost and   replication of a distributed database has been interrupted.</li> </ul>"},{"location":"releases/#changes_24","title":"Changes","text":"<ul> <li>Introduces sizing of the Nessie object cache using a relative value of the max Java heap size.   The defaults have been changed to 70% of the Java max heap size (from the previous default of 64MB).   If a fixed cache size setting has been explicitly configured, consider to change it to the fraction based one.</li> <li>Relative hashes are now supported in table references, thus allowing SQL queries to specify a relative hash   in the <code>FROM</code> clause, e.g. <code>FROM table1@main#1234^1</code>.</li> <li>BigTable backend: ability to disable telemetry (which is enabled by default).</li> <li>Spark SQL extensions use Nessie API V2 now.</li> <li>DynamoDB backend now supports table prefixes.</li> <li>Advanced configuration options for BigTable backend.</li> </ul>"},{"location":"releases/#fixes_43","title":"Fixes","text":"<ul> <li>Quarkus 3.4.3 includes a Netty version bump to address CVE-2023-44487 (HTTP/2 rapid reset). Note: Nessie uses undertow only for testing purposes, so the undertow release used in Nessie does not expose this CVE to users.</li> </ul>"},{"location":"releases/#commits_80","title":"Commits","text":"<ul> <li>Set OAuth2Client idle when Nessie client is idle (#7626)</li> <li>BigTable: simplify configuration of clients (#7639)</li> <li>Allow Bigtable RPC retries (#7636)</li> <li>Update undertow to 2.2.28.Final (#7616)</li> <li>Site: some maintenance (#7634)</li> <li>SQL+Cassandra: add new <code>refs</code> columns to schema check (#7631)</li> <li>CI/Publish: Mitigate Gradle\u2019s annoying CME bug and GH\u2019s HTTP/502 (#7625)</li> </ul>"},{"location":"releases/#0720-release-october-13-2023","title":"0.72.0 Release (October 13, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_38","title":"New Features","text":"<ul> <li>Spark SQL extensions now support the <code>DROP ... IF EXISTS</code> syntax for branches and tags.</li> <li><code>table-prefix</code> configuration option added to DynamoDB version store.</li> <li>Ability to export repositories in V1 format. This is useful for migrating repositories to older    Nessie servers that do not support the new storage model.</li> <li>Added support for Spark 3.5, removed support for Spark 3.1 - along with the version bump of Apache Iceberg to 1.4.0.</li> <li>Functionality that records current-HEAD changes of named references and APIs to expose the information.   This is useful to recover from a scenario when a \u201cprimary data center/region/zone\u201d has been lost and   replication of a distributed database has been interrupted.</li> </ul>"},{"location":"releases/#changes_25","title":"Changes","text":"<ul> <li>Introduces sizing of the Nessie object cache using a relative value of the max Java heap size.   The defaults have been changed to 70% of the Java max heap size (from the previous default of 64MB).   If a fixed cache size setting has been explicitly configured, consider to change it to the fraction based one.</li> <li>Relative hashes are now supported in table references, thus allowing SQL queries to specify a relative hash   in the <code>FROM</code> clause, e.g. <code>FROM table1@main#1234^1</code>.</li> <li>BigTable backend: ability to disable telemetry (which is enabled by default).</li> <li>Spark SQL extensions use Nessie API V2 now.</li> <li>DynamoDB backend now supports table prefixes.</li> <li>Advanced configuration options for BigTable backend.</li> </ul>"},{"location":"releases/#fixes_44","title":"Fixes","text":"<ul> <li>Quarkus 3.4.3 includes a Netty version bump to address CVE-2023-44487 (HTTP/2 rapid reset). Note: Nessie uses undertow only for testing purposes, so the undertow release used in Nessie does not expose this CVE to users.</li> </ul>"},{"location":"releases/#commits_81","title":"Commits","text":"<ul> <li>Record previous HEADs of named references (#7607)</li> <li>Ninja: Changelog note wrt CVE-2023-44487, fixed via Quarkus 3.4.3</li> <li>Remove OTel workaround for tests (#7623)</li> <li>Nit: replace TODO with comment (#7624)</li> <li>CI/NesQuEIT: Switch Iceberg master-&gt;main (#7618)</li> <li>CI: Use Java 21 in \u201cnewer java versions\u201d workflow (#7525)</li> <li>Workaround to run events-service tests with Java 21 (#7617)</li> <li>Tag all Nessie multi-environment tests (#7577)</li> <li>BigTable: reimplement scanAllObjects with server-side filtering (#7610)</li> <li>Spark 3.1 left-overs (#7612)</li> <li>Ninja: Changelog updates</li> <li>IntelliJ: Generate required classes (#7611)</li> <li>Update SQL extensions code to use API V2 (#7573)</li> <li>Bump Iceberg to 1.4.0, support Spark 3.5 (#7539)</li> <li>BigTable backend: ability to disable telemetry (#7597)</li> <li>Cassandra backend: remove ALLOW FILTERING when fetching objects (#7604)</li> <li>Ability to export repositories in V1 format (#7596)</li> <li>Add option for table prefix to DynamoDB (#7598)</li> <li>SQL extensions: add support for DROP IF EXISTS (#7570)</li> <li>Fix docs on website (#7559)</li> <li>Allow relative hashes in table references (#7565)</li> <li>CI: Remove Spark 3.1 from NesQuEIT job (removed in Iceberg 1.4) (#7564)</li> <li>Cleanup: explicitly annotate <code>ignoreUnknown</code> for types that allow this (#7561)</li> <li>BigTable: custom channel-pool settings, make retry-timeout configurable (#7552)</li> <li>Add client-based test for request parameter validation (#7557)</li> <li>Fix Nessie java client groupId in docs (#7554)</li> <li>Docs: Update Metadata authorization (#7553)</li> <li>Remove unused type parameter in BigTablePersist.doBulkFetch (#7551)</li> <li>Store-cache: allow cache sizing based on max-heap-size (#7540)</li> <li>Configure BigTable client to use MAX_BULK_READS (#7548)</li> <li>BigTable: Use parallel reads for some objects to fetch (#7547)</li> <li>Combined Nessie server+client for testing (#7527)</li> <li>GH actions - allow fork of Nessie in <code>projectnessie</code> (#7529)</li> <li>Bump Spark to 3.3.3/3.4.1 (#7538)</li> <li>Bump Scala 2.13 to 2.13.12 (#7537)</li> <li>CI/NesQuEIT: Change temp branch back to \u201cnormal\u201d (#7534)</li> <li>Build: allow Spark/Java configuration for <code>JavaExec</code> as well (#7531)</li> </ul>"},{"location":"releases/#0711-release-september-21-2023","title":"0.71.1 Release (September 21, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#changes_26","title":"Changes","text":"<ul> <li>Configuration of the <code>NessieClientBuilder</code> now uses system properties, environment, <code>~/.config/nessie/nessie-client.properties</code> and <code>~/.env</code></li> </ul>"},{"location":"releases/#deprecations_2","title":"Deprecations","text":"<ul> <li><code>HttpClientBuilder</code> class has been deprecated for removal, use <code>NessieClientBuilder</code> instead.</li> </ul>"},{"location":"releases/#commits_82","title":"Commits","text":"<ul> <li>Ninja: Fix release-create WF</li> </ul>"},{"location":"releases/#0710-release-september-21-2023","title":"0.71.0 Release (September 21, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#changes_27","title":"Changes","text":"<ul> <li>Configuration of the <code>NessieClientBuilder</code> now uses system properties, environment, <code>~/.config/nessie/nessie-client.properties</code> and <code>~/.env</code></li> </ul>"},{"location":"releases/#deprecations_3","title":"Deprecations","text":"<ul> <li><code>HttpClientBuilder</code> class has been deprecated for removal, use <code>NessieClientBuilder</code> instead.</li> </ul>"},{"location":"releases/#commits_83","title":"Commits","text":"<ul> <li>Ninja: changelog</li> <li>Cleanup nessie-client (#7516)</li> </ul>"},{"location":"releases/#0703-release-september-20-2023","title":"0.70.3 Release (September 20, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#fixes_45","title":"Fixes","text":"<ul> <li>Fix wrong <code>New value for key 'some-key' must not have a content ID</code> when swapping tables.</li> </ul>"},{"location":"releases/#commits_84","title":"Commits","text":"<ul> <li>Avoid re-running previously validated access checks on commit retries. (#7524)</li> <li>Make sure <code>effectiveReference</code> is not present in v1 REST API responses (#7512)</li> <li>Add OTel span events for commit retry sleeps (#7509)</li> <li>IntellJ workaround when using Iceberg snapshot builds (#7504)</li> <li>Remove superfluous <code>dependsOn</code> (#7505)</li> <li>Remove ref-log code (#7500)</li> </ul>"},{"location":"releases/#0702-release-september-12-2023","title":"0.70.2 Release (September 12, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#fixes_46","title":"Fixes","text":"<ul> <li>Fix wrong <code>New value for key 'some-key' must not have a content ID</code> when swapping tables.</li> </ul>"},{"location":"releases/#commits_85","title":"Commits","text":"<ul> <li>Fix behavior when swapping tables (#7498)</li> </ul>"},{"location":"releases/#0701-release-september-12-2023","title":"0.70.1 Release (September 12, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#changes_28","title":"Changes","text":"<ul> <li>Content Generator tool: added new <code>--limit</code> parameter to <code>commits</code>, <code>references</code> and <code>entries</code>    commands.</li> <li>Content Generator tool: tool now prints the total number of elements returned when running the    <code>commits</code>, <code>references</code> and <code>entries</code> commands.</li> <li>Helm charts: OpenTelemetry SDK is now completely disabled when tracing is disabled.</li> <li>Helm charts: when auth is disabled, Quarkus OIDC doesn\u2019t print warnings anymore during startup.</li> </ul>"},{"location":"releases/#fixes_47","title":"Fixes","text":"<ul> <li>GC: Handle delete manifests and row level delete files</li> </ul>"},{"location":"releases/#commits_86","title":"Commits","text":"<ul> <li>Ninja: update CHANGELOG</li> <li>GC: Handle delete manifests and row level delete files (#7481)</li> <li>GC: Bump spark version for integration tests (#7482)</li> <li>MultiEnvTestEngine: append the environment name to test names (#7478)</li> <li>Fix default-OLTP-port typo in docs (#7474)</li> <li>Add NessieUnavailableException (HTTP 503) (#7465)</li> <li>Helm chart: disable OIDC warnings when authentication is disabled (#7460)</li> <li>Helm chart: disable OpenTelemetry SDK when tracing is disabled (#7459)</li> <li>Content generator: add <code>--limit</code> parameter and print totals (#7457)</li> </ul>"},{"location":"releases/#0700-release-august-31-2023","title":"0.70.0 Release (August 31, 2023)","text":"<p>See Release information on GitHub.</p>"},{"location":"releases/#new-features_39","title":"New Features","text":"<ul> <li>Content Generator tool: added new <code>--hash</code> parameter to <code>commits</code>, <code>content</code> and <code>entries</code>    commands.</li> </ul>"},{"location":"releases/#changes_29","title":"Changes","text":"<ul> <li>Content Generator tool: commit hashes are now printed in full when running the <code>commits</code> command.</li> <li>For a \u201cget-keys\u201d operation that requests the content objects as well, the content objects are now   fetched using bulk-requests.</li> </ul>"},{"location":"releases/#fixes_48","title":"Fixes","text":"<ul> <li>Fixed potential index corruption when importing repositories with many keys into the new storage    model that could cause some contents to become inaccessible.</li> </ul>"},{"location":"releases/#commits_87","title":"Commits","text":"<ul> <li>Propagate index stripes from parent to child commits (#7452)</li> <li>Build: fix removal of superfluous system-property for NesQuEIT (#7455)</li> <li>Build: remove superfluous system-property hack (#7451)</li> <li>Bulk-fetch contents for get-keys (#7450)</li> <li>Print full hashes in <code>commits</code> command (#7449)</li> <li>Add \u2013hash parameter to content-generator commands (#7448)</li> </ul>"},{"location":"releases/#older-releases","title":"Older releases","text":"<p>See this page for 0.50.0 to 0.69.2</p> <p>See this page for 0.49.0 and older</p>"},{"location":"server-upgrade/","title":"Nessie Server upgrade notes","text":"<p>The following table lists the upgrade types from one Nessie version to another Nessie version.</p> <p>A check-mark in the Rolling Upgrade column means that it is okay to run Nessie instances running versions in the From column during the limited time of a rolling-upgrade with Nessie versions in the To column.</p> <p>A red cross in the Rolling Upgrade column means that rolling upgrades for the mentioned versions are not supported and must be avoided.</p> <p>Also read the release notes on GitHub and the release notes page.</p> Rolling Upgrade Supported From Nessie version To Nessie version 0.61.0 or newer 0.106.0 or newer 0.40.0 or newer 0.61.0 or newer 0.40.0 or newer 0.60.0 or newer &lt; 0.40.0 0.40.0 or newer 0.26.0 to 0.29.0 0.27.0 to 0.30.0 0.25.0 or older 0.26.0 or newer 0.18.0 to 0.24.0 0.19.0 to 0.25.0 <p>Older releases than 0.18.0 are not supported.</p> <p>See Releases for release notes.</p>"},{"location":"server-upgrade/#rolling-upgrades-target-version-notes","title":"Rolling upgrades target version notes","text":"<p>Also read the release notes on GitHub and the release notes page.</p>"},{"location":"server-upgrade/#legacy-version-store-types","title":"\u201cLegacy\u201d version store types","text":"<p>The version store types <code>ROCKS</code>, <code>MONGO</code>, <code>DYNAMO</code>, <code>TRANSACTIONAL</code> and <code>INMEMORY</code> were deprecated for a long time and have been removed in Nessie 0.75.0!</p> <p>If you are using one of these version types migrate to one of the supported version store type mentioned above before upgrading to Nessie 0.75.0 or newer.</p> <p>The migration process is described in the migration guide.</p>"},{"location":"server-upgrade/#nessie-0610","title":"Nessie 0.61.0","text":"<p>The serialized format of internal key-indexes got bumped. For a rolling-upgrade to version 0.61.0 or newer, follow these steps.</p> <ol> <li>Deploy Nessie with the system property <code>nessie.internal.store-index-format-version</code> set to <code>1</code></li> <li>Perform the rolling upgrade to 0.61.0 or newer</li> <li>Remove the system property <code>nessie.internal.store-index-format-version</code> (or set it to <code>2</code>)</li> <li>Perform a rolling restart</li> </ol> <p>Alternatively you can also stop all Nessie instances, upgrade those and just restart.</p>"},{"location":"blog/","title":"Nessie Blog","text":""},{"location":"blog/2022/05/31/rolling-upgrade-issue-to-0260/","title":"Rolling upgrade issue to 0.26.0","text":""},{"location":"blog/2022/05/31/rolling-upgrade-issue-to-0260/#symptom","title":"Symptom","text":"<p>During or after a rolling upgrade from Nessie version &lt;= 0.25.0 to &gt;= 0.26.0, exceptions/errors like <code>org.projectnessie.versioned.ReferenceNotFoundException: Global log entry '&lt;hex&gt;\u2019 not does not exist.</code> and/or <code>Iceberg content from reference must have global state, but has none</code> may occur.</p>"},{"location":"blog/2022/05/31/rolling-upgrade-issue-to-0260/#background","title":"Background","text":"<p>When Nessie runs against non-transactional databases, it uses a \u201cglobal pointer\u201d, which holds the mapping of all named references to their HEAD commit IDs, the HEAD of the ref-log and the HEAD of the global-log. Every update to the Nessie repository ends in a CAS<sup>1</sup> on that single global pointer. If the CAS is successful, the change, for example a commit or merge operation, was atomically &amp; consistently applied.<sup>2</sup></p> <p>The approach to maintain these three HEADs in a single \u201crow\u201d works, but it does not scale well. This \u201csingle point of contention\u201d was never meant to stay forever, just as long as we need it and/or do not have a better solution for it.</p> <p>We implemented Nessie using this concrete global pointer mechanism, because certain decisions haven\u2019t been made at that time, and we wanted to be on the \u201csafe side\u201d and then see what can be improved.</p>"},{"location":"blog/2022/05/31/rolling-upgrade-issue-to-0260/#analysis","title":"Analysis","text":"<p>Recently we were certain that having the so-called \u201cglobal state\u201d for Iceberg tables and views is actually not such a great thing. So the team decided that the \u201cglobal state\u201d can go away. This was implemented in the Nessie PR #3866. Since that change reduced the amount of global-log-entries to nearly 0, we could also get rid of the fact that every single change to the Nessie repository, even creating a branch or tag, creates a potentially empty global-log-entry. Not writing unnecessary global-log-entries was implemented in the Nessie PR #3909. Both PRs, 3866 and 3909, were released together as Nessie 0.26.0. All Nessie tests were passing and nobody realised that a little devil sneaked into these code changes, waiting to be woken up in production.</p> <p>The situation that the global-pointer contains a \u201cbroken\u201d list of global-log parent IDs is definitely confusing and cannot be explained by only looking at the code base of the target 0.26.0 release. It does not even help to only look at the code base of the source 0.25.0 release. Both code bases are completely fine, when only considering those in isolation.</p> <p>The \u201cfun part\u201d happens, when both versions are active at the same time and requests against the same Nessie repository are served by both versions.</p>"},{"location":"blog/2022/05/31/rolling-upgrade-issue-to-0260/#involved-parts-in-the-code-base","title":"Involved parts in the code base","text":"<p>The <code>GlobalStatePointer</code> before PR #3909 has a single field called <code>global_id</code>, which served two purposes. First, it served as the \u201ccondition field\u201d for the CAS<sup>1</sup> operation. The same field <code>global_id</code> also pointed to the HEAD of the global-log. PR #3909 changed this. The <code>global_id</code> field only serves as the \u201ccondition field\u201d for the CAS operation, the HEAD of the global log is held in <code>global_log_head</code>. Since <code>global_id</code> is no longer related to the HEAD of the global log, its value is a random value.</p> <p>As you may already guess, if a Nessie version before PR #3909 performs an update, it interprets the value of <code>global_id</code> as the HEAD of the global log. The linked part of the code then evaluates this if-condition to true, which is some other backwards compatibility code, and fills the list of global-log parents only with the value of <code>global_id</code>, because <code>currentEntry</code> is always <code>null</code>, because <code>global_id</code> does not point to a global log entry.</p> <p>Later, the function updateGlobalStatePointer populates the list of global-log-entries with the ID of the new global-log-entry and the collected parents, which is just that random global-id. So the list of global-log IDs in the global-pointer contains two entries - one that points to a \u201cvalid\u201d global log entry and one that does not seem to exist. This is exactly what has been seen.</p>"},{"location":"blog/2022/05/31/rolling-upgrade-issue-to-0260/#identification-of-the-issue-and-mitigation-in-the-live-system","title":"Identification of the issue and mitigation in the live system","text":"<p>Whether the symptom is actually caused by Nessie global pointer corruption can be validated by  accessing the Nessie storage data directly.</p> <p>Some tooling is required for this because Nessie stores its data as binary blobs.  The <code>servers/quarkus-cli</code> module is to be enhanced (in a follow-up PR) with additional commands so that these operations could be performed without additional coding work.</p> <p>Meanwhile, here\u2019s the outline of how to confirm and fix the problem in a live system.</p> <p>How to confirm the symptom:</p> <ol> <li>Fetch the Nessie Global Pointer</li> <li>For each global_log_head</li> </ol> <ul> <li>Parse it as a Nessie hash</li> <li>Check whether there\u2019s an entry in the Global Log table keyed by this hash</li> </ul> <ol> <li>If at least one of the parent hashes does not have a corresponding global log entry, that will mean that     the Global Pointer data has been corrupted</li> </ol> <p>How to fix the problem:</p> <ol> <li>Do a full scan of the Global Log table</li> <li>Find the last good Global Log entry</li> </ol> <ul> <li>Normally, if Nessie has substantial history good global log entries will have 20 parents       (or whatever was configured)</li> <li>Use log entry timestamps and common sense to identify the last good entry</li> </ul> <ol> <li>Check all global log entries referred to from the Global Pointer directly</li> </ol> <ul> <li>Check whether they have any \u201cputs\u201d,       i.e. contain Iceberg metadata information</li> </ul> <ol> <li>If those entries have \u201cputs\u201d construct a new entry that collectively contains their \u201cput\u201d data    and refers to the last good parent as its parent. Now this new entry becomes the last good log entry.</li> <li>If the entries from step 4 do not have \u201cputs\u201d they can be ignored.</li> <li>Construct a new Global Pointer using its all of its current data, but put the hash of the last    good global log entry as the only element in the  global_log_head list.</li> <li>Store the new Global Pointer overwriting the old (broken) Global Pointer data.</li> <li>Re-run the verification procedure (above) to validate the new Global Pointer and Global Log.</li> </ol>"},{"location":"blog/2022/05/31/rolling-upgrade-issue-to-0260/#additional-testing-effort","title":"Additional testing effort","text":"<p>Nessie already had a bunch of tests regarding compatibility and upgrades. There are tests exercising older Nessie API versions against current in-tree Nessie server, current in-tree Nessie API against older Nessie server versions, and tests exercising single-instance upgrade paths.</p> <p>Sadly, there were no tests that exercised rolling upgrade scenarios, especially none that exercised the case that hit both the old and new versions for multiple requests. For example, create a branch against the server running the \u201cold\u201d Nessie version, then a commit to that branch against the server running the \u201cnew\u201d Nessie version, and other situations.</p> <p>Today, Nessie has a test suite to validate rolling upgrades, implemented via Nessie PR #4350. As all compatibility tests, the new rolling upgrade tests are now part of the normal CI workflows for all PRs and the main branch.</p> <p>Upgrade paths are now documented on projectnessie.org here Nessie Server upgrade notes (via PR #4364 + issue #4348).</p>"},{"location":"blog/2022/05/31/rolling-upgrade-issue-to-0260/#big-changes-in-upcoming-releases","title":"Big changes in upcoming releases","text":"<p>Heads up: there will be more big changes coming in the next releases, that are already known to be not safe for a rolling upgrade. The Nessie PR #4234 eliminates the remaining contention issues in the global pointer. Because it does fundamentally change how named references and the ref-log are maintained, a rolling upgrade from Nessie &lt;= 0.30.0 would definitely cause issues and is therefore not supported.</p>"},{"location":"blog/2022/05/31/rolling-upgrade-issue-to-0260/#future-releases","title":"Future releases","text":"<p>The actual problem at play was not really the fact that upgrading Nessie &lt;= 0.25.0 to  Nessie &gt;= 0.26.0 can cause global-pointer corruption, which is bad, no question. The actual issue is that this fact was not noticed earlier.</p> <p>Learnings from this escalation:</p> <ul> <li>Implement regularly run rolling-upgrade tests in CI (#4350)</li> <li>Clearly document which versions do support rolling upgrades and which   combinations do not work</li> </ul> <ol> <li> <p>CAS means \u201ccompare and swap\u201d. See Wikipedia article \u21a9\u21a9</p> </li> <li> <p>If the CAS operation was not successful, Nessie will retry using a exponential backoff,   configured here.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2023/05/08/namespace-enforcement-in-nessie/","title":"Namespace enforcement in Nessie","text":"<p>Starting from Nessie version 0.52.3, it is required to have existing Namespaces before creating or committing to tables.  </p> <p>In case of tables that have implicit namespaces, new commits will fail after the upgrade as this rule will be enforced. Therefore, it is necessary to explicitly create the namespaces.</p> <p>Namespaces can be created using any of these options: * Using SQL command (for Spark, <code>CREATE NAMESPACE catalogName.namespaceName</code>).  * Using Iceberg Catalog API.  * Using content generator tool to batch create the missing namespaces with <code>create-missing-namespaces</code> option.</p> <p>More info from the spec.  Issue background.</p>"},{"location":"blog/2023/06/20/introducing-nessie-as-a-dremio-source/","title":"Introducing Nessie as a Dremio Source","text":"<p>Nessie is an open-source lakehouse catalog that enables git-like workflows and cross-table transactions on the lakehouse. Today, we\u2019re excited to announce that companies can now use Dremio to connect and work with Nessie as a source.</p> <p>Continue here \u2026</p>"},{"location":"blog/2024/04/11/projectnessieorg-site-reorg/","title":"projectnessie.org site reorg","text":"<p>The projectnessie.org web site became a bit hard to navigate and was missing a functionality to show the documentation/reference for particular Nessie releases. The reorg also reduced the amount of \u201ctabs\u201d from 9 to 5 and reorganized the content quite a bit, solving the issue that information is where site visitors would expect it to be.</p> <p>What we have now - and did not have before:</p> <ul> <li>Guides: A better organized collection of guides and general information about Nessie.</li> <li>Docs: Focused on Nessie releases and their corresponding docs. The earliest release   that is covered by this is version 0.79.0. Upcoming releases will be included here as well. This section also contains   the status of current development, aka nightly or snapshot. Release and server-upgrade notes are covered here as well.</li> <li>Downloads: A \u201cone stop\u201d page showing the download options of the current release.   Download   options for older releases are covered by in the Docs section.</li> <li>Downloads: Nothing changed here - it was already quite simple ;)</li> <li>Blog: The blog section received a little polishing.</li> </ul> <p>A bunch of files and folders have been moved around and links have changed, redirects ensure that existing (perma)links still work and are redirected to the new location.</p> <p>On top of the site-reorg, a couple of minor issues have been fixed.</p>"},{"location":"blog/2024/04/11/projectnessieorg-site-reorg/#versioned-docs","title":"Versioned docs","text":"<p>Versioned docs are not particularly rocket science, especially since other OSS projects do that already, and we could \u201d borrow\u201d a couple of things. Overall, it was neither too difficult nor too time-consuming to add this useful feature.</p> <p>Conceptually, versioned docs are \u201carchived\u201d in a separate branch on GitHub. We plan to add more version dependent documentation, potentially server configuration generated from source and maybe javadocs - that is why there is another branch, which is meant to hold \u201calready rendered\u201d (think: HTML) per Nessie release.</p> <p>Docs for the \u201clatest in-development\u201d are outside the mkdocs <code>docs/</code> tree, because those are rather templates than \u201cready to use\u201d markdown files. Although, the term \u201ctemplate\u201d is not exactly correct, but the <code>site/in-dev/</code> folder serves as the source for the Unreleased (nightly) and the contents are also used for Nessie releases. During a release, the \u201cnightly <code>index.md</code>\u201d is replaced by the <code>index-release.md</code> - so there is some very rudimentary but sufficient templating at play.</p> <p>Versioned docs are not added to the Nessie main source tree, because that would make the main source tree unnecessarily grow. But that concept requires some scripting before the static site can be built. Those scripting is wrapped in a very simple <code>Makefile</code>. There are basically two targets:</p> <pre><code>make build\n</code></pre> <p>and</p> <pre><code>make serve\n</code></pre> <p>Neither requires you to setup any Python virtual environment or install any dependency except the Python interpreter. The scripts triggered by these <code>Makefile</code> targets take care of all the boilerplate.</p>"},{"location":"blog/2024/04/11/projectnessieorg-site-reorg/#strict-is-good","title":"Strict is good","text":"<p>Notice that we build the site using the <code>strict: true</code> setting in <code>mkdocs.yml</code>. This mostly ensures that all links work. And TBH this is also a reason why the <code>site/in-dev/</code> folder is not inside the <code>site/docs/</code> folder, because that breaks a \u201cstrict build\u201d.</p>"},{"location":"blog/2024/04/11/projectnessieorg-site-reorg/#notes-on-ides","title":"Notes on IDEs","text":"<p>While IDEs became pretty smart wrt link highlighting and suggestions/autocompletion, links to and from <code>in-dev/</code> are potentially highlighted as \u201cbroken\u201d and suggestions are wrong - the IDEs just do not know how all that plays together. The escape and solution: Use <code>make build</code>/<code>make serve</code> and watch the output.</p>"},{"location":"blog/2024/06/05/nessie-cache-improvements/","title":"Nessie cache improvements","text":"<p>Caches are there to improve performance by holding the results of expensive operations and make those quickly available. In this post we explain a recent improvement coming in the next Nessie release 0.83.0.</p> <p>Nessie stores information in its backing data store in two tables: the <code>refs</code> (\u201creferences\u201d) table, which essentially stores the current \u201ctip\u201d of branches and tags, and the <code>objs</code> (\u201cobjects\u201d) table, which stores all commits and content objects. Objects in Nessie are (usually, see below) immutable. Immutable objects are great candidates to be cached - so we did exactly that: cache the contents of the <code>objs</code> table. With \u201cjust\u201d Caching for the <code>objs</code> table we see very few reads against that table in production systems.</p> <p>However, nothing is cached for the references (<code>refs</code>) table. This means that basically all Nessie API calls hit the database, no matter whether it is \u201cjust\u201d a read operation like \u201cget-the-tip-of-a-branch\u201d or a commit operation. Given that most operations against a lakehouse are reads, we could save a lot of read operations if there\u2019s a way to cache references.</p> <p>The PR to cache references introduced the capability to cache references, but only in the \u201clocal\u201d Nessie instance. A commit is immediately visible as a change to the \u201ctip\u201d/HEAD of the branch on the Nessie instance that performed the commit - but another Nessie instance serving the same Nessie repository would not know that the reference has changed and \u201chappily\u201d serve and work with the stale cache information. This was a known (and for development/\u201dcognitive review burden\u201d reasons accepted) limitation for that PR, and the most important reason why the functionality to cache references is labeled \u201cexperimental\u201d and is disabled by default. It would be fine to enable it, but only if only a single Nessie instance accesses the repository.</p> <p>Distributed cache invalidation introduced the ability for Nessie instances to tell other Nessie instances to invalidate certain cached objects, so that the cache stays consistent. A cache invalidation message instructs a Nessie instance to remove certain key(s) from its cache.</p> <p>Cache invalidation messages are rather \u201cmanagement\u201d concerns and the endpoints, even if protected by any mechanism, should not be publicly exposed. Thankfully Quarkus recently added the ability to separate management endpoints from public ones, Nessie incorporated this in the 0.82.0 release - management endpoints are available on port 9000 (default) and the Nessie REST APIs stay on the known port 19120. In other words, management endpoints are not publicly available, which is what we want.</p> <p>The Nessie endpoint to receive cache invalidation messages is only available on the management port. If you deploy Nessie via the Helm chart, or via our Kubernetes operator (not released yet), you don\u2019t need any additional configuration for distributed cache invalidations - it\u2019s setup and configured automatically. If you have your own Helm chart or custom deployment, make sure to configure the IPs of all Nessie instances via <code>nessie.version.store.persist.cache-invalidations.service-names</code>.</p> <p>Reference caching itself is not enabled by default. You can give it a try by setting these configuration options:</p> <ul> <li><code>nessie.version.store.persist.reference-cache-ttl</code> defines the time how long a reference will be kept in the cache.   A probably good value for this setting is a couple minutes, for example <code>PT15M</code>. If this parameter is not configured,   reference caching is disabled (default for now).</li> <li><code>nessie.version.store.persist.reference-cache-negative-ttl</code> defines the time how long the fact that a reference   does not exist will be kept in the cache. If you enable reference caching, it is strongly recommended to   configure this value as well. Nessie uses Git-like names for tags (<code>refs/tags/&lt;tag-name&gt;</code>) and branches   (<code>refs/heads/&lt;branch-name&gt;</code>), but the Nessie APIs use \u201csimplified\u201d/short \u201chuman friendly\u201d names - so just the   branch/tag name without the <code>refs/tags/</code>/<code>refs/heads</code> prefixes. To look up a reference by its name, Nessie performs   two read operations - one against each of the branch/tag prefixes - one exists and the other doesn\u2019t - therefore   we need to store \u201cnegative cache sentinels\u201d as well.</li> </ul> <p>PS: Kudos to Ben Manes for his awesome Window-Tiny-LFU implementation Caffeine and to the Quarkus project for their responsiveness!</p>"},{"location":"blog/2024/05/13/nessie-integration-of-iceberg-rest/","title":"Nessie integration of Iceberg REST","text":"<p>Dear Project Nessie and Apache Iceberg communities,</p> <p>We\u2019re proud to announce that we will soon integrate the Apache Iceberg REST spec into open-source Nessie! With this integration, you\u2019ll be able to use any client that supports the Iceberg REST spec with Nessie. We plan to roll out this new capability in the next few weeks.</p> <p>By integrating the Iceberg REST spec into Nessie, clients will work interchangeably with the Iceberg REST catalog and the Nessie catalog. In addition, Iceberg REST will enable Nessie to manage access to object storage more simply and securely by supporting S3 signed requests and session credentials. You\u2019ll be able to easily use the Iceberg REST catalog with Nessie by configuring your Iceberg client to point to the Nessie Iceberg REST base URI. The existing Nessie catalog in Iceberg will be unaffected by this change.</p> <p>We are actively working with the Apache Iceberg community and other major adopters to improve the Iceberg REST spec, add more features and functionality, and make it an even better option to use Nessie with Iceberg.</p> <p>Please reach out to us in the Project Nessie Zulip chat or Apache Iceberg community if you\u2019d like to ask questions or share feedback.</p> <p>Best,</p> <p>The Nessie OSS team</p>"},{"location":"blog/2024/05/24/support-for-mariadb-and-mysql/","title":"Support for MariaDB and MySQL","text":"<p>We are happy to announce that Nessie now supports MariaDB and MySQL as backends. This is a significant milestone for Nessie, as it opens up new possibilities for Nessie users.</p> <p>A big thank you to Vayuj Rajan for his contribution to this feature!</p>"},{"location":"blog/2024/05/24/support-for-mariadb-and-mysql/#why-mariadb-and-mysql","title":"Why MariaDB and MySQL?","text":"<p>MariaDB and MySQL are popular open-source relational databases that are widely used in the industry. By adding support for MariaDB and MySQL, we are making it easier for users to get started with Nessie.</p> <p>Important note: MariaDB and MySQL support is considered experimental at this time, and we are looking for feedback from the community to improve it further.</p>"},{"location":"blog/2024/05/24/support-for-mariadb-and-mysql/#how-does-it-work","title":"How does it work?","text":"<p>Nessie now ships with the MariaDB driver and the corresponding Quarkus extension. The MariaDB driver allows Nessie to work not only with MariaDB servers, but also with MySQL ones, thanks to MariaDB\u2019s protocol &amp; schema compatibility with MySQL.</p>"},{"location":"blog/2024/05/24/support-for-mariadb-and-mysql/#how-to-get-started-with-mariadb-or-mysql","title":"How to get started with MariaDB or MySQL","text":"<p>To get started with MariaDB or MySQL, you can follow the instructions in the Nessie documentation, but here is a quick step-by-step guide:</p> <ol> <li>Select the <code>JDBC</code> version store type as usual:</li> </ol> <pre><code>nessie.version.store.type=JDBC\n</code></pre> <ol> <li>Declare the JDBC datasource to use as either <code>mariadb</code> or <code>mysql</code> depending on your case:</li> </ol> <p>For MariaDB:</p> <pre><code>nessie.version.store.persist.jdbc.datasource=mariadb\n</code></pre> <p>For MySQL:</p> <pre><code>nessie.version.store.persist.jdbc.datasource=mysql\n</code></pre> <ol> <li>Configure the datasource using either the prefix <code>quarkus.datasource.mariadb.*</code> or    <code>quarkus.datasource.mysql.*</code>, depending on your case:</li> </ol> <p>For MariaDB:</p> <pre><code>quarkus.datasource.mariadb.jdbc.url=jdbc:mariadb://example.com:3306/my_db\nquarkus.datasource.mariadb.username=my_user\nquarkus.datasource.mariadb.password=${env:DB_PASSWORD}\n</code></pre> <p>For MySQL:</p> <pre><code>quarkus.datasource.mysql.jdbc.url=jdbc:mysql://example.com:3306/my_db\nquarkus.datasource.mysql.username=my_user\nquarkus.datasource.mysql.password=${env:DB_PASSWORD}\n</code></pre> <p>Check the Quarkus documentation for more details on how to configure the datasource.</p> <p>Important Notes: </p> <ul> <li> <p>When connecting to a MySQL server, the actual driver being used is the MariaDB driver, as it is   compatible with MySQL. You can use either <code>jdbc:mariadb:</code> or <code>jdbc:mysql</code> in your JDBC URL.   Generally, all MySQL JDBC URL parameters should still work, but your mileage may vary \u2013 we would   love to hear your feedback!</p> </li> <li> <p>When connecting to MariaDB, please do NOT set the following options to <code>true</code>; they are   incompatible with Nessie\u2019s current implementation and may cause somme commits to be rejected:</p> </li> </ul> <pre><code>useBulkStmts=false\nuseBulkStmtsForInserts=false\n</code></pre>"},{"location":"blog/2024/05/24/support-for-mariadb-and-mysql/#what-does-this-mean-for-existing-nessie-users","title":"What does this mean for existing Nessie users?","text":"<p>If you are already using Nessie with a version store type other than <code>JDBC</code>, then this change does not affect you.</p> <p>If you are using the <code>JDBC</code> version store type, you are probably using a PostgreSQL database. You can continue to use PostgreSQL without any modifications to your configuration, but please note that some properties are now deprecated and will be removed in a future release. We recommend that you update your configuration to use the new properties as soon as possible. Here is what you need to do:</p> <ol> <li>Include the new property below in your configuration:</li> </ol> <pre><code>nessie.version.store.persist.jdbc.datasource=postgresql\n</code></pre> <ol> <li>Migrate any property with the prefix <code>quarkus.datasource.*</code> to <code>quarkus.datasource.postgresql.*</code>.    For example, the below configuration:</li> </ol> <pre><code>quarkus.datasource.jdbc.url=jdbc:postgresql://example.com:5432/my_db\nquarkus.datasource.username=my_user\nquarkus.datasource.password=${env:DB_PASSWORD}\n</code></pre> <p>should be migrated to:</p> <pre><code>quarkus.datasource.postgresql.jdbc.url=jdbc:postgresql://example.com:5432/my_db\nquarkus.datasource.postgresql.username=my_user\nquarkus.datasource.postgresql.password=${env:DB_PASSWORD}\n</code></pre>"},{"location":"blog/2024/05/24/support-for-mariadb-and-mysql/#what-about-the-helm-chart","title":"What about the Helm chart?","text":"<p>The Nessie Helm chart has been updated to support MariaDB and MySQL. You can now deploy Nessie with MariaDB or MySQL as the version store backend. </p> <p>The main change is that the section called <code>postgres</code> in the <code>values.yaml</code> file has been renamed to <code>jdbc</code>. The old properties are still supported for backward compatibility, but we recommend that you update your configuration to use the new properties as soon as possible.</p> <p>Check the Nessie Helm chart documentation for more details, but here is a quick example of how to configure MariaDB or MySQL as the version store backend:</p> <pre><code>jdbc:\n  jdbcUrl: jdbc:mariadb://example.com:3306/my_db\n  secret:\n    name: mariadb-credentials\n    username: mariadb_user\n    password: mariadb_password\n</code></pre> <p>It\u2019s that simple! The Helm chart will take care of the rest.</p>"},{"location":"blog/2024/05/24/support-for-mariadb-and-mysql/#what-about-the-gc-tool","title":"What about the GC tool?","text":"<p>We have good news for you: the Nessie GC tool now supports MariaDB and MySQL. Check the Nessie GC tool documentation for more details. Again, please note: support for GC with these databases is also considered experimental, and we are looking for feedback from the community.</p> <p>As an example, here is how to create the SQL schema for the Nessie GC tool using MariaDB or MySQL:</p> <pre><code>java -jar nessie-gc.jar create-sql-schema \\\n  --jdbc-url jdbc:mariadb://example.com:3306/my_db \\\n  --jdbc-user my_user \\\n  --jdbc-password $DB_PASSWORD\n</code></pre> <p>That\u2019s it! You can now use the Nessie GC tool with MariaDB or MySQL.</p>"},{"location":"blog/2024/05/24/support-for-mariadb-and-mysql/#what-about-the-server-admin-tool","title":"What about the Server Admin tool?","text":"<p>The Nessie Server Admin tool has also been updated to support MariaDB and MySQL. Check the Nessie Server Admin tool documentation for more details.</p>"},{"location":"blog/2024/05/24/support-for-mariadb-and-mysql/#conclusion","title":"Conclusion","text":"<p>We hope that you are as excited as we are about this new feature. We are looking forward to your feedback and suggestions on how we can improve the MariaDB and MySQL support in Nessie. Please feel free to reach out to us on the Project Nessie Zulip chat or GitHub with your thoughts or questions.</p>"},{"location":"blog/2024/05/02/new-nessie-cli-tool/","title":"New Nessie CLI tool","text":"<p>There is a new CLI tool for Nessie, replacing the old Python based CLI.</p> <p></p> <p><code>pynessie</code> was the command line tool when working against Nessie from a command line. It exists since the early days of Nessie.  Since then, <code>pynessie</code> did not change very much and is still using the Nessie REST API v1, which is deprecated and will be removed at some point. <code>pynessie</code> also does not have support for the more complex OAuth2 device-code and authorization-code flows. </p> <p>We were thinking about how to deal with a CLI tool for Nessie, considering a bunch of options and choices:</p>"},{"location":"blog/2024/05/02/new-nessie-cli-tool/#functionalities-and-features-of-the-new-nessie-cli-so-far","title":"Functionalities and Features of the new Nessie CLI so far","text":"<ul> <li>It has a SQL-ish syntax.</li> <li>It\u2019s a REPL.</li> <li>It has built-in online <code>HELP</code> command (and <code>HELP &lt;command&gt;</code> command ;),   that also shows the commands\u2019 syntaxes in the online help. The same   information is also published on the projectnessie.org site.</li> <li>Auto-completion of commands, keywords and reference names (use TAB).</li> <li>Syntax highlighting (ugh - colors!).</li> <li>Paging of long results - showing a lot of content keys or a long commit   log just works like <code>less</code> on your Linux or macOS box.</li> <li>Command history, persisted in your home directory (can be turned off).</li> <li>\u201cJust\u201d run a one or more Nessie REPL commands - pass those as arguments   on the command line or as a script.</li> <li>It\u2019s small! Currently just ~14.5MB - not bad for a fully functional   REPL.</li> <li>Available commands to manage branches and tags, drop tables and views,   manage namespaces, list and show tables &amp; views, merge and help.</li> </ul>"},{"location":"blog/2024/05/02/new-nessie-cli-tool/#shall-we-continue-with-python-or-use-java","title":"Shall we continue with Python or use Java?","text":"<p>You may say: Python, that\u2019s it! Much easier to run! With easier people usually think of \u201cjust running it\u201d from bash or zsh. But is it really easier to run? You have to setup a virtual-env and have the (right) Python version in place.</p> <p>Dependency management in Python is (at least from our perspective) not an easy thing. API compatibility between versions of the dependencies is not always maintained.</p> <p>TL;DR we think, there\u2019s not really a benefit from using Python.</p>"},{"location":"blog/2024/05/02/new-nessie-cli-tool/#feature-parity","title":"Feature \u201cparity\u201d","text":"<p><code>pynessie</code>, since written in Python, requires that we duplicate everything (Nessie REST API model classes, REST calls,  authentication mechanisms, also OAuth2 flows) - resulting in a duplicate engineering effort. It\u2019s more work to do all that in Python.</p>"},{"location":"blog/2024/05/02/new-nessie-cli-tool/#repl","title":"REPL?","text":"<p>Having a REPL for the Nessie CLI, including a \u201cscripting language\u201d, would be cool, no? pynessie does not have that.</p>"},{"location":"blog/2024/05/02/new-nessie-cli-tool/#tool-startup-time","title":"Tool startup time","text":"<p>Admittedly, pynessie starts pretty quickly. However, nowadays Java tools start up pretty quick as well - if done right, you\u2019ll not notice, especially for an interactive tool (think: REPL).</p>"},{"location":"blog/2024/05/02/new-nessie-cli-tool/#what-about-the-nessie-content-generator-tool","title":"What about the <code>nessie-content-generator</code> tool?","text":"<p>That tool was, as the name suggests, a tool to generate dummy content in Nessie for testing purposes. It\u2019s never really been meant to be used as a CLI replacement.</p> <p><code>nessie-content-generator</code> is relatively easy to use - so there is some risk that you generate a lot of \u201cbroken\u201d content in a production Nessie repository, maybe not the best thing.</p>"},{"location":"blog/2024/05/02/new-nessie-cli-tool/#java-it-is","title":"Java it is!","text":"<p>We decided to go with Java, we have all the libraries that we need:</p> <ul> <li>picocli is really neat for command line options et al</li> <li>jline + jansi is a really cool library combination when working   with terminals - colors, REPL tool functionality, command history,   etc.</li> <li>Nessie client - we already have a fully featured client for Java   that also already supports all the OAuth2 flows, including   device-code and authorization-code.</li> </ul>"},{"location":"blog/2024/05/02/new-nessie-cli-tool/#do-we-want-quarkus-to-run-the-clirepl","title":"Do we want Quarkus to run the CLI/REPL?","text":"<p>Sure, Quarkus offers a lot of things, but there\u2019s nothing we really need - it can\u2019t even \u201celiminate\u201d dead code for us, because nothing in the Nessie Java client uses CDI.</p> <p>Also, with Quarkus we\u2019d have to require Java 17, but requiring \u201cjust\u201d Java 11 is probably nicer. So we don\u2019t use Quarkus.</p> <p>BTW: It also saves ~4MB in the uber-jar, and a couple dozen milliseconds startup time.</p>"},{"location":"blog/2024/05/02/new-nessie-cli-tool/#future-of-pynessie","title":"Future of <code>pynessie</code>","text":"<p>Finally, you may ask what will happen with <code>pynessie</code>: we will retain it as long as we support Nessie REST API v1. We may decide to drop the CLI part of <code>pynessie</code> and make it a pure Python client for Nessie - no final decision has been made yet.</p>"},{"location":"blog/2024/08/02/open-source-polaris-announcement/","title":"Open Source Polaris announcement","text":"<p>As many of you have already heard, the Polaris repository is now open source under the Apache License 2. The announcement mentions \u201cThe team at Dremio is excited to help bring the various functions and capabilities of Nessie into the Polaris project.\u201d. Let me elaborate on what this means for now and the foreseeable future.</p> <p>The Nessie project was started about 4 years ago and has attracted a lot of users creating a big community around it. We, the Nessie OSS team, are really proud of this development. We also look forward to further strengthening the community around Nessie.</p> <p>The intent is to contribute Nessie\u2019s capabilities, like Catalog Level Versioning, Git-like semantics, multi-table transaction semantics to Polaris. We look forward to working with the Polaris community in this regard.</p>"},{"location":"develop/","title":"Architecture","text":"<p>Nessie builds on the recent ecosystem developments around table formats. The rise of very large metadata and eventually consistent cloud data lakes (S3 specifically) drove the need for an updated model around metadata management. Where consistent directory listings in HDFS used to be sufficient, there were many features lacking. This includes snapshotting, consistency and fast planning. Apache Iceberg was created to help alleviate those problems.</p> <p>For more insight into why we created Nessie, you can read the founding blog post by one of Nessie\u2019s creators.</p>"},{"location":"develop/#inspiration","title":"Inspiration","text":"<p>The Iceberg format relies on a set of metadata files stored with (or near) the actual data tables. This allows Iceberg to fulfill the same role as the Hive Metastore for transactions without the need for expensive metadata scans or centralized planning (see Iceberg performance). This includes things such as partitioning (including hidden partitions), schema migrations, appends and deletes.  It does however require a pointer to the active metadata set to function. This pointer allows the Iceberg client to acquire and read the current schema, files and partitions in the dataset. Iceberg currently relies on the Hive metastore or hdfs to perform this role. The requirements for this root pointer store is it must hold (at least) information about the location of the current up-to-date metadata file, and it must be able to update this location atomically. In Hive this is accomplished by locks and in hdfs by using atomic file swap operations. These operations don\u2019t exist in eventually consistent cloud object stores, necessitating a Hive metastore for cloud data lakes. The Nessie system is designed to store the root metadata pointer and perform atomic updates to this pointer, obviating the need for a Hive metastore. Removing the need for a Hive metastore simplifies deployment and broadens the reach of tools that can work with Iceberg tables.</p> <p>The Nessie service is a lightweight Java-based REST API server. It uses a standard optimistic locking strategy to ensure atomic transactions. This relies on every operation carrying an expected hash state for the store and allows for a very light weight and scalable implementation. The implementation uses configurable authentication (e.g. IAM on AWS, JWT elsewhere) and a configurable backend (currently supporting RocksDB for single-node, and Apache Cassandra, Google BigTable, Amazon DynamoDB or MongoDB) and uses the optimistic locking features of cloud based key value stores to ensure scalability across servers. This architecture allows for Nessie to run in a docker container or in a number of other configurations.</p>"},{"location":"develop/content-types/","title":"Nessie content types","text":"<p>Nessie comes with built-in content types. Every content-type relates to a corresponding content-type ID (Java enum like string), a payload ID and an implementation of the <code>org.projectnessie.model.Content</code> interface.</p> <p>Nessie clients and servers need to know about the used content types via instances of <code>org.projectnessie.model.types.ContentTypeBundle</code>. Instances of this interface are loaded via the standard Java services mechanism.</p> <p>Nessie servers, and tools that directly access the Nessie repository, need the <code>ContentTypeBundle</code> and instances of <code>org.projectnessie.versioned.store.ContentSerializerBundle</code>, also provided via the Java services mechanism, that provide serialization code via instances of <code>org.projectnessie.versioned.store.ContentSerializer</code>.</p>"},{"location":"develop/content-types/#known-and-assigned-content-types","title":"Known and assigned content types","text":"Payload ID Content Type Model class Description Implementor 0 n/a n/a Legacy fallback value if the payload is unknown. n/a 1 <code>ICEBERG_TABLE</code> <code>org.projectnessie.model.IcebergTable</code> Iceberg tables. Project Nessie 2 <code>DELTA_LAKE_TABLE</code> <code>org.projectnessie.model.DeltaLakeTable</code> Delta Lake tables. Project Nessie 3 <code>ICEBERG_VIEW</code> <code>org.projectnessie.model.IcebergView</code> Iceberg views. Project Nessie 4 <code>NAMESPACE</code> <code>org.projectnessie.model.Namespace</code> Namespaces. Project Nessie 5 <code>UDF</code> <code>org.projectnessie.model.UDF</code> UDFs. Project Nessie <p>Since the ID values for payloads and the namespace for content types must be globally unique, please register your Payload ID and Content Type via an issue.</p>"},{"location":"develop/content-types/#implementing-your-own-content-types","title":"Implementing your own content types","text":"<p>TBD</p>"},{"location":"develop/content-types/#content-type-bundle","title":"Content type bundle","text":"<p><code>ContentTypeBundle</code>s make content types available to Nessie clients and servers.</p> <p>Needs a resource file <code>META-INF/services/org.projectnessie.model.types.ContentTypeBundle</code>, which contains the class name(s) that implement the <code>org.projectnessie.model.types.ContentTypeBundle</code> interface.</p> <p>The <code>ContentTypeBundle.register(ContentTypeRegistry contentTypeRegistry)</code> implementation must call the given <code>Registrar</code> with name of each content type and the model interface type that extends <code>org.projectnessie.model.Content</code>.</p>"},{"location":"develop/content-types/#serializer-bundle","title":"Serializer bundle","text":"<p><code>ContentSerializer</code>s provide (de)serialization functionality using a space efficient binary representation for Nessie servers.</p> <p>Needs a resource file <code>META-INF/services/org.projectnessie.versioned.store.ContentSerializerBundle</code>, which contains the class name(s) that implement the <code>org.projectnessie.versioned.store.ContentSerializerBundle</code> interface.</p> <p>The <code>ContentSerializerBundle.register(ContentSerializerRegistry registry)</code> implementation must call the given <code>ContentTypeRegistry</code> with the <code>ContentSerializer</code> implementation for each content type serializer.</p>"},{"location":"develop/content-types/#distributing-content-type-and-serializer-bundles","title":"Distributing content type and serializer bundles","text":"<p>TBD</p> <ul> <li>One jar for Nessie clients</li> <li>One jar for Nessie servers/tools</li> </ul> <p>Open questions:</p> <ul> <li>Dependencies??</li> </ul>"},{"location":"develop/java/","title":"Java","text":""},{"location":"develop/java/#java-client","title":"Java Client","text":"<p>Nessie has a thin client designed to be incorporated into existing projects with minimum  difficulty. The client is a thin layer over Nessie\u2019s openapi Rest APIs.</p> <p>To use the Nessie client, you can add it as a dependency to your Java project using  Maven. The coordinates are:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;org.projectnessie.nessie&lt;/groupId&gt;\n  &lt;artifactId&gt;nessie-client&lt;/artifactId&gt;\n  &lt;version&gt;0.106.0&lt;/version&gt;\n&lt;/dependency&gt; \n</code></pre> <p>For ease of integration with tools that carry many dependencies, the Nessie client\u2019s  dependencies are declared as <code>optional</code>. It is designed to work with  any recent version of JAX-RS client (Jersey and Resteasy are both tested inside Nessie\u2019s  tests) + Jackson\u2019s DataBinding and JAX-RS modules (any version from the last ~3+ years).</p>"},{"location":"develop/java/#api","title":"API","text":"<p>The <code>NessieClientBuilder</code> and concrete builder implementations (such as <code>HttpClientBuilder</code>) provide an easy way of configuring and building a <code>NessieApi</code>. The currently stable API that should be used is <code>NessieApiV2</code>, which can be instantiated as shown below:</p> <pre><code>import java.net.URI;\nimport java.util.List;\nimport org.projectnessie.client.api.NessieApiV2;\nimport org.projectnessie.client.NessieClientBuilder;\nimport org.projectnessie.model.Reference;\n\nNessieApiV2 api = NessieClientBuilder.createClientBuilder(null, null)\n  .withUri(URI.create(\"http://localhost:19120/api/v2\"))\n  .build(NessieApiV2.class);\n\napi.getAllReferences()\n  .stream()\n  .map(Reference::getName)\n  .forEach(System.out::println);\n</code></pre> <p>The following subsections will outline how different actions can be done via that Nessie API.</p>"},{"location":"develop/java/#fetching-details-about-a-particular-reference","title":"Fetching details about a particular Reference","text":"<p>Fetches the <code>Reference</code> object of the <code>main</code> branch and then gets its hash <pre><code>api.getReference().refName(\"main\").get().getHash();\n</code></pre></p>"},{"location":"develop/java/#creating-a-reference","title":"Creating a Reference","text":"<p>Creates a new branch <code>dev</code> that points to the <code>main</code> branch <pre><code>Reference main = api.getReference().refName(\"main\").get();\nReference branch =\n    api.createReference()\n        .sourceRefName(main.getName())\n        .reference(Branch.of(\"dev\", main.getHash()))\n        .create();\n</code></pre></p> <p>Creates a new tag <code>dev-tag</code> that points to the <code>main</code> branch <pre><code>Reference main = api.getReference().refName(\"main\").get();\nReference tag =\n    api.createReference()\n        .sourceRefName(main.getName())\n        .reference(Tag.of(\"dev-tag\", main.getHash()))\n        .create();\n</code></pre></p>"},{"location":"develop/java/#assigning-a-reference","title":"Assigning a Reference","text":"<p>Assigns a previously created <code>devBranch2</code> to the <code>dev</code> branch <pre><code>Reference dev = api.getReference().refName(\"dev\").get();\napi.assignBranch()\n    .branchName(\"devBranch2\")\n    .hash(dev.getHash())\n    .assignTo(dev)\n    .assign();\n</code></pre></p> <p>Assigns a previously created <code>dev-tag</code> to the <code>dev</code> branch <pre><code>Reference dev = api.getReference().refName(\"dev\").get();\napi.assignTag()\n    .tagName(\"dev-tag\")\n    .hash(dev.getHash())\n    .assignTo(dev)\n    .assign();\n</code></pre></p>"},{"location":"develop/java/#deleting-a-reference","title":"Deleting a Reference","text":"<p>Deletes a previously created branch <pre><code>api.deleteBranch()\n    .branchName(dev.getName())\n    .hash(dev.getHash())\n    .delete();\n</code></pre></p> <p>Deletes a previously created tag <pre><code>api.deleteTag()\n    .tagName(devTag.getName())\n    .hash(devTag.getHash())\n    .delete();\n</code></pre></p>"},{"location":"develop/java/#fetching-the-server-configuration","title":"Fetching the Server Configuration","text":"<pre><code>NessieConfiguration config = api.getConfig();\nconfig.getDefaultBranch();\nconfig.getVersion();\n</code></pre>"},{"location":"develop/java/#committing","title":"Committing","text":"<p>Creates a new commit by adding metadata for an <code>IcebergTable</code> under the specified <code>ContentKey</code> instance represented by <code>key</code> and deletes content represented by <code>key2</code></p> <pre><code>ContentKey key = ContentKey.of(\"your-namespace\", \"your-table-name\");\nContentKey key2 = ContentKey.of(\"your-namespace2\", \"your-table-name2\");\nIcebergTable icebergTable = IcebergTable.of(\"path1\", 42L);\napi.commitMultipleOperations()\n    .branchName(branch)\n    .hash(main.getHash())\n    .operation(Put.of(key, icebergTable))\n    .operation(Delete.of(key2))\n    .commitMeta(CommitMeta.fromMessage(\"commit 1\"))\n    .commit();\n</code></pre>"},{"location":"develop/java/#fetching-content","title":"Fetching Content","text":"<p>Fetches the content for a single <code>ContentKey</code> <pre><code>ContentKey key = ContentKey.of(\"your-namespace\", \"your-table-name\");\nMap&lt;ContentKey, Content&gt; map = api.getContent().key(key).refName(\"dev\").get();\n</code></pre></p> <p>Fetches the content for multiple <code>ContentKey</code> instances <pre><code>List&lt;ContentKey&gt; keys =\n  Arrays.asList(\n  ContentKey.of(\"your-namespace1\", \"your-table-name1\"),\n  ContentKey.of(\"your-namespace1\", \"your-table-name2\"),\n  ContentKey.of(\"your-namespace2\", \"your-table-name3\"));\nMap&lt;ContentKey, Content&gt; allContent = api.getContent().keys(keys).refName(\"dev\").get();\n</code></pre></p>"},{"location":"develop/java/#fetching-the-commit-log","title":"Fetching the Commit Log","text":"<p>Fetches the commit log for the <code>dev</code> reference <pre><code>LogResponse log = api.getCommitLog().refName(\"dev\").get();\n</code></pre></p>"},{"location":"develop/java/#fetching-entries","title":"Fetching Entries","text":"<p>Fetches the entries for the <code>dev</code> reference <pre><code>EntriesResponse entries = api.getEntries().refName(\"dev\").get();\n</code></pre></p>"},{"location":"develop/java/#merging","title":"Merging","text":"<p>This merges <code>fromBranch</code> into the given <code>intoBranch</code> <pre><code>api.mergeRefIntoBranch()\n  .branchName(\"intoBranch\")\n  .hash(intoBranchHash)\n  .fromRefName(\"fromBranch\")\n  .fromHash(fromHash)\n  .merge();\n</code></pre></p>"},{"location":"develop/java/#transplanting","title":"Transplanting","text":"<p>Transplant/cherry-pick a bunch of commits from <code>main</code> into the <code>dev</code> branch <pre><code>Branch dev = ...\napi.transplantCommitsIntoBranch()\n    .branchName(dev.getName())\n    .hash(dev.getHash())\n    .fromRefName(\"main\")\n    .hashesToTransplant(Collections.singletonList(api.getReference().refName(\"main\").get().getHash()))\n    .transplant()\n</code></pre></p>"},{"location":"develop/java/#authentication","title":"Authentication","text":"<p>Nessie has multiple <code>NessieAuthenticationProvider</code> implementations that allow different client authentication mechanisms as can be seen below. The documentation for how to configure Nessie server authentication can be found here.</p> <p>When configured with authentication enabled, a Nessie server expects every HTTP request to contain a  valid Bearer token in an <code>Authorization</code> header. Two authentication providers allow a Nessie client to automatically add the required token to the HTTP requests:</p> <ol> <li> <p>The <code>BearerAuthenticationProvider</code> is the simplest one and directly takes the Bearer token as a  parameter; the token must be valid for the entire duration of the client\u2019s lifetime:</p> <pre><code>NessieApiV2 api =\n  NessieClientBuilder.createClientBuilder(null, null)\n  .withUri(URI.create(\"http://localhost:19120/api/v2\"))\n  .withAuthentication(BearerAuthenticationProvider.create(\"bearerToken\"))\n  .build(NessieApiV2.class);\n</code></pre> </li> <li> <p>The <code>Oauth2AuthenticationProvider</code> is more elaborate; at a minimum, it takes an OAuth2 token  endpoint URI, a Client ID and a Client Secret, and uses them to obtain an access token from the  token endpoint, which is then used as a Bearer token to authenticate against Nessie:</p> <p><pre><code>Map&lt;String, String&gt; authConfig =\n    Map.of(\n        CONF_NESSIE_AUTH_TYPE, \"OAUTH2\",\n        CONF_NESSIE_OAUTH2_TOKEN_ENDPOINT,\n            \"https://&lt;oidc-server&gt;/realms/&lt;realm-name&gt;/protocol/openid-connect/token\",\n        CONF_NESSIE_OAUTH2_CLIENT_ID, \"my_client_id\",\n        CONF_NESSIE_OAUTH2_CLIENT_SECRET, \"very_secret\");\nNessieApiV2 api =\n    NessieClientBuilder.createClientBuilder(null, null)\n        .withUri(URI.create(\"http://localhost:19120/api/v2\"))\n        .withAuthenticationFromConfig(authConfig::get)\n        .build(NessieApiV2.class);\n</code></pre> Since Nessie 0.75.1, the <code>Oauth2AuthenticationProvider</code> can also be configured programmatically; this can be convenient if it\u2019s necessary to supply a custom SSL context, a custom executor or  custom Jackson object mapper: <pre><code>URI tokenEndpointUri = ...;\nSSLContext sslContext = ...;\nExecutorService executor = ...;\nObjectMapper objectMapper = ...;\nOAuth2AuthenticatorConfig authConfig =\n    OAuth2AuthenticatorConfig.builder()\n        .tokenEndpoint(tokenEndpointUri)\n        .clientId(\"my_client_id\")\n        .clientSecret(\"very_secret\")\n        .sslContext(sslContext)\n        .executor(executor) \n        .objectMapper(objectMapper)\n        .build();\nNessieApiV2 api =\n    NessieClientBuilder.createClientBuilder(null, null)\n        .withUri(URI.create(\"http://localhost:19120/api/v2\"))\n        .withAuthentication(OAuth2AuthenticationProvider.create(authConfig))\n        .build(NessieApiV2.class);\n</code></pre></p> </li> </ol> <p>The main advantage of the <code>Oauth2AuthenticationProvider</code> over <code>BearerAuthenticationProvider</code> is  that the token is automatically refreshed when it expires. It has more configuration options,  which are documented in the Tools Configuration section.</p>"},{"location":"develop/kernel/","title":"Commit Kernel","text":"<p>Nessie\u2019s production commit kernel is optimized to provide high commit throughput against a distributed key value store that provides record level CAS (compare-and-swap) capability or transactional/relational databases. The commit kernel is the heart of Nessie\u2019s operations and enables it to provide lightweight creation of new tags/branches, merges, and rebases, all with very high concurrent commit rate.</p>"},{"location":"develop/kernel/#high-level-abstract","title":"High level abstract","text":"<p>Nessie 1.0 comes with a version store (aka commit kernel) implementation that is different from both Git and older Nessie version store implementations in Nessie versions before 1.0 and is abstracted as illustrated below. Nessie generally supports both non-transactional key-value databases and transactional databases (relational).</p> <p>The goal of all implementations is to spread the keys as much as possible, so data can be properly distributed, and to keep the number of operations against a database low to reduce operation time.</p> <p>Contention by itself is not avoidable, because operations against Nessie are guaranteed to be atomic and consistent.</p>"},{"location":"develop/kernel/#nessie-content-types","title":"Nessie Content Types","text":"<p>The state of so called <code>Content</code> objects like <code>IcebergTable</code> or <code>DeltaLakeTable</code> represents the current state of a table in a data lake. Whenever a table has changed via for example Iceberg, a so-called commit operation instructs Nessie to record the new state in a Nessie commit, which carries the <code>Content</code> object(s).</p> <p><code>IcebergTable</code> contains the pointer to Iceberg\u2019s table metadata plus the IDs of the snapshot, schema, partition spec, sort order defined in the table metadata. - Iceberg\u2019s table metadata manages information is stored in the Nessie commit. - The value of the snapshot-ID, schema-ID, partition-spec-ID, sort-order-ID is stored per Nessie named reference (branch or tag). For more information, please refer the spec On Reference State vs Global State</p> <p>Updating global-state and on-reference-state are technically operations against two different entities in Nessie\u2019s backend database. Classic, relational databases (usually) come with a transaction manager, which ensures that changes to different tables appear atomically to other users. Much more scalable key-value stores do not have a transaction manager, but usually only provide so-called \u201cCompare-and-Swap\u201d (CAS) operations, which conditionally update a single key-value pair. This means, that the data model has to be fundamentally different for non-transactional key-value stores and transactional databases. Support for non-transactional databases, the data model, is designed in a way that only requires a single CAS operation to ensure atomicity and consistency even when committing two logical entities, namely the global-state and the on-reference-state, respectively the update to the \u201cHEAD\u201d of the updated branch. Some more details are outlined below.</p>"},{"location":"develop/kernel/#version-store-and-database-adapters","title":"Version Store and Database Adapters","text":"<p>The information on this page is outdated. <code>DatabaseAdapter</code> has been replaced with <code>Persist</code> along with its own <code>VersionStore</code> implementation.</p> <p>Nessie\u2019s REST API implementation works against the <code>VersionStore</code> interface, which defines the contract for the REST API, deals with concrete contents objects like <code>IcebergTable</code> or <code>DeltaLakeTable</code>.</p> <p><code>PersistVersionStore</code> is an implementation of <code>VersionStore</code> and translates between the content type objects like <code>IcebergTable</code> or <code>DeltaLakeTable</code> and the \u201cbinary\u201d (think: \u201cBLOB\u201d) representation in the database adapters.</p> <p><code>DatabaseAdapter</code> interface defining the content type independent mechanisms to perform Nessie operations like commit, transplants and merges as well as retrieving data.</p> <p><code>AbstractDatabaseAdapter</code> implements the commit logic, commit conflict detection and operations to retrieve information. There are these subclasses: </p> <ul> <li><code>NonTransactionalDatabaseAdapter</code> is used as a base for key-value stores.</li> <li>Implementation for DynamoDB</li> <li>Implementation for MongoDB</li> <li>Implementation for RocksDB</li> <li>Implementation for InMemory</li> <li><code>TransactionalDatabaseAdapter</code> JDBC based implementation relying on relational database   transactions for conflict resolution (rollback).</li> <li>SQL/DDL/type definitions for Postgres, Cockroach, H2</li> </ul>"},{"location":"develop/kernel/#non-transactional-key-value-databases","title":"Non-transactional key-value databases","text":"<p>The information on this page is outdated. \u201cglobal state\u201d has been removed from Nessie.</p> <p>The data model for non-transactional key-value databases relies on a single global-state-pointer, which is technically a table with a single row pointing to the current entry in the global-log, current entry in the ref-log and the \u201cHEAD\u201ds of all named references (branches and tags).</p> <p>The global-log contains changes to global-state, which is needed for backwards compatibility.</p> <p>The ref-log contains the history with details of operations  like COMMIT, MERGE, TRANSPLANT, CREATE_REFERENCE, DELETE_REFERENCE, ASSIGN_REFERENCE.</p> <p>The commit-log contains the individual Nessie commits.</p> <p>All commit, transplant and merge operations as well as other write operations like creating, reassigning or deleting a named reference work inside a so-called \u201cCAS loop\u201d, which technically works like the following pseudocode. A CAS operation can be imagined as an SQL like <code>UPDATE global_pointer SET value = :new_value WHERE primary_key = 1 AND value = :expected_value</code>.</p> <pre><code>// Pseudo definition of a Nessie write operation like a commit, merge, transplant, createReference,\n// assignReference, deleteReference.\nFunctionResult nessieWriteOperation(parameters...) {\n  while (true) {\n    globalPointer = loadGlobalPointer();\n\n    // Try the actual operation.\n    //\n    // Return the keys of the optimistically written rows in the commit log and global log,\n    // the changes to the global pointer and the result to be returned to the caller.\n    optimisticallyWrittenRows, updatesToGlobalPointer, functionResult\n      = performNessieWriteOperation(globalPointer, parameters);\n\n    // Try the CAS operation on the global pointer.\n    success = tryUpdateGlobalPointer(globalPointer, updatesToGlobalPointer);\n\n    if (success) {\n      // If the CAS oepration was successfully applied, return the function's result to the user.\n      return functionResult;\n    }\n\n    // CAS was not successful\n    deleteOptimisticallyWrittenRows(optimisticallyWrittenRows);\n    if (!retryPolicy.allowsRetry()) {\n      throw new RetryFailureException();\n    }\n  }\n}\n</code></pre>"},{"location":"develop/kernel/#transactional-databases","title":"Transactional databases","text":"<p>The data model for transactional databases defines tables for</p> <ul> <li>the global-state, where the primary key is the globally unique content-id and the   value of the global-state,</li> <li>the named-references, which define the commit hash/id of the \u201cHEAD\u201d of each named reference,</li> <li>the commit-log, which contains all commits</li> <li>the ref-log contains the history with details of operations   like COMMIT, MERGE, TRANSPLANT, CREATE_REFERENCE, DELETE_REFERENCE, ASSIGN_REFERENCE.</li> <li>the ref-log-head contains current head of the ref_log entry.</li> </ul> <p>All commit, transplant and merge operations as well as other write operations like creating, reassigning or deleting a named reference work inside a so-called \u201coperation loop\u201d, which is rather somewhat similar to the \u201cCAS loop\u201d for non-transactional databases, but does not need to keep track of optimistically written data and can directly use conditional SQL DML statements like <code>UPDATE table SET col = :value WHERE key = :key AND col = :expected_value</code> resp. <code>INSERT INTO...</code>. The database then comes back with either an update count &gt; 0 to indicate success or an update count = 0 to indicate failure or an integrity constraint violation error.</p>"},{"location":"develop/kernel/#tracing-metrics","title":"Tracing &amp; Metrics","text":"<p>Two delegating implementations of the <code>VersionStore</code> interface exist to provide metrics and tracing using Micrometer and OpenTracing.</p>"},{"location":"develop/kernel/#implemented-database-adapters","title":"Implemented database adapters","text":"<p>All current implementations are based on the abstractions in the Maven modules <code>:nessie-versioned-persist-adapter</code> + either <code>:nessie-versioned-persist-non-transactional</code> (for key-value stores) or <code>:nessie-versioned-persist-transactional</code> (for relational/transactional databases).</p> <ul> <li>Non-transactional</li> <li>InMemory (testing and prototyping)</li> <li>RocksDB</li> <li>MongoDB</li> <li>DynamoDB (planned)</li> <li>Transactional</li> <li>H2</li> <li>Postgres</li> </ul> <p>Note: not all database adapters are available via Nessie running via Quarkus!</p>"},{"location":"develop/kernel/#nessie-logic-vs-database-specific-adapters","title":"Nessie logic vs database specific adapters","text":"<p>The whole logic around commits, merges, transplants, fetching keys and values resides in AbstractDatabaseAdapter and is shared across all kinds of database adapters.</p> <p>Database adapters, for both transactional and non-transactional databases, have the database specific implementations around the CAS loop for non-transactional, catching integrity constraint violations for transactional, the concrete physical data model and the concrete read &amp; write implementations.</p>"},{"location":"develop/kernel/#logical-data-model","title":"Logical Data model","text":"<p>The DatabaseAdapter interface defines the functions needed by the version store implementation to access the data.</p> <p>Implementations of <code>DatabaseAdapter</code> are free to implement their own optimizations.</p>"},{"location":"develop/kernel/#non-transactional","title":"Non-transactional","text":"<p>Implementations are based on NonTransactionalDatabaseAdapter and only implement the database specific \u201cprimitives\u201d to unconditionally read and write records and perform the mandatory CAS (compare-and-swap) operation.</p> <p>Key-value stores are all non-transactional as those are built for scale-out. Most key-value stores support atomic CAS (compare-and-swap) operations against a single row/record, but atomic and conditional updates to multiple rows/records is either not supported at all or extremely slow.</p> <p>Nessie differentiates between content types that do require so called global-state and those that do not. Global-state is maintained globally and evaluated when a content value object is being retrieved, combined with the requested on-reference state on a Nessie commit. For Nessie commits, which are atomic, this means that Nessie has to update both the global-state and the on-reference-state for a content type that requires global state. While this is not an issue with a relational/transactional database, it is an issue in a key-value store. Nessie solves this with a single \u201cglobal pointer\u201d, which is updated using a CAS operation.</p> <p>Nessie commits (and similar operations like \u201ctransplant\u201d and \u201cmerge\u201d) optimistically write all the data to the commit log and global state log first and then try to perform the CAS operation against the global pointer. If the CAS operation succeeds, the Nessie commit operation has succeeded. If the CAS operation failed, all optimistically written rows are deleted and the whole Nessie commit is retried.</p> <p>The logical data model shared by all non-transactional database adapters consists of five entities:</p> <ul> <li>Global-pointer a single \u201ctable row\u201d that points to the current global-state-log and all HEADs   for all named references. Consistent updates are guaranteed via a CAS operation on this entity   comparing the HEAD of the global-state-log.</li> <li>Commit-log contains all commit log entries, identified by a deterministic hash. This is the same   as for transactional databases.</li> <li>Global-state-log contains all changes to the global state for content types that do require   global state. The row keys are random IDs.</li> <li>Key-lists acts as an \u201coverflow\u201d for large key lists that do not fit entirely into a single   commit log entry\u2019s embedded key list.</li> <li>Ref-log contains the history with details of operations     like COMMIT, MERGE, TRANSPLANT, CREATE_REFERENCE, DELETE_REFERENCE, ASSIGN_REFERENCE.</li> </ul>"},{"location":"develop/kernel/#transactional","title":"Transactional","text":"<p>Implementations are based on TxDatabaseAdapter and currently only implement the database specific nuances in the SQL syntax and Nessie data type mappings.</p> <p>The data for transactional database adapters consists of six tables:</p> <ul> <li>Named-references contains all named references and their current HEAD, the latter is used to   guarantee consistent updates.</li> <li>Global-state contains the current global state for a contents ID for content types that require   global state. Consistent changes are guaranteed by tracking a checksum</li> <li>value of the contents of the value representing the global state.</li> <li>Commit-log contains all commit log entries, identified by a deterministic hash. This is the same   as for non-transactional databases.</li> <li>Key-lists acts as an \u201coverflow\u201d for large key lists that do not fit entirely into a single   commit log entry\u2019s embedded key list.</li> <li>Ref-log contains the history with details of operations   like COMMIT, MERGE, TRANSPLANT, CREATE_REFERENCE, DELETE_REFERENCE, ASSIGN_REFERENCE.</li> <li>Ref-log-head contains current head of the ref_log entry.</li> </ul>"},{"location":"develop/kernel/#performance","title":"Performance","text":"<p>The non-transactional and transactional variants have different performance characteristics. As outlined above, the non-transactional variant uses a central global pointer and the transactional variant leverages the transaction manager of the database.</p> <p>The implementation can perform many hundred to many thousand commits per second, depending on the performance of the backend database and the characteristics of the use case. The two important factors are:</p> <ul> <li>Concurrent commits against different branches are \u201cfaster\u201d than concurrent commits against a   single branch</li> <li>Concurrent commits against the same table (think: Iceberg or Deltalake table) are slower than   concurrent commits against different tables.</li> </ul>"},{"location":"develop/kernel/#gatling-benchmarks","title":"Gatling Benchmarks","text":"<p>Nessie has a framework to simulate \u201chigher level use cases\u201d using Gatling. See the readmes here and here. Please note that all kinds of performance tests are only meaningful in production-like environments using production-like use cases.</p>"},{"location":"develop/kernel/#microbenchmarks","title":"Microbenchmarks","text":"<p>There are microbenchmarks available, which can be useful to investigate the overall performance of a database. Please note that performance tests, even microbenchmarks, are only meaningful in production-like environments using production-like use cases. See Nessie Persistence Microbenchmarks README.me .</p>"},{"location":"develop/kernel/#retry-mechanism","title":"Retry Mechanism","text":"<p>All write operations do support retries. Retries happen, if a non-transactional CAS operation failed or a transactional DML operation ran into an \u201cintegrity constraint violation\u201d. Both the number of retries and total time for the operation are bounded. There is an (exponentially increasing) sleep time between two tries. The actual values for the retry mechanism are configurable. </p>"},{"location":"develop/python/","title":"Python","text":"<pre><code># using python 3\npip install pynessie\n</code></pre>"},{"location":"develop/python/#configuration","title":"Configuration","text":"<p>When you install pynessie, you get the Python client along with a Python CLI. Configuration  for both is covered in our reference for the command line interface.</p> <p>Warn</p> <p>pynessie CLI has been deprecated in favor of the new Nessie CLI/REPL tool!</p>"},{"location":"develop/python/#usage","title":"Usage","text":"<p>To instantiate a client simply run</p> <pre><code>from pynessie import init\nclient = init() # this will look for the client config as per above\nbranches = client.list_branches()\nprint(branches)\n</code></pre> <p>All endpoint options are available from this client.</p>"},{"location":"develop/python/#spark-usage-from-python","title":"Spark usage from Python","text":"<p>A common way to interact with Nessie is via Spark. You can read more about working  with Nessie, Spark and Iceberg together on our Iceberg Client docs page.</p>"},{"location":"develop/python/#api-documentation","title":"API Documentation","text":"<p>API docs are hosted on readthedocs</p>"},{"location":"develop/repository-configs/","title":"Nessie repository configurations","text":"<p>Nessie allows to retrieve and persist configuration objects via its API. Read and/or write access to all or some types of repository configuration types can be restricted.</p> <p>The Nessie server side must know the schema of each repository configuration type via an interface that extends <code>org.projectnessie.model.RepositoryConfig</code> and its type must be registered. This ensures that all repository configuration objects comply with the implicitly defined schema.</p> <p>Creating and/or updating repository configurations is supposed to be a rare operation.</p> <p>Note: Nessie repository configurations are not supported with the old, legacy Nessie data model.  </p> <p>Nessie clients and servers need to know about the used repository config types via instances of <code>org.projectnessie.model.types.RepositoryConfigTypeBundle</code>. Instances of this interface are loaded via the standard Java services mechanism.</p>"},{"location":"develop/repository-configs/#known-and-assigned-repository-config-types","title":"Known and assigned repository config  types","text":"Repository Config Type Model class Description Implementor <code>GARBAGE_COLLECTOR</code> <code>org.projectnessie.model.GarbageCollectorConfig</code> Configuration for Nessie GC. Project Nessie <p>Since the ID values for repository config types must be globally unique, please register your Repository config  type via an issue.</p> <p>In case the (Java) client reads a repository configuration object for which it does not have the corresponding <code>RepositoryConfigTypeBundle</code>, it will provide an instance of <code>GenericRepositoryConfig</code>, which provides a <code>Map</code> representing the JSON attributes (Nessie uses Jackson for JSON (de)serialization). This means, that clients are always able to deserialize all repository configuration types, even if the matching <code>RepositoryConfigTypeBundle</code> is not available to the Nessie Java client. In case you are using types of repository configs that might not be available to the Nessie Java client, for example if the Nessie classes are relocated, as in Apache Iceberg, be prepared to get an instance of <code>GenericRepositoryConfig</code> instead of the \u201cright\u201d Java type. </p>"},{"location":"develop/repository-configs/#implementing-your-own-content-types","title":"Implementing your own content types","text":"<p>TBD</p>"},{"location":"develop/repository-configs/#repository-config-type-bundle","title":"Repository config type bundle","text":"<p><code>RepositoryConfigTypeBundle</code>s make repository config types available to Nessie clients and servers.</p> <p>Needs a resource file <code>META-INF/services/org.projectnessie.model.types.RepositoryConfigTypeBundle</code>, which contains the class name(s) that implement the <code>org.projectnessie.model.types.RepositoryConfigTypeBundle</code> interface.</p> <p>The <code>RepositoryConfigTypeBundle.register(RepositoryConfigTypeRegistry repositoryConfigTypeRegistry)</code> implementation must call the given <code>Registrar</code> with name of each repository config type and the model interface type that  extends <code>org.projectnessie.model.RepositoryConfig</code>.</p>"},{"location":"develop/rest/","title":"Rest API","text":"<p>Nessie\u2019s REST APIs are how all applications interact with Nessie. The APIs are specified  according to the openapi v3 standard and are available when running the server by at the path <code>/nessie-openapi/openapi.yaml</code> (for example via <code>curl http://127.0.0.1:19120/nessie-openapi/openapi.yaml</code>) and on the Download/Release pages.</p>"},{"location":"develop/spec/","title":"Nessie Specification","text":"<p>This page documents the complete Nessie specification. This includes:</p> <ul> <li>API and its constraints</li> <li>Contract for value objects</li> </ul>"},{"location":"develop/spec/#api-contract","title":"API contract","text":"<p>The Nessie API is used by Nessie integrations within for example Apache Iceberg and user facing applications like Web UIs.</p> <p>Nessie defines a REST API (OpenAPI) and implementations for Java and Python.</p>"},{"location":"develop/spec/#content-managed-by-nessie","title":"Content managed by Nessie","text":""},{"location":"develop/spec/#general-contract","title":"General Contract","text":"<p>Content Objects describe the state of a data lake object like a table or view. Nessie currently provides types for Iceberg tables views. Nessie uses two identifiers for a single Content object:</p> <ol> <li>The Content Id is used to identify a content object across all branches even    if the content object is being referred to using different table or view names.</li> <li>The Content Key is used to look up a content object by name, like a table name    or view name. The Content Key changes when the associated table or view is renamed.</li> </ol>"},{"location":"develop/spec/#content-key","title":"Content Key","text":"<p>The Content Key consists of multiple strings and is used to resolve a symbolic name, like a table name or a view name used in SQL statements, to a Content object.</p> <p>When a table or view is renamed using for example an SQL <code>ALTER TABLE RENAME</code> operation, Nessie will record this operation using a remove operation on the old key plus a put operation on the new key (see below).</p>"},{"location":"develop/spec/#on-reference-state-vs-global-state","title":"On Reference State vs Global State","text":"<p>Nessie is designed to support multiple table formats like Apache Iceberg.  Since different Nessie commits, think: on different branches in Nessie, can refer to the same physical table but with different state of the data and potentially different schema, some table formats require Nessie to refer to a single Global State.</p> <p>IDs of the Iceberg snapshot, Iceberg schema, Iceberg partition spec, Iceberg sort order  within the Iceberg table metadata are also stored per Nessie named reference (branch or tag), as the so-called on-reference-state.</p> <p>Note</p> <p>The term all information in all Nessie commits used above precisely means all information in all Nessie commits that are considered \u201clive\u201d, have not been garbage-collected by Nessie. See also Management Services.</p>"},{"location":"develop/spec/#content-id","title":"Content Id","text":"<p>All contents object must have an <code>id</code> field. This field is unique to the object and immutable once created. By convention, it is a UUID though this is not enforced by this Specification. There are several expectations on this field:</p> <ol> <li>Content Ids are immutable. Once created the object will keep the same <code>id</code> for its entire    lifetime.</li> <li>If the object is moved (e.g. stored under a different <code>Key</code>) it will keep the id.</li> <li>The same content object, i.e. the same content-id, can be referred to using different keys    on different branches.</li> </ol> <p>There is no API to look up an object by <code>id</code> and the intention of an <code>id</code> is not to serve in that capacity. An example usage of the <code>id</code> field might be storing auxiliary data on an object in a local cache and using <code>id</code> to look up that auxiliary data.</p> <p>Note</p> <p>A note about caching: The <code>Content</code> objects or the values of the referred information (e.g. schema, partitions etc.) might be cached locally by services using Nessie.</p> <p>For content types that do not track Global State, the hash of the contents object does uniquely reference an object in the Nessie history and is a suitable key to identify an object at a particular point in its history.</p> <p>Evolution of the Global State is performed in a way that keeps old contents resp. contents on different branches (and tags) available. This is the case for Apache Iceberg.</p> <p>Content types that do track Global State, the Content Id must be included in the cache key.</p> <p>For simplicity, it is recommeded to always include the Content Id.</p> <p>Since the Content object is immutable, the hash is stable and since it is disconnected from Nessie\u2019s version store properties it exists across commits/branches and survives GC and other table maintenance operations.</p> <p>The commit hash on the other hand makes a poor cache key because multiple commits can refer to the same state of a Content object, e.g. a merge or transplant will change the commit hash but not the state of the Content object.</p>"},{"location":"develop/spec/#content-types","title":"Content Types","text":"<p>Nessie is designed to support various table formats, and currently supports the following types. See also Tables &amp; Views.</p>"},{"location":"develop/spec/#iceberg-table","title":"Iceberg Table","text":"<p>Apache Iceberg describes any table using the so called table metadata, see Iceberg Table Spec. Each Iceberg operation that modifies data, for example an append or rewrite operation or more generally each Iceberg transaction, creates a new Iceberg snapshot. Any Nessie commit refers to a particular Iceberg snapshot for an Iceberg table, which translates to the state of an Iceberg table for a particular Nessie commit.</p> <p>The Nessie <code>IcebergTable</code> object passed to Nessie in a Put operation therefore consists of</p> <ol> <li>the pointer to the Iceberg table metadata and</li> <li>the IDs of the Iceberg snapshot, Iceberg schema, Iceberg partition spec, Iceberg sort order  within the Iceberg table metadata.  (so-called On Reference State)</li> </ol> <p>Note</p> <p>This model puts a strong restriction on the Iceberg table. All metadata JSON documents must be stored and none of the built-in iceberg maintenance procedures can be used. There are potentially serious issues regarding schema migrations in this model as well. Therefore, the Iceberg table spec should be considered subject to change in the near future.</p>"},{"location":"develop/spec/#iceberg-view","title":"Iceberg View","text":"<p>Note</p> <p>Iceberg Views are experimental and subject to change!</p> <p>The state of an Iceberg view is represented using the attributes <code>versionId</code>, <code>schemaId</code>, <code>sqlText</code> and <code>dialect</code>.</p> <p>Iceberg views are handled similar to Iceberg Tables.</p>"},{"location":"develop/spec/#operations-in-a-nessie-commit","title":"Operations in a Nessie commit","text":"<p>Each Nessie commit carries one or more operations. Each operation contains the Content Key and is either a Put, Delete or Unmodified operation.</p> <p>A Content Key must only occur once in a Nessie commit.</p> <p>Operations present in a commit are passed into Nessie as a list of operations.</p>"},{"location":"develop/spec/#mapping-sql-ddl-to-nessie-commit-operations","title":"Mapping SQL DDL to Nessie commit operations","text":"<p>A <code>CREATE TABLE</code> is mapped to one Put operation.</p> <p>An <code>ALTER TABLE RENAME</code> is mapped to a Delete operation using the Content Key for the table being renamed plus at least one Put operation using the Content Key of the table\u2019s new name, using the Content Id of the table being renamed.</p> <p>A <code>DROP TABLE</code> is represented as a Nessie Delete operation (without a Put operation for the same Content Id).</p> <p>A <code>DROP TABLE</code> + <code>CREATE TABLE</code> using the same table name (Content Key) in a single commit are mapped to one Put operation with a different Content Id.</p>"},{"location":"develop/spec/#put-operation","title":"Put operation","text":"<p>A Put operation modifies the state of the included Content object. It must contain the Content object and, if the Put operation modifies an existing content object, also the the expected contents. The expected contents attribute can be omitted, if the Content object refers to a new Content Id, e.g. a newly created table or view. See also Conflict Resolution.</p> <p>A Nessie Put operation is created for everything that modifies a table or a view, either its definition (think: SQL DDL) or data (think: SQL DML).</p>"},{"location":"develop/spec/#delete-operation","title":"Delete operation","text":"<p>A Delete operation does not carry any Content object and is used to indicate that a Content object is no longer referenced using the Content Key of the Delete operation.</p>"},{"location":"develop/spec/#unmodified-operation","title":"Unmodified operation","text":"<p>An Unmodified operation does not represent any change of the data, but can be included in a Nessie commit operation to enforce strict serializable transactions. The presence of an Unmodified operation means that the Content object referred to via the operation\u2019s Content Key must not have been modified since the Nessie commit\u2019s <code>expectedHash</code>.</p> <p>The Unmodified operation is not persisted.</p>"},{"location":"develop/spec/#version-store","title":"Version Store","text":"<p>See Commit Kernel for details.</p>"},{"location":"develop/spec/#conflict-resolution","title":"Conflict Resolution","text":"<p>The API passes an <code>expectedHash</code> parameter with a Nessie commit operation. This is the commit that the client thinks is the most up to date (its HEAD). The Nessie backend will check to see if the key has been modified since that <code>expectedHash</code> and if so, it will reject the requested modification with a <code>NessieConflictException</code>. This is basically an optimistic lock that accounts for the fact that the commit hash is global and nessie branch could have moved on from <code>expectedHash</code> without modifying the key in question.</p> <p>A Nessie Put operation that updates an existing content object must pass the so-called expected state, which might be used to compare the current recorded state of a content object with the state in the expected state in the Put operation. If both values differ, Nessie will reject the operation with a <code>NessieConflictException</code>.</p> <p>The reason for these conditions is to behave like a \u2018real\u2019 database. You shouldn\u2019t have to update your reference before transacting on table <code>A</code> because it just happened to update table <code>B</code> whilst you were preparing your transaction.</p>"},{"location":"downloads/","title":"Nessie 0.106.0 Downloads","text":"<p>Older release downloads are available via GitHub.</p> <p>Download options for this Nessie 0.106.0 release:</p> <ul> <li>Nessie Server</li> <li>CLI &amp; REPL</li> <li>GC Tool</li> <li>Server Admin Tool</li> <li>REST OpenAPI</li> <li>Maven Central</li> <li>License Reports</li> </ul> <p>See also the Nessie release page page for version 0.106.0 on GitHub</p>"},{"location":"downloads/#nessie-server","title":"Nessie Server","text":"<p>The main Nessie server serves the Nessie repository using the Iceberg REST API and Nessie\u2019s native REST API.</p> Docker ImageHelm ChartStandalone Jar <p>Docker images are multiplatform images for amd64, arm64, ppc64le and s390x. They are available from the following repositories:</p> <ul> <li>GitHub Container Registry:</li> </ul> <pre><code>docker pull ghcr.io/projectnessie/nessie:0.106.0\ndocker run -p 19120:19120 -p 9000:9000 ghcr.io/projectnessie/nessie:0.106.0\n</code></pre> <ul> <li>Quay.io:</li> </ul> <pre><code>docker pull quay.io/projectnessie/nessie:0.106.0\ndocker run -p 19120:19120 -p 9000:9000 quay.io/projectnessie/nessie:0.106.0\n</code></pre> <p>Nessie 0.106.0 Helm chart is available from the following locations:</p> <ul> <li>Nessie Helm Repo:</li> </ul> <pre><code>helm repo add nessie https://charts.projectnessie.org/\nhelm repo update\nhelm install my-nessie nessie/nessie --version \"0.106.0\"\n</code></pre> <ul> <li>Artifact Hub.</li> <li>Nessie 0.106.0 Helm Chart Tarball.</li> </ul> <p>Java version: minimum 17, 21 recommended, supported operating systems</p> <pre><code>curl -L -o nessie-quarkus-0.106.0-runner.jar \\\n  https://github.com/projectnessie/nessie/releases/download/nessie-0.106.0/nessie-quarkus-0.106.0-runner.jar\njava -jar nessie-quarkus-0.106.0-runner.jar\n</code></pre>"},{"location":"downloads/#nessie-cli-repl","title":"Nessie CLI &amp; REPL","text":"<p>Nessie CLI is both a command-line interface but primarily a REPL.</p> Docker ImageStandalone Jar <p>Docker images are multiplatform images for amd64, arm64, ppc64le, s390x. They are available from the following repositories:</p> <ul> <li>GitHub Container Registry:</li> </ul> <pre><code>docker pull ghcr.io/projectnessie/nessie-cli:0.106.0\ndocker run -it ghcr.io/projectnessie/nessie-cli:0.106.0\n</code></pre> <ul> <li>Quay.io:</li> </ul> <pre><code>docker pull quay.io/projectnessie/nessie-cli:0.106.0\ndocker run -it quay.io/projectnessie/nessie-cli:0.106.0\n</code></pre> <p>Requires Java 11, Java 21 recommended.</p> <pre><code>curl -L -o nessie-cli-0.106.0.jar \\\n  https://github.com/projectnessie/nessie/releases/download/nessie-0.106.0/nessie-cli-0.106.0.jar\njava -jar nessie-cli-0.106.0.jar\n</code></pre>"},{"location":"downloads/#nessie-gc-tool","title":"Nessie GC Tool","text":"<p>Nessie GC allows mark and sweep data files based on flexible expiration policies.</p> Docker ImageStandalone Jar <p>Docker images are multiplatform images for amd64, arm64, ppc64le, s390x. They are available from the following repositories:</p> <ul> <li>GitHub Container Registry:</li> </ul> <pre><code>docker pull ghcr.io/projectnessie/nessie-gc:0.106.0\ndocker run ghcr.io/projectnessie/nessie-gc:0.106.0 --help\n</code></pre> <ul> <li>Quay.io:</li> </ul> <pre><code>docker pull quay.io/projectnessie/nessie-gc:0.106.0\ndocker run quay.io/projectnessie/nessie-gc:0.106.0 --help\n</code></pre> <p>Requires Java 11, Java 21 recommended.</p> <pre><code>curl -L -o nessie-gc-0.106.0.jar \\\n  https://github.com/projectnessie/nessie/releases/download/nessie-0.106.0/nessie-gc-0.106.0.jar\njava -jar nessie-gc-0.106.0.jar\n</code></pre>"},{"location":"downloads/#nessie-server-admin-tool","title":"Nessie Server Admin Tool","text":"<p>Nessie\u2019s Server Admin Tool allows migration (export/import) of a Nessie repository.</p> Docker ImageStandalone Jar <p>Docker images are multiplatform images for amd64, arm64, ppc64le, s390x. They are available from the following repositories:</p> <ul> <li>GitHub Container Registry;</li> </ul> <pre><code>docker pull ghcr.io/projectnessie/nessie-server-admin:0.106.0\ndocker run ghcr.io/projectnessie/nessie-server-admin:0.106.0 --help\n</code></pre> <ul> <li>Quay.io;</li> </ul> <pre><code>docker pull quay.io/projectnessie/nessie-server-admin:0.106.0\ndocker run quay.io/projectnessie/nessie-server-admin:0.106.0 --help\n</code></pre> <p>Java version: minimum 17, 21 recommended, supported operating systems</p> <pre><code>curl -L -o nessie-server-admin-tool-0.106.0-runner.jar \\\n  https://github.com/projectnessie/nessie/releases/download/nessie-0.106.0/nessie-server-admin-tool-0.106.0-runner.jar\njava -jar nessie-server-admin-tool-0.106.0-runner.jar\n</code></pre>"},{"location":"downloads/#nessie-rest-api","title":"Nessie REST API","text":"Download <p>OpenAPI Download</p>"},{"location":"downloads/#nessie-artifacts-on-maven-central","title":"Nessie artifacts on Maven Central","text":"<p>Artifacts are available in two groups: <code>org.projectnessie.nessie</code> and <code>org.projectnessie.nessie-integrations</code>. Most users will only need the <code>org.projectnessie.nessie</code> group, which contains the Nessie server and CLI. The <code>org.projectnessie.nessie-integrations</code> group contains additional tools and integrations:</p> <ul> <li>Spark extensions</li> <li>Nessie GC tool</li> </ul> <p>Useful links:</p> <ul> <li>Nessie 0.106.0 BOM (Bill of Materials)</li> <li>Nessie 0.106.0 <code>org.projectnessie.nessie</code> artifacts</li> <li>Nessie 0.106.0 <code>org.projectnessie.nessie-integrations</code> artifacts</li> </ul> <p>The following examples show how to add the Nessie BOM to your build configuration:</p> MavenGradle (Kotlin) <p>In your Maven <code>pom.xml</code> add the Nessie BOM as a dependency: <pre><code>&lt;dependencyManagement&gt;\n  &lt;dependencies&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;org.projectnessie.nessie&lt;/groupId&gt;\n      &lt;artifactId&gt;nessie-bom&lt;/artifactId&gt;\n      &lt;version&gt;0.106.0&lt;/version&gt;\n      &lt;type&gt;pom&lt;/type&gt;\n      &lt;scope&gt;import&lt;/scope&gt;\n    &lt;/dependency&gt;\n  &lt;/dependencies&gt;\n&lt;/dependencyManagement&gt;\n</code></pre> Then you can use all Nessie artifacts like this: <pre><code>&lt;dependencies&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;org.projectnessie.nessie&lt;/groupId&gt;\n    &lt;artifactId&gt;nessie-client&lt;/artifactId&gt;\n  &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre></p> <p>In your Gradle project\u2019s <code>build.gradle.kts</code> add the Nessie BOM as an enforced platform: <pre><code>dependencies {\n  enforcedPlatform(\"org.projectnessie.nessie:nessie-bom:0.106.0\")\n}\n</code></pre> A full example using the <code>nessie-client</code> artifact: <pre><code>dependencies {\n  enforcedPlatform(\"org.projectnessie.nessie:nessie-bom:0.106.0\")\n  implementation(\"org.projectnessie.nessie:nessie-client\")\n}\n</code></pre></p>"},{"location":"downloads/#license-reports","title":"License Reports","text":"<p>License reports for this release are available via this link (zip file).</p>"},{"location":"guides/","title":"Overview","text":"<p>Nessie is an OSS service and libraries that enable you to maintain multiple versions  of your data and leverage Git-like Branches &amp; Tags for your Data Lake. Nessie enhances the  Apache Iceberg table format with version control techniques!</p> <p></p> <ul> <li>Try Nessie with Iceberg REST</li> <li>Configure Nessie with Iceberg REST</li> <li>About Nessie</li> <li>Introduction</li> <li>And the other guides</li> </ul> <p>For more production-like information, please refer to the</p> <ul> <li>Kubernetes Guide</li> <li>Configuration reference</li> </ul>"},{"location":"guides/about/","title":"About Nessie","text":"<p>Nessie is to Data Lakes what Git is to source code repositories. Therefore, Nessie uses many terms from both Git and data lakes.</p> <p>This page explains how Nessie makes working with data in data lakes much easier without requiring much prior knowledge of either Git or data lakes.</p> <p>Nessie is designed to give users an always-consistent view of their data across all involved data sets (tables). Changes to your data, for example from batch jobs, happen independently and are completely isolated. Users will not see any incomplete changes. Once all the changes are done, all the changes can be atomically and consistently applied and become visible to your users.</p> <p>Nessie completely eliminates the hard and often manual work required to keep track of the individual data files. Nessie knows which data files are being used and which data files can safely be deleted.</p> <p>Production, staging and development environments can use the same data lake without risking the consistent state of production data.</p> <p>Nessie does not copy your data, instead it references the existing data, which works fine, because data files<sup>1</sup> are immutable.</p>"},{"location":"guides/about/#nessie-101","title":"Nessie 101","text":"<ul> <li>Changes to the contents of the data lake are   recorded in Nessie as commits without copying   the actual data.</li> <li>Add meaning to the changes to your data lake.</li> <li>Always-consistent view to all the data.</li> <li>Sets of changes, like the whole work of a distributed Spark job.   or experiments of data engineers are   isolated in Nessie via branches. Failed jobs do not add additional harm to the data.</li> <li>Known, fixed versions of all data can be tagged.</li> <li>Automatic removal of unused data files (garbage collection).</li> </ul>"},{"location":"guides/about/#data-lake-101","title":"Data Lake 101","text":"<p>\u201cA data lake is a system or repository of data stored in its natural/raw format, usually object blobs or files.\u201d (cite from Wikipedia)</p> <p>Data is stored in immutable data files<sup>1</sup>. Each data file defines the schema of the data (i.e. names and types of the columns) and contains the data. A single, logical table (for example a <code>customers</code> or a <code>bank_account_transactions</code> table) consists of many data files.</p> <p>A common (mis)understanding of Data Lakes is \u201cthrow everything in and see what happens\u201d. This might work for some time, leaving data, especially large amounts of data, unorganized is a rather bad idea. A common best-practice is still to properly organize the (immutable) data files in directories that reflect both organizational (think: units/groups in your company) and structural (think: table schema) aspects.</p> <p>New data files can be added to the set of files for a particular table. Data files can also contain updates to and deletions of existing data. For example: if you need to make changes to the data in data-file <code>A</code>, you basically have to read that data-file, apply the changes and write a new data-file <code>A'</code> with the changes, which makes data-file <code>A</code> irrelevant.</p> <p>The amount of data held in data lakes is rather huge (GBs, TBs, PBs), and so is the number of tables and data files (100s of thousands, millions).</p> <p>Managing that amount of data and data files while keeping track of schema changes, for example adding or removing a column, changing a column\u2019s type, renaming a column in a table and views, is one of the things that Nessie tackles.</p> <p>Data in a data lake is usually consumed and written using tools like Apache Hive<sup>2</sup> or Apache Spark<sup>2</sup>. Your existing jobs can easily integrate Nessie without any production code changes, it\u2019s a simple configuration change.</p>"},{"location":"guides/about/#git-101","title":"Git 101","text":"<p>\u201cGit is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency\u201d (cite from git-scm.com)</p> <p>Git maintains the history or all changes of a software project from the very first commit until the current state.</p> <p>Git is used by humans, i.e. developers.</p> <p>Many of the concepts of Git for source code are implemented by Nessie for all the data in your data lake. It would be rather confusing to explain all Git concepts here and then outline the differences in the next chapter. If you want to learn more about Git, we recommend looking this Git book (available in many languages) or the About Git pages as a quick start.</p>"},{"location":"guides/about/#terms-summary","title":"Terms summary","text":"Term Meaning in Nessie Commit An atomic change to a set of data files. Hash Nessie-commits are identified by a commit id.<sup>3</sup> (Multi-table) transaction Since a Nessie commit can group data data files from many tables, you can think of a Nessie commit as a (multi-table) transaction. Branch Named reference to a commit. A new commit to a branch updates the branch to the new commit. Tag Named reference to a commit. Not automatically changed. Merge Combination of two commits. Usually applies the changes of one source-branch onto another target-branch."},{"location":"guides/about/#working-with-data-in-nessie","title":"Working with data in Nessie","text":"<p>Each individual state in Nessie is defined by a Nessie commit. Each commit in Nessie, except the very first one, has references to its predecessor, the previous versions of the data.</p> <p>For those who know Git and merge-commits: One important difference of Nessie-merges is that Nessie-commits have only one parent (predecessor). Nessie-merge operations technically work a bit different: the changes in branch to be merged are replayed on top of the target branch.</p> <p>Each Nessie commit also indirectly \u201cknows\u201d about the data files (via some metadata) in your data lake, which represent the state of all data in all tables.</p> <p>The following example illustrates that our current commit adds a 3<sup>rd</sup> data file. The other two data files 1+2 have been added by previous commit. <pre><code> +-------------------+       +-------------------------+\n |  previous commit  | --&lt;-- |     current commit      |\n +-------------------+       +-------------------------+\n     |         |                 |        |        |\n   (add)     (add)               |        |      (add)\n     |         |                 |        |        |\n  +------+  +------+          +------+ +------+ +------+\n  | data |  | data |          | data | | data | | data |\n  | file |  | file |          | file | | file | | file |\n  | #1   |  | #2   |          | #1   | | #2   | | #3   |\n  |     _|  |     _|          |     _| |     _| |     _|\n  |  __/    |  __/            |  __/   |  __/   |  __/  \n  |_/       |_/               |_/      |_/      |_/  \n</code></pre> In \u201crelational SQL\u201d you can think of the following sequence of SQL statements: <pre><code>BEGIN TRANSACTION;\n  -- The data for data file #1\n  INSERT INTO table_one (...) VALUES (...);\n  -- The data for data file #2\n  INSERT INTO other_table (...) VALUES (...);\n-- creates our \"previous commit\"\nCOMMIT TRANSACTION;\n\nBEGIN TRANSACTION;\n  -- Data added to 'table_one' will \"land\" in a new data file #3, because\n  -- data files are immutable.\n  INSERT INTO table_one (...) VALUES (...);\n-- Creates our \"current commit\"\nCOMMIT TRANSACTION;\n</code></pre></p> <p>Each commit is identified by a sequence of hexadecimal characters like <code>2898591840e992ec5a7d5c811c58c8b42a8e0d0914f86a37badbeedeadaffe</code><sup>3</sup>, which is not easy to read and remember for us humans.</p>"},{"location":"guides/about/#transaction-in-nessie","title":"Transaction in Nessie","text":"<p>The term \u201ctransaction\u201d has different meanings to different people coming from different backgrounds. It is probably fair to say that, in general, a transaction is a group of changes applied to some data.</p> <p>The term \u201ctransaction\u201d alone does not define any guarantees. Different systems provide different guarantees, for example whether (or: when) changes performed in a transaction become visible to others, whether (parts of) the data gets locked, and so on.</p> <p>Relational database systems (RDBMS) for example usually provide certain levels of isolation (think: others cannot see uncommitted changes) and also ensure that either a change within a transaction succeeds, the request times out or fails straight away. Relational databases have a single and central transaction-coordinator<sup>4</sup> and are designed to always provide a consistent data set.</p> <p>The smallest atomic change in Nessie is a single commit. It is fair to say, that a commit is the smallest possible transaction in Nessie.</p> <p>A single Nessie commit in Nessie:</p> <ul> <li>\u2026 can be \u201cjust\u201d the set of changes of a single worker out of many distributed   workers.</li> <li>\u2026 can cover a quite small change or cover a huge amount of changes and/or huge   amount of changed data or even group many Nessie commits into an atomic merge   operation (think: a transaction over many transactions).</li> </ul> <p>The major difference between \u201cNessie\u2019s (distributed) transactions\u201d and transactions in a relational database is that Nessie\u2019s concept of having multiple commits plus the concept of merging one branch into another branch provides a lot of flexibility.</p>"},{"location":"guides/about/#branches","title":"Branches","text":"<p>Nessie uses the concept of \u201cbranches\u201d to always reference the latest version in a chain of commits. Our example branch is named \u201cmain\u201d and has just a single commit: <pre><code> +-------------+\n |  commit #1  |\n +-------------+\n        ^\n        |\n        |\n      \"main\"\n      branch\n</code></pre> When we add changes to our \u201cmain\u201d branch, a new <code>commit #2</code> will be created:</p> <ul> <li>the new <code>commit #2</code> will reference <code>commit #1</code> as its predecessor and</li> <li>the named reference \u201cmain\u201d will be updated to point to our new <code>commit #2</code></li> </ul> <p><pre><code> +-------------+       +-------------+\n |  commit #1  | --&lt;-- |  commit #2  |\n +-------------+       +-------------+\n                              ^\n                              |\n                              |\n                            \"main\"\n                            branch\n</code></pre> This behavior ensures that the named reference \u201cmain\u201d always points to the very latest version of our data.</p>"},{"location":"guides/about/#working-branches-for-analytics-jobs","title":"Working-branches for analytics jobs","text":"<p>The above example with a single branch works well, if all changes to all tables can be grouped into a single commit. In a distributed world, computational work is distributed across many machines running many processes. All these individual tasks generate commits, but only the \u201csum\u201d of all commits from all the tasks represents a consistent state.</p> <p>If all the tasks of a job would directly commit onto our \u201cmain\u201d branch, the \u201cmain\u201d branch would be inconsistent at least until not all tasks have finished. Further, if the whole job fails, it would be hard to roll back the changes, especially if other jobs are running. Last but not least, the \u201cmain\u201d branch would contain a lot of commits (for example <code>job#213, task#47346, add 1234 rows to table x</code>), which do not make a lot of sense on their own, but a single commit (for example <code>aggregate-financial-stuff 2020/12/24</code>) would.</p> <p>To get around that issue, jobs can create a new \u201cwork\u201d-branch when they start. The results from all tasks of a job are recorded as individual commits into that \u201cwork\u201d-branch. Once the job has finished, all changes are then merged into the \u201cmain\u201d branch at once. <pre><code>    \"work\"\n    branch\n      |\n      |\n      v\n+-----------+\n| commit #1 |\n+-----------+\n      ^\n      |\n      |\n    \"main\"\n    branch\n</code></pre> Our example Spark job has two tasks, each generates a separate commit, which are only visible on our \u201cwork\u201d-branch: <pre><code>          task#1         task#2   \"work\"\n          result         result   branch\n            |                |     |\n            v                v     v\n      +-----------+       +-----------+\n      | commit #2 | --&lt;-- | commit #3 |\n      +-----------+       +-----------+\n         |\n         v\n         |\n+-----------+\n| commit #1 |\n+-----------+\n      ^\n      |\n      |\n    \"main\"\n    branch\n</code></pre> When the job has finished, you can merge the now consistent result back into the \u201cmain\u201d-branch. <pre><code>          task#1         task#2   \"work\"\n          result         result   branch\n            |                |     |\n            v                v     v\n      +-----------+       +-----------+\n      | commit #2 | --&lt;-- | commit #3 |\n      +-----------+       +-----------+\n         |                          |  \n         v                          ^\n         |                          |\n+-----------+                     +-----------+\n| commit #1 | --------&lt;---------- | commit #4 |  \n+-----------+                     +-----------+\n                                      ^\n                                      |\n                                      |\n                                    \"main\"\n                                    branch\n</code></pre></p> <p>Technically, Nessie replays <code>commit #2</code> and <code>commit #3</code> on top of the most-recent commit of the \u201cmain\u201d branch.</p> <p>For those who know Git and merge-commits: One important difference of Nessie-merges is that Nessie-commits have only one parent (predecessor). Nessie-merge operations technically work a bit different: the changes in branch to be merged are replayed on top of the target branch.</p> <p>It is recommended to give a commit a meaningful commit message and to let someone review the changes.</p> <p>As described above in Transactions in Nessie, the merge operation in the above example can be considered a Nessie distributed transaction.</p>"},{"location":"guides/about/#working-branches-for-humans","title":"Working branches for \u201chumans\u201d","text":"<p>You can also use \u201cdeveloper\u201d branches to run experiments against your data, test changes of your jobs etc.</p> <p>Production, staging and development environments can use the same data lake without risking the consistent state of production data.</p>"},{"location":"guides/about/#squashing","title":"Squashing","text":"<p>Nessie can not yet squash commits.</p>"},{"location":"guides/about/#tags","title":"Tags","text":"<p>Another type of named references are tags. Nessie tags are named references to specific commits. Tags do always point to the same commit and won\u2019t be changed automatically.</p> <p>This means, that tags are useful to reference specific commits, for example a tag named <code>financial-data-of-FY2021</code> could reference all sources of financial data relevant used for some financial year report.</p> <p>See Git tags for comparison and to learn how tagging works in Git.</p>"},{"location":"guides/about/#commit-messages-and-more","title":"Commit messages and more","text":"<p>As briefly mentioned above, every commit in Nessie has a set of attributes. Some of the more important ones are \u201csummary\u201d and \u201cdescription\u201d, which are exactly that - meaningful summaries and detailed descriptions that explain what has been changed and why it has been changed.</p> <p>In addition to \u201csummary\u201d and \u201cdescription\u201d, there are a bunch of additional attributes as shown in the following table. We plan to add more structure to these attributes in the future.</p> Attribute Meaning in Nessie commit timestamp The timestamp when the commit was recorded in Nessie. committer The one (human user, system id) that actually recorded the change in Nessie. author timestamp the timestamp when a change has been implemented (can be different from the commit timestamp). author The one (human user, system id) that authored the change, can be different if someone else actually commits the change to Nessie. summary A short, one-line meaningful summary of the changes. description potentially long description of the changes. \u2026 There are potentially way more attributes, just too many to mention here."},{"location":"guides/about/#garbage-collection","title":"Garbage collection","text":"<p>Data lakes contain a lot of data. The amount of data has a direct relation to the cost of ownership of a data lake. Keeping all data forever is probably going to be just too expensive, practically not useful and can also collide with data privacy regulations (for example GDPR or CCPA).</p> <p>Nessie keeps track of unused data files and collects the garbage for you. See Table Management</p>"},{"location":"guides/about/#footnotes","title":"Footnotes","text":"<ol> <li> <p>Common data file formats means Apache Iceberg Tables \u21a9\u21a9</p> </li> <li> <p>Apache, Hive, Spark, Iceberg, Parquet are trademarks of The Apache Software Foundation.\u00a0\u21a9\u21a9</p> </li> <li> <p>Nessie-commits are identified by a commit-id. All commits in Nessie (and in Git) are   identified using such a hash. The value of each hash is generated from the relevant contents   and attributes of each commit that are stored in Nessie.\u00a0\u21a9\u21a9</p> </li> <li> <p>There are distributed relational databases that are not implemented as a single monolith.   Those \u201cproper\u201d distributed relational databases use distributed consensus algorithms like   RAFT to provide the same (or even better) guarantees that classic relational databases give.   However, the concepts of a classic relational database still apply.\u00a0\u21a9</p> </li> </ol>"},{"location":"guides/best-practices/","title":"Best Practices","text":""},{"location":"guides/best-practices/#commit-messages","title":"Commit Messages","text":"<p>Give Nessie commits a meaningful commit summary and message, like <code>aggregate-financial-stuff 2020/12/24</code>, so people that look through the history of the data can grasp what that commit changes and why it\u2019s there.</p>"},{"location":"guides/best-practices/#reviews","title":"Reviews","text":"<p>Before merging manually performed changes back, it is really helpful to let someone else who is familiar with the topic, the changes applied in a work-branch (aka \u201cdevelopment branch\u201d), review the changes.</p>"},{"location":"guides/docker/","title":"Setting Up Nessie","text":"<p>As part of each release, Nessie is made available as a Docker image. This is the easiest and fastest way to try out Nessie locally and test all its capabilities.</p> <p>The primary repository for Nessie images is GitHub Container Registry. Images are also mirrored to Quay.io. Note that Nessie images are no longer published or synced to Docker Hub.</p> <p>The image is relatively small and builds on top of standard base images. To get started:</p> <pre><code>docker pull ghcr.io/projectnessie/nessie\n</code></pre> <p>You should see something like this:</p> <pre><code>Pulling from ghcr.io/projectnessie/nessie\n0fd3b5213a9b: Already exists\naebb8c556853: Already exists\na50558612231: Pull complete\nDigest: sha256:bda3dead4eb51a4c0ff87c7ce5a81ad49a37dd17d785f2549f4559f06cbf24d6\nStatus: Downloaded newer image for ghcr.io/projectnessie/nessie\n</code></pre> <p>Once the image is downloaded, you can start it with:</p> <pre><code>docker run -p 19120:19120 ghcr.io/projectnessie/nessie\n</code></pre> <p>You should see something like this:</p> <pre><code>Starting the Java application using /opt/jboss/container/java/run/run-java.sh ...\nINFO exec -a \"java\" java -XX:MaxRAMPercentage=80.0 -XX:+UseParallelGC -XX:MinHeapFreeRatio=10 -XX:MaxHeapFreeRatio=20 -XX:GCTimeRatio=4 -XX:AdaptiveSizePolicyWeight=90 -XX:+ExitOnOutOfMemoryError -cp \".\" -jar /deployments/quarkus-run.jar \nINFO running in /deployments\n _   _               _         ____\n| \\ | |             (_)       / __ \\\n|  \\| | ___  ___ ___ _  ___  / /__\\/ ___ _ ____   _____ _ __\n| . ` |/ _ \\/ __/ __| |/ _ \\ \\___. \\/ _ \\ '__\\ \\ / / _ \\ '__|\n| |\\  |  __/\\__ \\__ \\ |  __/ /\\__/ /  __/ |   \\ V /  __/ |\n\\_| \\_/\\___||___/___/_|\\___| \\____/ \\___|_|    \\_/ \\___|_|\n\n                               https://projectnessie.org/\n\n                                     Powered by Quarkus 3.5.0\n2024-01-27 17:49:20,685 INFO  [org.pro.eve.ser.EventSubscribers] (main) Starting subscribers...\n2024-01-27 17:49:20,685 INFO  [org.pro.eve.ser.EventSubscribers] (main) Done starting subscribers.\n2024-01-27 17:49:20,820 INFO  [org.pro.qua.pro.sto.PersistProvider] (main) Creating/opening version store IN_MEMORY ...\n2024-01-27 17:49:20,846 INFO  [org.pro.qua.pro.sto.PersistProvider] (main) Using IN_MEMORY version store, with 3903 MB objects cache\n2024-01-27 17:49:20,876 INFO  [io.quarkus] (main) nessie-quarkus 0.73.0 on JVM (powered by Quarkus 3.5.0) started in 2.229s. Listening on: http://0.0.0.0:19120\n2024-01-27 17:49:20,876 INFO  [io.quarkus] (main) Profile prod activated. \n2024-01-27 17:49:20,876 INFO  [io.quarkus] (main) Installed features: [agroal, amazon-dynamodb, cassandra-client, cdi, google-cloud-bigtable, hibernate-validator, jdbc-postgresql, logging-sentry, micrometer, mongodb-client, narayana-jta, oidc, opentelemetry, reactive-routes, resteasy, resteasy-jackson, security, security-properties-file, smallrye-context-propagation, smallrye-health, smallrye-openapi, swagger-ui, vertx]\n</code></pre> <p>Note</p> <p>If you see a warning about <code>OIDC Server is not available</code> in the logs, you can safely ignore it  for now. This happens in older Nessie versions because Nessie was configured to use OIDC by  default to authenticate users, but no OIDC server was configured out of the box.</p> <p>If you need to configure Nessie, you can do so by passing in environment variables. For example, you can change the port Nessie listens on by passing in the <code>QUARKUS_HTTP_PORT</code> environment variable:</p> <pre><code>docker run -p 19120:19120 ghcr.io/projectnessie/nessie\n</code></pre> <p>Check all the available configuration options in the  configuration reference.</p> <p>From there, you can use one of the three main Nessie integrations of:</p> <ul> <li>Take a look at your current empty repository in the Web UI</li> <li>NessieCatalog for Spark via Iceberg integration</li> <li>Try Nessie on Kubernetes, on your cloud provider or    on your laptop</li> </ul> <p>You can also install the Nessie CLI/REPL.</p> <pre><code>curl -L -o nessie-cli-0.106.0-runner.jar \\\n  https://github.com/projectnessie/nessie/releases/download/nessie-0.106.0/nessie-cli-0.106.0-runner.jar\n</code></pre>"},{"location":"guides/grafana/","title":"Nessie Grafana Dashboard","text":"<p>Nessie provides a Grafana Dashboard which can be used to gain insight into different metrics.  Additional docs can be found in the Nessie source code repository.</p>"},{"location":"guides/iceberg-rest/","title":"Configure Nessie with Iceberg REST","text":"<p>Warn</p> <p>Support for Iceberg REST is currently considered experimental in Nessie!</p> <p>With Iceberg REST, Nessie manages the metadata of the tables and views. On top, Nessie provides mechanisms like S3 request signing and S3 session credentials. Using Nessie with Iceberg therefore requires Nessie to have access to your object store.</p> <p>The object stores used by tables and views need to be defined in the Nessie configuration, for example using the following configuration example snippet:</p> <p>Warn</p> <p>Nessie requires at least one object store and one warehouse to be configured before you can use Nessie\u2019s Iceberg REST integration.</p> <pre><code># Name of the default warehouse\nnessie.catalog.default-warehouse=warehouse\n\n# The base location of the warehouse named \"warehouse\"\nnessie.catalog.warehouses.warehouse.location=s3://my-bucket\n\n# Another warehouse named \"sales\"\nnessie.catalog.warehouses.sales.location=s3://sales-data\n\n# Default/global S3 configuration settings\nnessie.catalog.service.s3.default-options.region=us-west-2\nnessie.catalog.service.s3.default-options.access-key=urn:nessie-secret:quarkus:my-secrets-default\nmy-secrets-default.name=awsAccessKeyId\nmy-secrets-default.secret=awsSecretAccessKey\n# For non-AWS S3 you need to specify the endpoint and possibly enable path-style-access\nnessie.catalog.service.s3.default-options.endpoint=http://localhost:9000\nnessie.catalog.service.s3.default-options.path-style-access=true\n\n# S3 configuration settings that are different for \"bucket1\"\nnessie.catalog.service.s3.buckets.sales.access-key=urn:nessie-secret:quarkus:my-secrets-for-sales\nmy-secrets-for-sales.name=awsAccessKeyIdForSales\nmy-secrets-for-sales.secret=awsSecretAccessKeyForSales\nnessie.catalog.service.s3.buckets.sales.region=us-east-1\n</code></pre> <p>See Server configuration Reference.</p> <p>Note</p> <p>Up to Nessie including version 0.91.2 the above property names had to be specified without the <code>default-options.</code> part.</p> <p>Tip</p> <p>Secrets can be encrypted using config encryption available in Quarkus. We do plan to support other secrets management systems.</p> <p>Info</p> <p>The above S3 credentials (access key ID + secret access key) are never passed exposed to a client.</p> <p>Info</p> <p>GCS and ADLS object store can be configured, but are considered \u201cexperimental\u201d at the moment.</p> <p>In addition to an object store, see the S3 example above, Nessie needs at least a default warehouse to be configured.</p> <pre><code>nessie.catalog.default-warehouse=warehouse\nnessie.catalog.warehouses.warehouse.location=s3://mybucket/my-lakehouse/\n</code></pre>"},{"location":"guides/iceberg-rest/#warehouses-storage-locations","title":"Warehouses &amp; Storage Locations","text":"<p>Iceberg stores data in object stores like S3, GCS or ADLS. Nessie pushes the necessary configuration to Iceberg clients via Iceberg REST. This information includes the object store type (Iceberg <code>FileIO</code>), the configuration for this and short-lived and down-scoped credentials.</p> <p>The object storage used for a table is identified by the <code>location</code> Iceberg table metadata property, considering the scheme (<code>s3</code>/<code>s3a</code>/<code>s3n</code>, <code>gs</code> or <code>abfs</code>/<code>abfss</code>) and the bucket/file-system name.</p> <p>\u201cWarehouses\u201d are simply speaking named object storage locations (or the default warehouse\u2019s storage location). Warehouses are relevant in Nessie when creating new tables and to provide the <code>location</code> namespace property.</p> <p>Info</p> <p>The currently open issues #9331, #9558 and #9559 will further allow both more flexibility but also more security with respect to storage locations.</p>"},{"location":"guides/iceberg-rest/#pre-existing-tables","title":"Pre-existing tables","text":"<p>In Iceberg it is possible, although maybe not very likely, that tables within the same catalog use different (base) locations - one table may \u201clive\u201d for example in <code>s3://my-bucket/</code> while another \u201clives\u201d in <code>gs://other-data/</code> and yet another in <code>s3://cold-data-bucket/</code>, potentially mixing one bucket in AWS S3 with a another bucket in a self-hosted Minio. </p>"},{"location":"guides/iceberg-rest/#warehouse","title":"\u201cWarehouse\u201d","text":"<p>The term \u201cwarehouse\u201d stems from Iceberg\u2019s meaning of a warehouse, which in turn comes from Spark\u2019s terminology for a \u201cwarehouse\u201d, which effectively means the (default) storage location.</p> <p>A \u201cwarehouse\u201d in Nessie is a named storage location - this \u201cwarehouse name\u201d is configured by the user in the Iceberg REST client using the <code>uri</code> parameter. If the <code>uri</code> does not define a specific \u201cwarehouse\u201d, Nessie uses the default \u201cwarehouse\u201d configured for Nessie.</p> <p>Each \u201cwarehouse\u201d in Nessie defines a (base) storage location, for example <code>s3://my-bucket/</code>.</p> <p>Iceberg configuration defaults and overrides can be configured for each warehouse.</p> <p>Note</p> <p>See above for an example Nessie configuration.</p>"},{"location":"guides/iceberg-rest/#example-iceberg-rest-uri-parameters","title":"Example Iceberg REST <code>uri</code> parameters","text":"<ul> <li>to connect to the default warehouse: <code>http://127.0.0.1:19120/iceberg</code></li> <li>to connect to the warehouse named \u201cpii\u201d: <code>http://127.0.0.1:19120/iceberg/|pii</code> - note the mandatory <code>|</code> character</li> <li>to connect to the warehouse named \u201csales\u201d using the Nessie branch \u201cexperiments\u201d: <code>http://127.0.0.1:19120/iceberg/experiments|sales</code></li> </ul> <p>Note</p> <p>It is mandatory to configure the default (unnamed) warehouse and the storage location that this warehouse references.</p>"},{"location":"guides/iceberg-rest/#creating-new-tables","title":"Creating new tables","text":"<p>The default storage <code>location</code> when creating a table is determined from the current \u201cwarehouse\u201d and the namespace in which the table is being created.</p> <p>Examples, considering the above example Nessie configuration, assuming that a namespace <code>foo</code> exists:</p> <ul> <li>Creating a table named <code>foo.fancy_table</code> when connecting to the default warehouse, would set the <code>location</code>   of that table by default to <code>s3://my-bucket/foo/fancy_table_&lt;random-UUID&gt;</code></li> <li>Creating a table named <code>foo.more_data</code> when connecting to the warehouse named \u201csales\u201d, would set the   <code>location</code> of that table by default to <code>s3://sales-data/foo/more_data_&lt;random-UUID&gt;</code></li> </ul> <p>Note</p> <p>After the table has been created using Iceberg\u2019s \u201cstaged table creation workflow\u201d, it is not possible to change the table\u2019s <code>location</code> property via Nessie.</p>"},{"location":"guides/iceberg-rest/#object-storage-configurations","title":"Object Storage Configurations","text":"<p>The Nessie configuration allows configuring default settings for each object storage type (S3, GCS, ADLS). Bucket (or ADLS file-system) specific settings can be configured as well.</p> <p>The effective configuration for an object storage location is determined by looking up the bucket specific settings. Configuration options that have not been explicitly configured in the bucket specific settings, will be taken from the default settings. If no bucket specific settings exists, only the default settings will be used.</p> <p>For example, considering the above example Nessie configuration:</p> <ul> <li>For a table <code>location</code> starting with <code>s3://my-bucket/</code>, the S3 configuration will use the endpoint   <code>http://localhost:9000</code> using path-style access and the region <code>us-west-2</code>.</li> <li>For a table <code>location</code> starting with <code>s3://sales-data/</code>, the S3 configuration will also use the endpoint   <code>http://localhost:9000</code> using path-style access but the region <code>us-east-1</code>.</li> </ul>"},{"location":"guides/iceberg-rest/#namespaces","title":"Namespaces","text":"<p>Some query engines require the <code>location</code> property to be set on namespaces. Nessie always returns the <code>location</code> property for every namespace. If the <code>location</code> has not been explicitly configured, it defaults to the current warehouse\u2019s storage location plus the namespace elements separated by slashes. If one of the parent namespaces defines the <code>location</code> property, the \u201cremaining\u201d path parts for the requested namespace are appended to that.</p>"},{"location":"guides/iceberg-rest/#accessing-a-table","title":"Accessing a table","text":"<p>When clients access a table, Nessie uses the <code>location</code> table property to find the object storage configuration by matching the <code>location</code> property against the object storage configurations. The \u201cwarehouse\u201d is irrelevant for the process of looking up the object storage configuration.</p> <p>Based on the object storage configuration, Nessie returns the necessary configuration to access the table by providing the matching Iceberg <code>FileIO</code> settings and, if configured, the down-scoped credentials.</p> <p>For example, considering the above example Nessie configuration:</p> <ul> <li>Loading the table metadata for a table having <code>location</code> set to <code>s3://my-bucket/foo/fancy_table</code>, Nessie   returns the <code>S3FileIO</code> type configured for the endpoint <code>http://localhost:9000</code> using path-style access and   the region <code>us-west-2</code>.</li> <li>Loading the table metadata for a table having <code>location</code> set to <code>s3://sales-data/foo/more_data</code>, Nessie   returns the <code>S3FileIO</code> type configured for the endpoint <code>http://localhost:9000</code> using path-style access but   the region <code>us-east-1</code>.</li> </ul>"},{"location":"guides/iceberg-rest/#object-store-credentials","title":"Object store credentials","text":"<p>The short-lived and down-scoped credentials that Nessie provides to clients according to the privileges the client has been granted (\u201cauthorization\u201d). Alternatively, Nessie allows using S3 signing, which provides the same guarantees.</p> <p>Warn</p> <p>Some query engines clients may request either only S3 request signing or vended credentials. It is important to configure/enable those mechanisms tailored for your specific use case / environment.</p> <p>Info</p> <p>See Server configuration Reference for details on how to configure object store credentials for Iceberg clients.</p> <p>Warn</p> <p>ADLS does not have a concept of IAM/STS like S3 or GCS. ADLS only provides the ability to restrict list/read/write/add/delete privileges for the whole file system, while S3 and GCS allow much finer grained access control on path as well. This is a restriction imposed on us by Azure.</p>"},{"location":"guides/iceberg-rest/#vendeddown-scoped-credentials-vs-request-signing","title":"Vended/down-scoped credentials vs request signing","text":"<p>Credential vending requires Nessie to ask an STS service (AWS, Google Cloud and some alternative S3 implementations like Minio) to generate object storage credentials with a privilege set matching the client\u2019s privileges on that table.</p> <p>S3 request signing on the other hand does not require any STS service, instead each individual request to S3 has to be signed by Nessie. This requires a REST call for every S3 request.</p> <p>Although credential vending requires an STS round-trip when loading the table-metadata, it does not require any additional round trips and is overall the faster and less resource intensive approach.</p> <p>S3 request signing might be the only option, if your S3 implementation does not provide an STS service.</p>"},{"location":"guides/iceberg-rest/#seamless-migration-from-nessie-to-nessie-with-iceberg-rest","title":"Seamless migration from \u201cNessie\u201d to \u201cNessie with Iceberg REST\u201d","text":"<p>You can safely use your current Nessie applications, those that use <code>type=nessie</code> when using Iceberg, concurrently with applications using Nessie via Iceberg REST (<code>type=rest</code> with a URI like <code>uri=http://127.0.0.1:19120/iceberg</code>).</p>"},{"location":"guides/iceberg-rest/#migrate-an-iceberg-client-configuration","title":"Migrate an Iceberg client configuration","text":"<p>To migrate existing Iceberg clients that use the <code>NessieCatalog</code> to use Nessie via Iceberg REST refer to the following table.</p> Iceberg option Old value New value Description/notes <code>type</code> <code>nessie</code> <code>rest</code> Change the catalog type from \u201cnessie\u201d to \u201crest\u201d. <code>catalog-impl</code> <code>...NessieCatalog</code> Use <code>type</code> = <code>rest</code> <code>uri</code> <code>http://.../api/v2</code> (or <code>v1</code>) <code>http://.../iceberg</code> Replace <code>api/v1</code> or <code>api/v2</code> with <code>iceberg</code>. If you want to connect to Nessie using a different branch, append the branch or tag name to the <code>uri</code> parameter, for example: <code>http://.../iceberg/my_branch</code>. To use a different warehouse (default storage location), append the <code>\\|</code> character followed by then name of the warehouse configured in Nessie. <code>ref</code> Nessie branch name n/a Migrate to <code>prefix</code> option <code>prefix</code> n/a (don\u2019t set, see description) Don\u2019t set this for Nessie. If you want to connect to Nessie using a different branch, append the branch or tag name to the <code>uri</code> parameter, for example: <code>http://.../iceberg/my_branch</code>. Note that setting the <code>prefix</code> parameter doesn\u2019t work for pyiceberg. <code>warehouse</code> * n/a Migrate object store configurations to the Nessie server configuration <code>io</code> * n/a Migrate object store configurations to the Nessie server configuration (all S3/GCS/ADLS settings) Remove all object store settings <p>Warn</p> <p>Current Iceberg REST clients do not support the OAuth2 authorization code and device code flows, like Nessie does!</p> <p>Only Bearer and client-ID/secret work. We recommend bearer tokens over client-ID/client-secret configuration to not put those credentials at the risk of being compromised - it is easier to revoke a single bearer token than to change a password used by many applications. Keep in mind that bearer token are only valid for some given period of time.</p> <p>We want to contribute the advanced OAuth2 functionality that already exists for Nessie client to Apache Iceberg. </p>"},{"location":"guides/iceberg-rest/#using-object-storage-file-layout","title":"Using \u201cobject storage\u201d file layout","text":"<p>Nessie respects the <code>write.object-storage.enabled=true</code> setting. With Nessie, it is not necessary to set the <code>write.data.path</code> (or <code>write.object-storage.path</code> or <code>write.folder-storage.path</code>), because Nessie automatically returns the table property <code>write.data.path</code> set to the warehouse location.</p> <p>Both S3 request signing and credentials vending (\u201cassume role\u201d) work with <code>write.object-storage.enabled</code>.</p>"},{"location":"guides/iceberg-rest/#noteworthy","title":"Noteworthy","text":"<ul> <li>The (base) location of tables created via Iceberg REST are mandated by Nessie, which will choose   the table\u2019s location underneath the location of the warehouse.</li> <li>Changes to the table base location are ignored.</li> <li>Nessie will always return only the Iceberg table snapshot that corresponds to the Nessie commit.   This solves the mismatch between Nessie commits and Iceberg snapshot history. Similarly Nessie   returns the Iceberg view version corresponding to the Nessie commit.</li> </ul>"},{"location":"guides/iceberg-rest/#nessie-cli","title":"Nessie CLI","text":"<p>The Nessie CLI has been enhanced with basic support for Iceberg REST when used with Nessie. It will transparently connect with Iceberg REST as well. You can also use the Iceberg REST base URI instead of the Nessie REST base URI.</p> <p>Tip</p> <p>If you use <code>CONNECT TO http://127.0.0.1:19120/api/v2 USING \"nessie.authentication.type\" = BEARER</code>, Nessie CLI will prompt you for the bearer token. If you use <code>CONNECT TO http://127.0.0.1:19120/api/v2 USING \"token\" = \"&lt;bearer token&gt;\"\"</code>, Nessie CLI will use bearer authorization for both Nessie and Iceberg REST APIs.</p>"},{"location":"guides/iceberg-rest/#time-travel-with-iceberg-rest","title":"Time travel with Iceberg REST","text":"<p>Nessie provides catalog level versioning providing a consistent and reproducible state across the whole catalog. In other words: atomic transactions across many tables and views and namespaces are natively built into Nessie. This \u201cGit for data\u201d approach allows, for example, merging all changes to tables and views in a branch into another branch, for example the \u201cmain\u201d branch.</p> <p>To retain Nessie\u2019s consistency and cross-branch/tag isolation guarantees, we have deliberately chosen to only return the state of a table or view as a single snapshot in Iceberg.</p> <p>You can still do time-travel queries by specifying the Nessie commit ID, or the branch/tag name, or a timestamp.</p> <p>Assuming you connect to Nessie using Iceberg REST with <code>prefix</code> set to <code>main</code> and you have a table called <code>my_table</code>.</p>"},{"location":"guides/iceberg-rest/#time-travel","title":"Time travel","text":"<p>The following example <code>SELECT</code>s the state/contents of the table <code>my_namespace.my_table</code> as of July 1<sup>st</sup>, 2024 at midnight UTC.</p> <p>Generally:</p> <ul> <li>the table name must be quoted using backticks (<code>`</code>)</li> <li>specify a branch or tag name after the at-char (<code>@</code>) (for example <code>table@branch</code>)</li> <li>specify a timestamp after the hash-char (<code>#</code>) (for example <code>table#2024-07-01T00:00:00Z</code>) or</li> <li>specify a Nessie commit ID after the hash-char (<code>#</code>) (for example <code>table#748586fa39e02bd1e359df105c6c08287ad5ed7a53235f71c455afb10fbff14c</code>) </li> </ul> <pre><code>SELECT * FROM nessie.my_namespace.`my_table#2024-07-01T00:00:00Z`;\n</code></pre> <p>Similarly, but for the time zone at offset -09:00:</p> <pre><code>SELECT * FROM nessie.my_namespace.`my_table#2024-07-01T00:00:00-09:00`;\n</code></pre> <p>Read from a different Nessie branch or tag:</p> <pre><code>SELECT * FROM nessie.my_namespace.`my_table@my_other_branch`;\n</code></pre> <p>Read from a different Nessie branch or tag at a specific timestamp:</p> <pre><code>SELECT * FROM nessie.my_namespace.`my_table@my_other_branch#2024-07-01T00:00:00Z`;\n</code></pre> <p>You can also specify Nessie commit IDs:</p> <pre><code>SELECT * FROM nessie.my_namespace.`my_table#748586fa39e02bd1e359df105c6c08287ad5ed7a53235f71c455afb10fbff14c`;\n</code></pre> <p><code>INSERT</code>ing or <code>UPDATE</code>ing data works similarly:</p> <pre><code>INSERT INTO nessie.my_namespace.`my_table@my_other_branch` ( id, val ) VALUES ( 123, 'some value' );\n</code></pre> <p><code>CREATE</code>ing a table on a different branch:</p> <pre><code>CREATE TABLE nessie.my_namespace.`my_table@my_other_branch` ( id INT, val VARCHAR );\n</code></pre>"},{"location":"guides/iceberg-rest/#customizing-nessie-commit-author-et-al","title":"Customizing Nessie commit author et al","text":"<p>It is possible to specify the author(s) and signed-off-by fields recorded in Nessie commits via Iceberg REST by using these Nessie specific REST/HTTP headers.</p> Header Meaning <code>Nessie-Commit-Authors</code> Comma separated list of authors to record and show in Nessie commit log. Example: <code>My Name &lt;name@domain.internal&gt;, Other user &lt;other@company.internal&gt;</code>. <code>Nessie-Commit-SignedOffBy</code> Comma separated list of signed-off-by to record and show in Nessie commit log. Example: <code>My Name &lt;name@domain.internal&gt;, Other user &lt;other@company.internal&gt;</code>. <code>Nessie-Commit-Message</code> Custom commit message, overrides all commit messages - use with care, because generated commit messages contain useful information."},{"location":"guides/introduction/","title":"Introduction","text":"<p>Nessie is an OSS service and libraries that enable you to maintain multiple versions  of your data and leverage Git-like Branches &amp; Tags for your Data Lake. Nessie enhances the following  table formats with version control techniques:</p> <ul> <li>Apache Iceberg Tables (more)</li> <li>Apache Iceberg Views (more)</li> </ul>"},{"location":"guides/introduction/#basic-concepts","title":"Basic Concepts","text":"<p>Nessie is heavily inspired by Git. The main concepts Nessie exposes map directly to  Git concepts. In most cases, you simply need to replace  references of files and directories in Git with Tables in Nessie. The primary concepts in Nessie are:</p> <ul> <li>Commit: Consistent snapshot of all tables at a particular point in time.</li> <li>Branch: Human-friendly reference that a user can add commits to.</li> <li>Tag: Human-friendly reference that points to a particular commit.</li> <li>Hash: Hexadecimal string representation of a particular commit.</li> </ul> <p>Out of the box, Nessie starts with a single branch called <code>main</code> that points to the  beginning of time. A user can immediately start adding tables to that branch. For example  (in pseudocode):</p> <pre><code>$ create t1\n...\n$ insert 2 records into t1\n...\n$ create t2\n...\n$ insert 2 records into t2\n...\n</code></pre> <p>A user can then use the Nessie CLI to view the history of the main branch. You\u2019ll see  that each operation was automatically recorded as a commit within Nessie:</p> <pre><code>$ nessie log\nhash4    t2 data added \nhash3    t2 created\nhash2    t1 data added\nhash1    t1 created\n</code></pre> <p>A user can then create a new tag referencing this point in time. After doing  so, a user can continue changing the tables but that point in time snapshot will  maintain that version of data.</p> <pre><code>$ nessie tag mytag hash4\n\n$ insert records into t1\n\n$ select count(*) from t1 join t2\n.. record 1 ..\n.. record 2 ..\n.. record 3 ..\n.. 3 records ..\n\n$ select count(*) from t1@mytag join t2@mytag\n.. record 1 ..\n.. record 2 ..\n.. only 2 records ..\n</code></pre>"},{"location":"guides/introduction/#data-and-metadata","title":"Data and Metadata","text":"<p>Nessie does not make copies of your underlying data. Instead, it works by versioning  separate lists of files associated with your dataset. Whether using Spark or  some other tool, each mutation operation you do will add or delete one or more files from  the definition of your table. Nessie keeps tracks of which files are related to each  of your tables at every point in time and then allows you to recall those as needed.</p>"},{"location":"guides/introduction/#scale-performance","title":"Scale &amp; Performance","text":"<p>Nessie is built for very large data warehouses. Nessie supports  millions of tables and thousands of commits/second. Because Nessie builds on top of Iceberg,  each table can have millions of files. As such, Nessie can support  data warehouses several magnitudes larger than the largest in the world today. This  is possible in large part due to the separation of transaction management (Nessie) from  table metadata management.</p>"},{"location":"guides/introduction/#technology","title":"Technology","text":"<p>Nessie can be deployed in multiple ways and is composed primarily of the Nessie service,  which exposes a set of REST APIs and a simple browser UI. This service works with multiple libraries to expose Nessie\u2019s version control capabilities to common data management technologies.</p> <p>Nessie was built as a Cloud native technology and is designed to be highly scalable,  performant and resilient. It is built  on Java and leveraging Quarkus. This makes Nessie work very well in Docker and FaaS environments.  Nessie has a pluggable storage backend and comes pre-packaged with support for Amazon\u2019s DynamoDB, Google BigTable, Apache Cassandra, PostgreSQL (incl. Cockroach), MariaDB, MySQL, and local storage using RocksDB.</p>"},{"location":"guides/introduction/#license-and-governance","title":"License and Governance","text":"<p>Nessie is Apache-Licensed and built in an open source, consensus-driven GitHub community.  Nessie was originally conceived and built by engineers at Dremio.</p>"},{"location":"guides/introduction/#getting-started","title":"Getting Started","text":"<ul> <li>Read more about Nessie transactions</li> <li>Get started with the Nessie quickstart.</li> </ul>"},{"location":"guides/keycloak/","title":"Authentication with Keycloak","text":"<p>In this guide we walk through the process of configuring a Nessie Server to authenticate clients against  a local Keycloak server. Docker is use at the runtime environments for both servers.</p>"},{"location":"guides/keycloak/#setting-up-keycloak","title":"Setting up Keycloak","text":"<p>For the purposes of this guide we will only do use a simple Keycloak configuration, that is still sufficient to demonstrate how OpenID authentication works in Nessie servers.</p> <p>First, start a Keycloak container using its latest Docker image.</p> <pre><code>docker run -p 8080:8080 -e KC_BOOTSTRAP_ADMIN_USERNAME=admin -e KC_BOOTSTRAP_ADMIN_PASSWORD=admin \\\n  --name keycloak quay.io/keycloak/keycloak:latest start-dev\n</code></pre> <p>Note the <code>admin</code> username and password. Those values will be required to log into the Keycloak Administration Console that should now be available at http://localhost:8080/admin/.</p> <p>Note: when using keycloak &lt; 17 change the URL to <code>http://localhost:8080/auth/admin/</code></p> <p>The default realm is called <code>Master</code>. On the left-hand pane find the <code>Manage &gt; Users</code> page and click <code>Add User</code> on the right side of the (initially empty) users table.</p> <p>Enter the username \u201cnessie\u201d and click <code>Save</code>. Now, under the <code>Credentials</code> tab of the <code>nessie</code> user page set password to <code>nessie</code> and turn off the <code>Temporary</code> flag. Click <code>Set Password</code>. Be sure also to remove all the <code>Required User Actions</code> if any.</p> <p>For the sake of convenience let\u2019s increase the default token expiration time. Goto <code>Clients</code> &gt; <code>admin-cli</code> &gt; <code>Advanced Settings</code>. Set <code>Access Token Lifespan</code> to 1 day and click <code>Save</code>.</p> <p>Now we are ready to generate an <code>access_token</code> for the <code>nessie</code> user. Use the following command to obtain a token. Then, store it in the <code>NESSIE_AUTH_TOKEN</code> environment variable. It will be required to access Nessie APIs later.</p> Plain CommandBash <pre><code>curl -X POST \\\n  http://localhost:8080/realms/master/protocol/openid-connect/token \\\n  --user admin-cli:none \\\n  -d 'username=nessie' \\\n  -d 'password=nessie' \\\n  -d 'grant_type=password'\n</code></pre> <pre><code>export NESSIE_AUTH_TOKEN=$(curl -X POST \\\n  http://localhost:8080/realms/master/protocol/openid-connect/token \\\n  --user admin-cli:none \\\n  -d 'username=nessie' \\\n  -d 'password=nessie' \\\n  -d 'grant_type=password' |jq -r .access_token\n  )\n</code></pre> <p>Note: when using keycloak &lt; 17 change the URL to <code>http://localhost:8080/auth/realms/master/protocol/openid-connect/token</code></p>"},{"location":"guides/keycloak/#setting-up-nessie-server","title":"Setting up Nessie Server","text":"<p>Start the Nessie server container from the <code>projectnessie/nessie</code> Docker image in authenticated mode, using the Keycloak server for validating user credentials.</p> <pre><code>docker run -p 19120:19120 \\\n  -e QUARKUS_OIDC_AUTH_SERVER_URL=http://localhost:8080/realms/master \\\n  -e QUARKUS_OIDC_CLIENT_ID=projectnessie \\\n  -e NESSIE_SERVER_AUTHENTICATION_ENABLED=true \\\n  --network host ghcr.io/projectnessie/nessie:latest\n</code></pre> <p>Note: when using keycloak &lt; 17 change the URL to <code>http://localhost:8080/auth/realms/master/protocol/openid-connect/token</code></p> <p>Note: this example uses a snapshot build. When Nessie 1.0 is released, the <code>latest</code> stable image will be usable with the instructions from this guide.</p>"},{"location":"guides/keycloak/#using-nessie-cli","title":"Using Nessie CLI","text":"<p>Now that the Nessie server runs in authenticated mode with a Keycloak, clients have to provide credentials in the form of bearer authentication tokens. For example:</p> <pre><code>nessie --auth-token $NESSIE_AUTH_TOKEN remote show\n</code></pre> <p>Note: since the name of the <code>NESSIE_AUTH_TOKEN</code> variable matches Nessie CLI configuration naming conventions, the client can automatically find it in the environment, and it does not have to be specified as a command line option. All <code>nessie</code> CLI command will automatically use that token for authenticating their requests. For example:</p> <pre><code>nessie log\n</code></pre>"},{"location":"guides/kubernetes/","title":"Nessie on Kubernetes","text":"<p>The easiest and recommended way to get started with Nessie on Kubernetes is to use the Helm chart described below. </p> <p>Note</p> <p>We are also working on a Kubernetes Operator for Nessie, but it is not available yet. If you are interested in deploying Nessie via an operator, please get in touch.</p> <p>Note</p> <p>See separate page about how to configure Nessie for using a reverse proxy like istio or nginx. </p> <p>For more information on Helm and Helm charts, see the Helm docs.</p>"},{"location":"guides/kubernetes/#installing-the-helm-chart","title":"Installing the Helm chart","text":"<p>Add the Nessie Helm repo:</p> <pre><code>helm repo add nessie-helm https://charts.projectnessie.org\nhelm repo update\n</code></pre> <p>Install the Helm chart in the <code>nessie-ns</code> namespace (create the namespace first if it doesn\u2019t exist), and name the release <code>nessie</code>:</p> <pre><code>helm install -n nessie-ns nessie nessie-helm/nessie\n</code></pre> <p>Additional docs (incl. all configuration settings) can be found in the Nessie Helm chart docs hosted in Nessie\u2019s GitHub repository. </p>"},{"location":"guides/kubernetes/#customizing-the-helm-chart","title":"Customizing the Helm chart","text":"<p>For example, to install the Helm chart with a predefined image, simply do this:</p> <pre><code>helm install -n nessie-ns nessie nessie-helm/nessie \\\n    --set image.repository=ghcr.io/projectnessie/nessie \\\n    --set image.tag=0.106.0\n</code></pre> <p>It\u2019s also useful to create more than one replica of the Nessie server. To do this, simply set the <code>replicaCount</code> value:</p> <pre><code>helm install -n nessie-ns nessie nessie-helm/nessie --set replicaCount=3\n</code></pre>"},{"location":"guides/kubernetes/#configuring-memory-and-cpu","title":"Configuring memory and CPU","text":"<p>By default, the Helm chart does not set any resource limits. It is generally recommended though to set memory requests and limits (usually to the same value), as well as CPU requests.</p> <p>There are no one-size-fits-all values for these settings, so you should adjust them according to your needs. A good starting point for a production deployment is to set the memory request and limit to 8Gi or higher, and the CPU request to a minimum of 4.</p> <p>For example, to set the memory request and limit to 8Gi, and the CPU request to 4 cores, you can do this:</p> <pre><code>helm install -n nessie-ns nessie nessie-helm/nessie \\\n    --set-string resources.requests.memory=8Gi \\\n    --set-string resources.limits.memory=8Gi \\\n    --set-string resources.requests.cpu=4\n</code></pre> <p>Regarding memory: when a limit is set with <code>resources.limits.memory</code>, Nessie will by default use 80% of that limit as the maximum heap size. For example, if the limit is 8Gi, then the maximum effective heap size will be 6.4Gi. If you want to change this, you can set the <code>JAVA_MAX_MEM_RATIO</code> environment variable to a different value. For example, to set the maximum heap size to 50% of the memory limit, you can do this:</p> <pre><code>helm install -n nessie-ns nessie nessie-helm/nessie \\\n    --set-string 'extraEnv[0].name=JAVA_MAX_MEM_RATIO' \\\n    --set-string 'extraEnv[0].value=50'\n</code></pre> <p>You can also set <code>JAVA_MAX_MEM_RATIO</code> to <code>0</code>, in which case the maximum heap size will not be constrained and will be only bounded by the container\u2019s memory limit size itself.</p> <p>See the server configuration page for more details about memory settings.</p>"},{"location":"guides/kubernetes/#configuring-database-authentication","title":"Configuring database authentication","text":"<p>Nessie supports a variety of version stores, each of which requires different configuration. For example, the JDBC version store requires a JDBC URL, username and password, while the DynamoDB version store requires AWS credentials. </p> <p>All database authentication options must be provided as Kubernetes secrets, and these must be created before installing the Helm chart.</p>"},{"location":"guides/kubernetes/#providing-secrets-for-jdbc-datastores","title":"Providing secrets for JDBC datastores","text":"<ul> <li>Make sure you have a Secret in the following form (assuming PostgreSQL, but the same applies to   other JDBC datastores):</li> </ul> <pre><code>&gt; cat $PWD/postgres-creds\npostgres_username=YOUR_USERNAME\npostgres_password=YOUR_PASSWORD\n</code></pre> <ul> <li>Create the secret from the given file:</li> </ul> <pre><code>kubectl create secret generic postgres-creds --from-env-file=\"$PWD/postgres-creds\"\n</code></pre> <ul> <li>The <code>postgres-creds</code> secret will now be picked up when you use <code>JDBC</code> as the version store  when installing Nessie (see below).</li> </ul>"},{"location":"guides/kubernetes/#providing-secrets-for-mongodb","title":"Providing secrets for MongoDB","text":"<ul> <li>Providing secrets for MongoDB is strongly recommended, but not enforced.</li> <li>Make sure you have a Secret in the following form:</li> </ul> <pre><code>&gt; cat $PWD/mongodb-creds\nmongodb_username=YOUR_USERNAME\nmongodb_password=YOUR_PASSWORD\n</code></pre> <ul> <li>Create the secret from the given file:</li> </ul> <pre><code>kubectl create secret generic mongodb-creds --from-env-file=\"$PWD/mongodb-creds\"\n</code></pre> <ul> <li>The <code>mongodb-creds</code> secret will now be picked up when you use <code>MONGODB</code> as the version store  when installing Nessie (see below).</li> </ul>"},{"location":"guides/kubernetes/#providing-secrets-for-cassandra","title":"Providing secrets for Cassandra","text":"<ul> <li>Providing secrets for Cassandra is strongly recommended, but not enforced.</li> <li>Make sure you have a Secret in the following form:</li> </ul> <pre><code>&gt; cat $PWD/cassandra-creds\ncassandra_username=YOUR_USERNAME\ncassandra_password=YOUR_PASSWORD\n</code></pre> <ul> <li>Create the secret from the given file:</li> </ul> <pre><code>kubectl create secret generic cassandra-creds --from-env-file=\"$PWD/cassandra-creds\"\n</code></pre> <ul> <li>The <code>cassandra-creds</code> secret will now be picked up when you use <code>CASSANDRA</code> as the version store  when installing Nessie (see below).</li> </ul>"},{"location":"guides/kubernetes/#providing-secrets-for-dynamodb","title":"Providing secrets for DynamoDB","text":"<ul> <li>Make sure you have a Secret in the following form:</li> </ul> <pre><code>&gt; cat $PWD/awscreds\naws_access_key_id=YOURACCESSKEYDATA\naws_secret_access_key=YOURSECRETKEYDATA\n</code></pre> <ul> <li>Create the secret from the given file:</li> </ul> <pre><code>kubectl create secret generic awscreds --from-env-file=\"$PWD/awscreds\"\n</code></pre> <ul> <li>The <code>awscreds</code> secret will now be picked up when you use <code>DYNAMODB</code> as the version store  when installing Nessie (see below).</li> </ul>"},{"location":"guides/kubernetes/#providing-secrets-for-bigtable","title":"Providing secrets for Bigtable","text":"<p>A secret is not required for Bigtable. If one is present, it is assumed that authentication will use a service account JSON key. See  this page for details on how to create a service account key.</p> <p>If no secret is used, then Workload Identity usage is assumed instead; in this case, make sure that the pod\u2019s service account has been granted access to BigTable. See this page for details on how to create a suitable service account.</p> <p>Important: when using Workload Identity, unless the cluster is in Autopilot mode, it is also required to add the following <code>nodeSelector</code> label:</p> <pre><code>iam.gke.io/gke-metadata-server-enabled: \"true\"\n</code></pre> <p>This is not done automatically by the chart because this selector would be invalid for Autopilot clusters.</p> <ul> <li>Make sure you have a Secret in the following form:</li> </ul> <pre><code>&gt; cat $PWD/bigtable-creds\nsa_json=YOUR_SA_JSON_KEY\n</code></pre> <ul> <li>Create the secret from the given file:</li> </ul> <pre><code>kubectl create secret generic bigtable-creds --from-env-file=\"$PWD/bigtable-creds\"\n</code></pre> <ul> <li>The <code>bigtable-creds</code> secret will now be picked up when you use <code>BIGTABLE</code> as the version store  when installing Nessie (see below).</li> </ul>"},{"location":"guides/kubernetes/#configuring-the-version-store","title":"Configuring the version store","text":""},{"location":"guides/kubernetes/#configuring-jdbc-version-stores","title":"Configuring JDBC version stores","text":"<p>Note</p> <p>When setting up your SQL backend, both the database (sometimes called catalog) and the  schema (sometimes called namespace, for backends that distinguish between database and schema)  must be created beforehand, as the Helm chart will not create them for you. Check your database  documentation for more information, especially around the <code>CREATE DATABASE</code> and <code>CREATE SCHEMA</code>  commands. You must also create a user with the necessary permissions to access the database and  schema.</p> <p>Let\u2019s assume that we want to use a PostgreSQL service, that the database is called <code>nessiedb</code> and the schema <code>nessie</code>. The PostgreSQL service is running at <code>postgres:5432</code> in the same namespace.</p> <p>Next, we need to configure the Helm chart to use the <code>JDBC</code> version store type and to pull the database credentials from the secret that was created previously. We can do this by creating a <code>values.yaml</code> file with the following content:</p> <pre><code>versionStoreType: JDBC\njdbc:\n  jdbcUrl: jdbc:postgresql://postgres:5432/nessiedb?currentSchema=nessie\n  secret:\n    name: postgres-creds\n    username: postgres_username\n    password: postgres_password\n</code></pre> <p>Let\u2019s now assume that we are using MariaDB or MySQL instead of PostgreSQL. These backends do not support schemas, thus only the database name needs to be provided. MariaDB and MySQL share the same JDBC driver (the MariaDB one), so the JDBC URL is roughly the same for both; a minimal JDBC URL for these backends would look like this:</p> <p>For MariaDB:</p> <pre><code>jdbcUrl: jdbc:mariadb://mariadb:3306/nessiedb\n</code></pre> <p>For MySQL:</p> <pre><code>jdbcUrl: jdbc:mysql://mysql:3306/nessiedb\n</code></pre> <p>In the above examples, <code>mariadb</code> and <code>mysql</code> are the service names of the MariaDB and MySQL services, respectively. The database name is <code>nessiedb</code>.</p> <p>Note</p> <p>The exact format of the JDBC URL may vary depending on the database you are using. Also, JDBC drivers usually support various optional connection properties. Check the documentation of  your database and its JDBC driver for more information (for PostgreSQL, check out this page and for MariaDB, check out this one).</p> <p>Note</p> <p>While the database and the schema must be created beforehand, the required tables can be created automatically by Nessie if they don\u2019t exist, in the target database and schema. If they  do exist, they will be used as-is. You must ensure that their structure is up-to-date with the version of Nessie that you are using. Check the Nessie release notes for more information on schema upgrades.</p> <p>Then, we can install the Helm chart with the following values:</p> <pre><code>helm install -n nessie-ns nessie nessie-helm/nessie -f values.yaml\n</code></pre>"},{"location":"guides/kubernetes/#configuring-mongodb-version-stores","title":"Configuring MongoDB version stores","text":"<p>Let\u2019s assume that we want to use a MongoDB database. The database must be created beforehand, as the Helm chart will not create it for you. Let\u2019s assume that the database is called <code>nessie</code>. The MongoDB service is running at <code>mongodb:27017</code> in the same namespace.</p> <p>Then, we need to configure the Helm chart to use the <code>MONGODB</code> version store type and to pull the database credentials from the secret that was created previously. We can do this by creating a <code>values.yaml</code> file with the following content:</p> <pre><code>versionStoreType: MONGODB\nmongodb:\n  database: nessie\n  connectionString: mongodb://mongodb:27017\n  secret:\n    name: mongodb-creds\n    username: mongodb_username\n    password: mongodb_password\n</code></pre>"},{"location":"guides/kubernetes/#configuring-dynamodb-version-stores","title":"Configuring DynamoDB version stores","text":"<p>Let\u2019s assume that we want to use a DynamoDB database in the <code>us-west-2</code> region. The tables will be created automatically by Nessie if they don\u2019t exist.</p> <p>Then, we need to configure the Helm chart to use the <code>DYNAMODB</code> version store type and to pull the AWS credentials from the secret that was created previously. We can do this by creating a <code>values.yaml</code> file with the following content:</p> <pre><code>versionStoreType: DYNAMODB\ndynamodb:\n  region: us-west-2\n  secret:\n    name: awscreds\n    awsAccessKeyId: aws_access_key_id\n    awsSecretAccessKey: aws_secret_access_key\n</code></pre>"},{"location":"guides/kubernetes/#configuring-bigtable-version-stores","title":"Configuring Bigtable version stores","text":"<p>Let\u2019s assume that we want to use a Bigtable instance named <code>nessie-bigtable</code> in the <code>prod-us</code> project, using the default profile id. The tables will be created automatically by Nessie if they don\u2019t exist, but the instance must be created and configured beforehand.</p> <p>Then, we need to configure the Helm chart to use the <code>BIGTABLE</code> version store type and to pull the Bigtable credentials from the secret that was created previously. We can do this by creating a <code>values.yaml</code> file with the following content:</p> <pre><code>versionStoreType: BIGTABLE\nbigtable:\n  projectId: prod-us\n  instanceId: nessie-bigtable\n  appProfileId: default\n</code></pre> <p>The above will use Workload Identity. If you are using instead a service account JSON key as described above, you can also specify it in the <code>values.yaml</code> file:</p> <pre><code>versionStoreType: BIGTABLE\nbigtable:\n  projectId: prod-us\n  instanceId: nessie-bigtable\n  appProfileId: default\n  secret:\n    name: bigtable-creds\n    key: sa_json\n</code></pre>"},{"location":"guides/kubernetes/#configuring-other-datasource-types","title":"Configuring other datasource types","text":"<p>Other datasource types are supported, and most of them have mandatory and optional configuration options. Again, check the Nessie Helm chart docs for more information.</p>"},{"location":"guides/kubernetes/#uninstalling-the-helm-chart","title":"Uninstalling the Helm chart","text":"<p>To uninstall the Helm chart and delete the <code>nessie</code> release from the <code>nessie-ns</code> namespace:</p> <pre><code>helm uninstall -n nessie-ns nessie\n</code></pre>"},{"location":"guides/kubernetes/#troubleshooting","title":"Troubleshooting","text":"<p>The first step in troubleshooting a Nessie Kubernetes deployment is to check the logs of the Nessie server pod. You can do this by running:</p> <pre><code>kubectl logs -n &lt;namespace&gt; &lt;pod&gt;\n</code></pre> <p>You can also check the status of the pod:</p> <pre><code>kubectl describe pod -n &lt;namespace&gt; &lt;pod&gt;\n</code></pre> <p>It\u2019s also possible to get a terminal into the Nessie pod\u2019s main container:</p> <pre><code>kubectl exec -it -n &lt;namespace&gt; &lt;pod&gt; -- /bin/bash\n</code></pre> <p>But beware that the container does not have some tools installed, e.g. <code>curl</code>, <code>wget</code>, etc. are not present.</p>"},{"location":"guides/kubernetes/#troubleshooting-connectivity","title":"Troubleshooting connectivity","text":"<p>Connectivity issues require more powerful tools. One useful technique is to run an ephemeral container in the same pod as the Nessie server, which shares the same network namespace and can access the Nessie server as <code>localhost</code>. This can be done with the <code>kubectl debug</code> command:</p> <pre><code>kubectl debug -it -n &lt;namespace&gt; &lt;pod&gt; --image=nicolaka/netshoot --target=nessie --share-processes\n</code></pre> <p>The above example uses the <code>nicolaka/netshoot</code> image, which contains a lot of useful tools for debugging. See the nicolaka/netshoot Docker Hub page for more information. The command should give you a shell in the Nessie pod, where you can use <code>curl</code>, <code>wget</code>, <code>netstat</code>, <code>dig</code>, <code>tcpdump</code>, etc. </p> <p>For example, once you get a shell in the debug container, you can:</p> <ul> <li>Check which processes are running in the Nessie pod with <code>ps aux</code>;</li> <li>Check Nessie\u2019s management API on port 9000 to see if the server is healthy:     <pre><code>curl http://127.0.0.1:9000/q/health\n</code></pre></li> <li>Check the Nessie server\u2019s API endpoint on port 19120:     <pre><code>curl http://127.0.0.1:19120/api/v2/config\n</code></pre></li> </ul>"},{"location":"guides/kubernetes/#advanced-nessie-jvm-troubleshooting","title":"Advanced Nessie JVM troubleshooting","text":"<p>JVM issues such as memory leaks, high CPU usage, etc. can be debugged using JVM tools.</p> <p>The Nessie container ships with few utilities, but it does have <code>jcmd</code>, a command-line utility for interacting with the JVM, installed.</p> <p>First, get a shell in the Nessie container:</p> <pre><code>kubectl exec -it -n &lt;namespace&gt; &lt;pod&gt; -c nessie -- /bin/bash\n</code></pre> <p>Then, you can use <code>jcmd</code> to capture a thread dump, heap dump, etc.:</p> <pre><code>jcmd 1 Thread.print\njcmd 1 GC.heap_dump /tmp/heapdump.hprof\n</code></pre> <p>Tip</p> <p>Nessie server PID is usually 1. You can double-check with <code>ps aux</code> or <code>jps</code>.</p> <p>If you need other JVM tools, such as <code>jfr</code> or <code>async-profiler</code>, a more complex setup is required.</p> <p>First, restart the Nessie pod with some extra JVM options. The most useful option to add is the <code>--XX:+StartAttachListener</code> JVM option; without it, the JVM will not allow attaching to it and tools like <code>jcmd</code> will fail.</p> <p>This can be done by modifying the pod template spec in the deployment spec, and adding/updating the <code>JAVA_OPTS_APPEND</code> environment variable:</p> <pre><code>java_opts_append=$(kubectl get deployment -n &lt;namespace&gt; &lt;deployment&gt; -o jsonpath='{.spec.template.spec.containers[0].env[?(@.name==\"JAVA_OPTS_APPEND\")].value}')\nkubectl set env -n &lt;namespace&gt; deployment &lt;deployment&gt; JAVA_OPTS_APPEND=\"$java_opts_append -XX:+StartAttachListener\"\n</code></pre> <p>Warning</p> <p>The above command will restart all Nessie pods! Unfortunately that\u2019s inevitable, because  environment variables cannot be changed in the pod spec directly, if it belongs to a deployment.</p> <p>Once the target pod is ready to be attached, you will need an image with the required tools. One example is the <code>lightrun-platform/koolkits/koolkit-jvm</code> image, which contains a JVM-based toolset for debugging Java applications:</p> <pre><code>kubectl debug -it -n &lt;namespace&gt; &lt;pod&gt; --image=lightruncom/koolkits:jvm --target=nessie --share-processes\n</code></pre> <p>See the JVM KoolKits page for more information. Beware that the image is quite large, so it may take some time to download.</p> <p>A few preliminary commands may need to be executed prior to be able to use the tools, for example if users and groups don\u2019t match: the Nessie process runs as UID 10000 and GID 10001 by default, while in many debug containers, the running user is root (UID 0). If that is the case, you won\u2019t be able to attach to the Nessie JVM. You can solve the problem by creating a new user with the same UID and GID as the Nessie process. In the case of JVM KoolKits, you also need to copy a few files before switching to the new user; here is the whole snippet to run:</p> <pre><code>addgroup --gid 10001 debug\nadduser --home /home/debug --uid 10000 --gid 10001 --disabled-password --gecos \"\" debug\ncp -Rf /root/.sdkman/ /home/debug/ &amp;&amp; chown -R 10000:10001 /home/debug/.sdkman\ncp /root/.bashrc /home/debug/ &amp;&amp; chown 10000:10001 /home/debug/.bashrc \nsu - debug\n</code></pre> <p>After running the above commands, you should be able to use <code>jps</code>, <code>jcmd</code>, <code>jmap</code>, etc. as well as other tools like <code>jfr</code> (Java Flight Recorder), etc. For example, you can use JFR to record a profile for 30 seconds:</p> <pre><code>jcmd 1 JFR.start duration=30s filename=/tmp/profile.jfr\n</code></pre> <p>Note that the profile will be saved in the Nessie container, not the debug container. You can copy it to your local machine with <code>kubectl cp</code>:</p> <pre><code>kubectl cp -n &lt;namespace&gt; &lt;pod&gt;:/tmp/profile.jfr profile.jfr\n</code></pre>"},{"location":"guides/kubernetes/#remote-debugging","title":"Remote debugging","text":"<p>If the above doesn\u2019t help, you can also enable remote debugging, by attaching to the Nessie pod with a remote debugger.</p> <p>Again, this will require restarting the Nessie pod with some extra JVM options. The most useful option to add is the <code>-agentlib:jdwp</code> JVM option, which enables the Java Debug Wire Protocol (JDWP).</p> <p>This can be done more easily done by simply setting the <code>JAVA_DEBUG</code> and <code>JAVA_DEBUG_PORT</code> environment variables in the deployment spec:</p> <pre><code>kubectl set env -n &lt;namespace&gt; deployment &lt;deployment&gt; JAVA_DEBUG=\"true\" JAVA_DEBUG_PORT=\"*:5005\"\n</code></pre> <p>Once the pod is ready to be debugged, you must forward the 5005 port to your local machine:</p> <pre><code>kubectl port-forward -n &lt;namespace&gt; &lt;pod&gt; 5005:5005\n</code></pre> <p>Then you can attach to the Nessie server with your favorite IDE or command-line debugger.</p> <p>Tip</p> <p>If you are using IntelliJ IDEA, you should create a new \u201cRemote JVM Debug\u201d run configuration.  Using the \u201cAttach to process\u2026\u201d option will not work, since it only supports attaching to local  JVM processes.</p>"},{"location":"guides/management/","title":"Management Services","text":"<p>Nessie can and needs to manage several operations within your data lake.</p> <p>Each management service can be scheduled and Nessie reports the outcome of each operation.</p>"},{"location":"guides/management/#garbage-collection","title":"Garbage Collection","text":"<p>Since Nessie is maintaining many versions of metadata and data-pointers simultaneously, you must rely on Nessie to clean up old data. Users should run Nessie GC regularly.</p> <p>Nessie GC needs to know which content versions need to be retained. To identify this so called \u201clive\u201d content, Nessie GC uses some rules which are applied on each named reference. Those rules are described below.</p> <p>Nessie GC is composed of multiple phases: 1. Identify (or \u201cmark\u201d) phase: Inspects the Nessie repository to identify all commits and    content version (in Iceberg terms: a table\u2019s snapshot). These so-called \u201ccontent references\u201d are    stored as a live-content-set, ideally in a separate database (see below for compatible     databases). This phase requires access to the Nessie repository, but does not require access to     the data lake. 2. Expire (\u201csweep\u201d) phase: Uses the actual table format (e.g. Iceberg) to map the content    references from a live-content-set to a set of file-references, which are then matched against    a recursive listing of all files for the respective tables. Files that are not contained in the    set of file-references are going to be deleted. Deletion either happens immediately or is    persisted in the live-content-set as a set of orphan files. 3. The delete phase can be split out of the expire phase, it basically means that orphan     files are first collected, so these can be inspected, and then explicitly deleted. </p> <p>All relevant operations required for Nessie GC can be run via the <code>nessie-gc.jar</code> tool, which can be downloaded from the release page on GitHub.</p> <p>Info</p> <p>Currently the GC algorithm only works for Iceberg tables. A supported JDBC database is  recommended as the storage for the live-content-sets.</p> <p>Info</p> <p>Information about Nessie GC can be found here.</p>"},{"location":"guides/management/#nessie-gc-tool","title":"Nessie GC tool","text":"<p>It is recommended to run all Nessie GC phases via the Nessie GC command line tool <code>nessie-gc.jar</code>, which can be downloaded from the release page on GitHub.</p> <p>The Nessie GC tool comes as an uber-jar packaged with everything you need to run Nessie GC against a data lake using Iceberg.</p> <p>Note</p> <p>Use <code>java -jar nessie-gc.jar help</code> to get a list of commands supported by the Nessie GC tool.</p>"},{"location":"guides/management/#setting-up-the-database-for-nessie-gc","title":"Setting up the database for Nessie GC","text":"<p>Note</p> <p>The Nessie GC tool is compatible with the following databases: PostgreSQL (production-ready), MariaDB and MySQL (experimental at the moment).</p> <p>You can create the tables in two ways:</p> <p>Manually: use the DDL statements emitted by <code>java -jar nessie-gc.jar show-sql-create-schema-script</code> as a template that can be enriched with database specific optimizations.</p> <p>Or alternatively, let the Nessie GC tool create the schema in your existing database, for example like this for PostgreSQL:</p> <pre><code>java -jar nessie-gc.jar create-sql-schema \\\n  --jdbc-url jdbc:postgresql://127.0.0.1:5432/nessie_gc \\\n  --jdbc-user pguser \\\n  --jdbc-password mysecretpassword\n</code></pre> <p>Note</p> <p>Instead of specifying the JDBC parameters, especially the password, everytime on the command line, most command line option values can be specified via environment variables. The naming scheme follows this Java pseudo-code: <code>\"NESSIE_GC_\" + optionName.substring(2).replace('-', '_').toUpperCase()</code>. For example, the <code>--jdbc-password</code> command line option\u2019s value is taken from the environment variable <code>NESSIE_GC_JDBC_PASSWORD</code>.</p> <p>Note</p> <p>The availability of the database for Nessie GC is not critical for Nessie itself. Nessie does not require anything from Nessie GC to continue to work.</p> <p>Note</p> <p>For small, experimental Nessie repositories, that do not access any production data lake information, you can experiment with the <code>java -jar nessie-gc.jar gc</code> command, which also  accepts the <code>--inmemory</code> command line option, which does not require an external database for live-content-set persistence. In fact, the <code>--inmemory</code> option does not persist anything and keeps the live-content-set information in memory. The <code>gc</code> command runs the identity, expire and delete phases sequentially.</p>"},{"location":"guides/management/#live-content-sets","title":"Live content sets","text":"<p>All Nessie GC operations work on exactly one so-called \u201clive content set\u201d. Each live content set is composed of:</p> <ul> <li>Unique ID each live-content-set is identified by a UUID. The <code>java -jar nessie-gc.jar    mark-live</code> command emits the ID of the live-content-set to the console, but it\u2019s recommended to    write the new live-content-set ID to a file using the <code>--write-live-set-id-to</code> option. Other    commands that work on a live-content-set allow reading the ID of the live-content-set using the    command line option <code>--read-live-set-id-from</code>.</li> <li>Status tracks the state and/or progress of a live-content-set and is used to know whether   the identify and sweep phases started resp. ended and whether those finished successfully   or with an error. If, for example, the identify phase did not finish successfully, the sweep   phase cannot be started. A summary of the error message is stored with the live-content-set.</li> <li>Timestamps of when the identify and expire phases started and completed.</li> <li>collection of content-IDs as the result of the identify phase</li> <li>set of content-references for each content-ID as the result of the identify phase</li> <li>set of base-table-locations as the result of the sweep phase</li> <li>set of file-references to be deleted as the result of the identify phase, if Nessie GC   was told to defer deletes using the <code>--defer-deletes</code> command line option.</li> </ul> <p>A couple of <code>nessie-gc.jar</code> commands allow the listing of all and inspection of individual live-content-sets. Those are:</p> <ul> <li><code>list</code> lists all live-content-sets, starting with the most recent live-set.</li> <li><code>show</code> shows information about one live-content-set, optionally with details about the content   references or base-locations or deferred deletes.</li> <li><code>list-deferred</code> to show the file-references from a sweep phase with the <code>--defer-deletes</code>   option.</li> <li><code>deferred-deletes</code> to delete the files referenced by file-references collected during a sweep   phase with the <code>--defer-deletes</code> option.</li> <li><code>delete</code> deletes a live-content-set.</li> </ul>"},{"location":"guides/management/#running-the-mark-or-identify-phase-identifying-live-content-references","title":"Running the mark (or identify) phase: Identifying live content references","text":"<p>The mark or identify phase is run via the <code>mark-live</code> (or <code>identify</code> as an alias) <code>nessie-gc.jar</code> command.</p> <pre><code>java -jar nessie-gc.jar mark-live \\\n  --jdbc... # JDBC settings omitted in this example\n</code></pre> <p>It will walk the commits in all named references, and collect all content-references from the visited Nessie commits. So called \u201ccut off policies\u201d define, when the mark phase should stop walking the commit log for a named reference. The default \u201ccut off policy\u201d is <code>NONE</code>, which means that all Nessie commits and therefore all contents in the named references using the <code>NONE</code> policy are considered live.</p> <p>Note</p> <p>Since the mark phase requires access to Nessie, make sure to use the <code>--uri</code> command line option to configure the Nessie endpoint and the <code>--nessie-option</code> command line option to configure additional Nessie client parameters, for example a bearer token. The Nessie repository is never modified by Nessie GC.</p> <p>Note</p> <p>The mark phase does not access the data lake nor does it use Iceberg.</p>"},{"location":"guides/management/#cut-off-policies","title":"Cut off policies","text":"<p>Nessie GC supports three types of cut off policies:</p> <ul> <li><code>NONE</code>, not explicitly selectable via the CLI, it is the implicit default when a named reference   has no matching policy. It means, there is no cut-off time, everything in the named reference is   considered \u201clive\u201d.</li> <li>by number of commits: The given number of most recent commits are considered live.</li> <li>by cut off timestamp: All commits that are younger than the configured timestamp are   considered live.</li> <li>by cut off duration: Similar to cut off timestamp, all commits younger <code>now - duration</code> are   considered live. In the Nessie GC tool, a duration is always converted to a timestamp using a   common reference timestamp.</li> </ul> <p>Relevant command line options for <code>java -jar nessie-gc.jar mark-live</code> (alias <code>java -jar  nessie-gc.jar identify</code>):</p> <ul> <li><code>--cutoff reference-name-regex=cut-off-policy</code> the specified <code>cut-off-policy</code> is applied to all   named references that match the given reference name regular expression.</li> <li><code>--cutoff-ref-time</code> Defaults to \u201cnow\u201d, but can also be configured to another timestamp, if   necessary.</li> </ul> <p>Cut-off policies are parsed using the following logic and precedence:</p> <ol> <li>An integer number is translated to the cut-off-policy using number of commits.</li> <li>The string representation of a <code>java.time.Duration</code> is translated to a duration. Java durations    string representation starts with <code>P</code> followed by the duration value. Examples:<ul> <li><code>P10D</code> means 10 days</li> <li><code>PT10H</code> means 10 hours</li> </ul> </li> <li>The string representation of a <code>java.time.format.DateTimeFormatter.ISO_INSTANT</code> is translated    to an exact cut-off-timestamp. Example using UTC: <code>2011-12-03T10:15:30Z</code></li> </ol> <p>Note</p> <p>Nessie GC\u2019s mark phase processes up to 4 named references in parallel. This setting can be changed using the <code>--identify-parallelism</code> command line option.</p>"},{"location":"guides/management/#running-the-sweep-or-expire-phase-identifying-live-content-references","title":"Running the sweep (or expire) phase: Identifying live content references","text":"<p>Nessie GC\u2019s sweep phase uses the actual table format, for example Iceberg, to map the collected live content references to live file references. The sweep phase operates on each content-ID. So it collects the live file references for each content ID. Those file references refer to Iceberg assets:</p> <ul> <li>metadata files</li> <li>manifest lists</li> <li>manifest files</li> <li>data files</li> </ul> <p>After the expire phase identified the live file references for a content-ID, it collects all files in the base table locations. While traversing the base locations, it collects the files that are definitely not live file references. Those non-live file references are then deleted, aka immediate orphan files deletion.</p> <p>As an alternative, the expire phase can just record the orphan files instead of immediately deleting those. This is called deferred deletion in Nessie GC.</p> <p>Configuration options for Iceberg and Hadoop can be specified using the <code>--iceberg</code> and <code>--hadoop</code> options. Examples: <code>--iceberg s3.access-key-id=S3_ACCESS_KEY</code> and <code>--hadoop fs.s3a.access.key=S3_ACCESS_KEY</code>.</p> <p>Example of running the expire command follows.</p> <pre><code>java -jar nessie-gc.jar expire --live-set-id 0baaa1ff-90db-4ee5-b6d2-b60aea148c76 \\\n  --jdbc... # JDBC settings omitted in this example\n</code></pre> <p>Example of running an expire with deferred deletion:</p> <pre><code>java -jar nessie-gc.jar expire --live-set-id 0baaa1ff-90db-4ee5-b6d2-b60aea148c76 \\\n  --defer-deletes \\\n  --jdbc... # JDBC settings omitted in this example\n\n# You can inspect the files to be deleted this way ...\njava -jar nessie-gc.jar list-deferred --live-set-id 0baaa1ff-90db-4ee5-b6d2-b60aea148c76 \\\n  --jdbc... # JDBC settings omitted in this example\n\n# ... or this way\njava -jar nessie-gc.jar show --live-set-id 0baaa1ff-90db-4ee5-b6d2-b60aea148c76 \\\n  --with-deferred-deletes \\\n  --jdbc... # JDBC settings omitted in this example\n\n# Now perform the file deletions\njava -jar nessie-gc.jar deferred-deletes --live-set-id 0baaa1ff-90db-4ee5-b6d2-b60aea148c76 \\\n  --jdbc... # JDBC settings omitted in this example\n</code></pre> <p>Note</p> <p>The sweep phase does not access Nessie. It does use Iceberg and accesses the data lake. If deferred deletion is requested, no files will be deleted.</p> <p>Note</p> <p>Since data lakes can easily contain a huge amount of files, the expire phase does not remember every live data file (see the Iceberg assets above) individually, but uses a probabilistic data structure (bloom filter). The default settings expect, for each content ID, 1,000,000 files and uses a false-positive-probability of 0.0001 (those defaults may change, but can be inspected with <code>java -jar nessie-gc.jar help expire</code>). The expire phase will abort, if it hits a content-ID  that massively exceeds the configured false-positive-probability, because it hits way more live file references.</p> <p>Note</p> <p>Nessie GC\u2019s expire phase processes up to 4 content-IDs in parallel. This setting can be changed using the <code>--expiry-parallelism</code> command line option.</p>"},{"location":"guides/management/#recommended-production-setup-for-nessie-gc","title":"Recommended production setup for Nessie GC","text":"<p>It is highly recommended to use one of the supported databases to persist the live-content-sets. Running the different Nessie GC phases separately is only supported with such a database.</p> <p>Make yourself familiar with all the commands offered by <code>nessie-gc.jar</code> and the available command line options. It is safe to run <code>java -jar nessie-gc.jar mark-live</code>, because it is non-destructive. Use <code>java -jar nessie-gc.jar show --with-content-references</code> to inspect the collected live content references.</p> <p>Use deferred deletion and inspect the files to be deleted before running <code>java -jar nessie-gc.jar deferred-deletes</code>.</p> <p>Use separate invocations for the mark, the sweep and the deferred deletion phases.</p>"},{"location":"guides/management/#all-in-one","title":"All-in-one","text":"<p>As briefly mentioned above, the <code>java -jar nessie-gc.jar gc</code> command can be used to combine the mark and sweep phases, optionally using the <code>--inmemory</code> option.</p> <p><code>gc</code> is equivalent to first running <code>identify</code> and then <code>expire</code>, and it takes the same set of command line options. </p>"},{"location":"guides/management/#troubleshooting","title":"Troubleshooting","text":"<p>Nessie GC tool emits the log output at <code>INFO</code> level. The default log level for the console can be overridden using the Java system property <code>log.level.console</code>, for example using the following command:</p> <pre><code>java -Dlog.level.console=DEBUG -jar nessie-gc.jar\n</code></pre>"},{"location":"guides/migration/","title":"Migration &amp; Backup/Recovery","text":"<p>The Nessie Server Admin tool can be used to migrate a Nessie repository from one version store type  to another, for example from MongoDB to Postgres. It can also be used to migrate from a legacy  version store type (before Nessie 0.75) to a new version store type. The export/import functionality can also be used for backup &amp; recovery use cases.</p> <p>Note</p> <p>The Nessie Server Admin tool is an executable jar, also available as a Docker image, that can be used to interact with a Nessie database directly. It should not be confused with the Nessie CLI tool, which is a tool that is used to interact with Nessie servers.</p> <p>In Nessie versions before 0.83.2, the Nessie Server Admin tool was called Nessie Quarkus CLI.</p>"},{"location":"guides/migration/#use-cases","title":"Use cases","text":"<p>There are two major use cases for \u201cexport\u201d/\u201dimport\u201d:</p>"},{"location":"guides/migration/#backup-recovery-use-case","title":"Backup / recovery use case","text":"<p>To export a Nessie repository for the backup/recovery use case, it is not necessary to stop the Nessie servers, if you are using a Nessie Server admin tool version since 0.94.0.</p> <p>With Nessie Server admin tool versions before 0.94.0, you must stop the Nessie servers before starting the export, otherwise the exported data will be inconsistent.</p> <p>For recovery (\u201cimport\u201d) it is mandatory that the Nessie servers are not running.</p>"},{"location":"guides/migration/#migration-use-case","title":"Migration use case","text":"<p>The migration process naturally requires downtime of the Nessie server. Nessie Servers must be stopped before starting the export to ensure that all information from the source repository is available in the target repository.</p> <p>The process for the migration use case in short:</p> <ol> <li>Stop all Nessie servers</li> <li>Export from the source repository</li> <li>Upgrade Nessie to the latest version</li> <li>Import into the target repository</li> <li>Configure Nessie servers to use the target repository</li> <li>Start all Nessie servers</li> </ol> <p>Warn</p> <p>Because support for legacy version store types was removed in Nessie 0.75, the migration process from a Nessie version &lt; 0.75 to a Nessie version &gt;= 0.75 requires a full repository migration prior to upgrading the Nessie server.</p> <p>Make sure to use the correct legacy version store types for the export and the correct version store type for the import.</p> <p>If you want to migrate from a Nessie version older than 0.75, you must use the <code>nessie-quarkus-cli</code> from that version. Download links are available on our GitHub releases page.</p>"},{"location":"guides/migration/#step-by-step-guide","title":"Step-by-step guide","text":"<p>Except for the migration use case from Nessie before version 0.75, it is safe to use the latest version of Nessie Server admin tool. You can find the downloads for a specific version on the Docs site.</p>"},{"location":"guides/migration/#stop-the-nessie-servers","title":"Stop the Nessie server(s).","text":"<p>When using a Nessie Server admin tool version before 0.94.0, you must always stop the Nessie servers before starting the export, otherwise the exported data will be inconsistent.</p> <p>If you want to export a repository for export for backup/recovery and you are using Nessie Server admin tool version since 0.94.0, the Nessie servers do not need to be stopped. </p>"},{"location":"guides/migration/#export","title":"Export","text":"<p>Export the source Nessie repository to a ZIP file. In this example we assume the MongoDB  database is called <code>nessie-source</code> and is hosted on a MongoDB instance available at  <code>nessie.example.com:27017</code>:</p> <p>See Nessie Server Admin tool reference docs for details.</p> Standalone JarDocker Image <p>Running the Nessie Server Admin tool standalone jar requires Java 17 or newer.</p> <pre><code>curl -L -o nessie-server-admin-tool-0.106.0-runner.jar \\\n  https://github.com/projectnessie/nessie/releases/download/nessie-0.106.0/nessie-server-admin-tool-0.106.0-runner.jar\n</code></pre> <pre><code>java \\\n  -Dnessie.version.store.type=MONGODB \\\n  -Dquarkus.mongodb.database=nessie-source \\\n  -Dquarkus.mongodb.connection-string=mongodb://&lt;user&gt;:&lt;password&gt;@nessie.example.com:27017 \\\n  -jar  nessie-server-admin-tool-0.106.0-runner.jar \\\n  export \\\n  --path \"/tmp/export-$(date +'%Y-%d-%m').zip\" \\\n  --commit-batch-size 1000\n</code></pre> <p>The Docker image is available for Nessie Server admin tool since version 0.83.1.</p> <p>With the below settings, the export file(s) are going to be placed in a directory named <code>export-data</code>, which is mapped as <code>/data</code> in the Docker container.</p> <pre><code>mkdir -p export-data\n\ndocker run --rm -ti \\\n  --volume ./export-data:/data \\\n  -e nessie.version.store.type=MONGODB \\\n  -e quarkus.mongodb.database=nessie-source \\\n  -e quarkus.mongodb.connection-string=mongodb://&lt;user&gt;:&lt;password&gt;@nessie.example.com:27017 \\\n  ghcr.io/projectnessie/nessie-server-admin:0.106.0 \\\n  export \\\n  --path \"/data/export-$(date +'%Y-%d-%m').zip\" \\\n  --commit-batch-size 1000\n</code></pre> <p>The <code>--commit-batch-size</code> option generally improves performance, but is not required.</p> <p>The export process will take some time, depending on the size of the Nessie repository. You should  see something like this:</p> <pre><code>Exporting from a MONGO version store...\nExporting commits...\n..........\n100 commits exported.\n\nExporting named references...\n1 named references exported.\n\nExported Nessie repository, 100 commits into 1 files, 1 named references into 1 files.\n</code></pre> <p>Once the export process has completed, the ZIP file will be available at <code>/tmp/export-&lt;date&gt;.zip</code>.</p>"},{"location":"guides/migration/#import","title":"Import","text":"<p>Download the <code>nessie-server-admin-tool-x.y.z-runner.jar</code> corresponding to the new Nessie version, here we assume version 0.106.0 (note: older versions have this tool under the name of <code>nessie-quarkus-cli</code>):</p> <p>Create/setup the database to host the new target Nessie repository. In this example we assume that a MongoDB database is called <code>nessie</code> and is hosted on the same MongoDB instance as the source Nessie repository.</p> <p>Refer to the Server Configuration reference docs for the respective Nessie version.</p> <p>Import the ZIP file into the new Nessie repository:</p> <p>See Nessie Server Admin tool reference docs for details.</p> Standalone JarDocker Image <p>Running the Nessie Server Admin tool standalone jar requires Java 17 or newer.</p> <pre><code>curl -L -o nessie-server-admin-tool-0.106.0-runner.jar \\\n  https://github.com/projectnessie/nessie/releases/download/nessie-0.106.0/nessie-server-admin-tool-0.106.0-runner.jar\n</code></pre> <pre><code>java \\\n  -Dnessie.version.store.type=MONGODB \\\n  -Dquarkus.mongodb.database=nessie \\\n  -Dquarkus.mongodb.connection-string=mongodb://nessie.example.com:27017 \\\n  -jar nessie-server-admin-tool-0.106.0-runner.jar \\\n  import \\\n  --path \"/tmp/export-$(date +'%Y-%d-%m').zip\" \\\n  --commit-batch-size 1000\n</code></pre> <p>The Docker image is available for Nessie Server admin tool since version 0.83.1.</p> <p>With the below settings, the export file(s) are expected to be in a directory named <code>export-data</code>, which is mapped as <code>/data</code> in the Docker container. See the Export section above.</p> <pre><code>docker run --rm -ti \\\n  --volume ./export-data:/data \\\n  -e nessie.version.store.type=MONGODB \\\n  -e quarkus.mongodb.database=nessie-source \\\n  -e quarkus.mongodb.connection-string=mongodb://&lt;user&gt;:&lt;password&gt;@nessie.example.com:27017 \\\n  ghcr.io/projectnessie/nessie-server-admin:0.106.0 \\\n  import \\\n  --path \"/data/export-$(date +'%Y-%d-%m').zip\" \\\n  --commit-batch-size 1000\n</code></pre> <p>The <code>--commit-batch-size</code> option generally improves performance, but is not required.</p> <p>The import process will take some time, depending on the size of the Nessie repository. You should see something like this:</p> <pre><code>Importing into a MONGODB version store...\nExport was created by Nessie version 0.71.0 on 2023-12-27T11:50:10.547Z, containing 1 named references (in 1 files) and 100 commits (in 1 files).\nPreparing repository...\nImporting 100 commits...\n..........\n100 commits imported, total duration: PT0.113285209S.\n\nImporting 1 named references...\n1 named references imported, total duration: PT0.018604083S.\n\nFinalizing import...\n..........\nImport finalization finished, total duration: PT0.217375166S.\n\nImported Nessie repository, 100 commits, 1 named references.\nTotal duration: PT0.632381167S.\n</code></pre> <p>Once the import process has completed, the new Nessie repository is ready to be used. Change the  Nessie server configuration to use the new version store type and the new MongoDB database by modifying the <code>application.properties</code> file (or equivalent environment variables) to contain:</p> <pre><code>nessie.version.store.type=MONGODB\nquarkus.mongodb.database=nessie\n</code></pre>"},{"location":"guides/migration/#start-the-nessie-servers","title":"Start the Nessie server(s).","text":"<p>Once the import completed, you can start your Nessie server(s) against the target version store (database).</p>"},{"location":"guides/minikube/","title":"Nessie on Minikube","text":"<p>If you followed our Docker guide, you can now try Nessie on Kubernetes with Minikube.</p> <p>Minikube is a tool that makes it easy to run Kubernetes locally. This document describes how to run Nessie on Minikube, using the Nessie Helm chart. Please familiarize yourself with the Nessie Helm chart before proceeding.</p>"},{"location":"guides/minikube/#setup","title":"Setup","text":"<ul> <li>Install Minikube as described in https://minikube.sigs.k8s.io/docs/start/</li> <li>Install Helm as described in https://helm.sh/docs/intro/install/</li> <li>Start Minikube cluster: <code>minikube start</code></li> <li>Create a namespace: <code>kubectl create namespace nessie-ns</code></li> </ul> <p>Then install Nessie with Helm:</p> <ul> <li>From the Helm repo: see instructions in Nessie Helm chart</li> <li>From Nessie source code root: <code>helm install nessie -n nessie-ns helm/nessie</code></li> <li>Note: this installs whatever is in your <code>helm/nessie</code> directory. Use only for development!</li> </ul> <p>The rest of this page describes how to do some common tasks with Nessie on Minikube:</p> <ul> <li>Ingress with Minikube</li> <li>OpenTelemetry Collector with Minikube</li> <li>Custom Docker images for Nessie with Minikube</li> </ul> <p>Additionally, you can check how to set up Ingress with TLS in this separate guide.</p> <p>Once you are done, you can stop or delete Minikube and / or uninstall Nessie:</p> <pre><code>helm uninstall --namespace nessie-ns nessie\nminikube stop\nminikube delete # if you want to delete the VM\n</code></pre>"},{"location":"guides/minikube/#ingress-with-minikube","title":"Ingress with Minikube","text":"<p>This is broadly following the example from https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/</p> <ul> <li>Start Minikube as described above and create the namespace <code>nessie-ns</code>.</li> <li> <p>Install Nessie Helm chart with Ingress enabled, from your local repository root:   <pre><code>helm install nessie -n nessie-ns helm/nessie \\\n  --set ingress.enabled=true \\\n  --set ingress.hosts[0].host='chart-example.local' \\\n  --set ingress.hosts[0].paths[0]='/'\n</code></pre></p> </li> <li> <p>Verify that the IP address is set:   <pre><code>kubectl get ingress -n nessie-ns\nNAME     CLASS   HOSTS   ADDRESS        PORTS   AGE\nnessie   nginx   *       192.168.49.2   80      4m35s\n</code></pre></p> </li> <li>Use the IP from the above output and add it to <code>/etc/hosts</code>, for example:   <pre><code>echo \"192.168.49.2 chart-example.local\" | sudo tee /etc/hosts`\n</code></pre></li> <li>Verify that <code>curl chart-example.local</code> works</li> </ul> <p>Note</p> <p>If you also need to enable TLS, check this separate guide.</p>"},{"location":"guides/minikube/#opentelemetry-collector-with-minikube","title":"OpenTelemetry Collector with Minikube","text":"<ul> <li>Start Minikube as described above and create the namespace <code>nessie-ns</code>.</li> <li>Install cert-manager:</li> </ul> <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.12.7/cert-manager.yaml\n</code></pre> <ul> <li>Install Jaeger Operator:</li> </ul> <pre><code>kubectl create namespace observability\nkubectl apply -f https://github.com/jaegertracing/jaeger-operator/releases/download/v1.53.0/jaeger-operator.yaml -n observability\n</code></pre> <p>If the above command fails with \u201cfailed to call webhook [\u2026] connection refused\u201d, then cert-manager was not yet ready. Wait a few seconds and try again.</p> <ul> <li>Create a Jaeger instance in Nessie\u2019s namespace:</li> </ul> <pre><code>kubectl apply -n nessie-ns -f - &lt;&lt;EOF\napiVersion: jaegertracing.io/v1\nkind: Jaeger\nmetadata:\n  name: jaeger\nEOF\n</code></pre> <p>If the above command fails with \u201cfailed to call webhook [\u2026] connection refused\u201d, then the Jaeger Operator was not yet ready. Wait a few seconds and try again.</p> <ul> <li>Install Nessie Helm chart with OpenTelemetry Collector enabled:</li> </ul> <pre><code>helm install nessie -n nessie-ns helm/nessie \\\n  --set tracing.enabled=true \\\n  --set tracing.endpoint=http://jaeger-collector:4317\n</code></pre> <ul> <li>Forward ports to Jaeger UI and Nessie UI:</li> </ul> <pre><code>kubectl port-forward -n nessie-ns service/nessie 19120:19120 &amp;\nkubectl port-forward -n nessie-ns service/jaeger-query 16686:16686 &amp;\n</code></pre> <ul> <li>Open the following URLs in your browser:</li> <li>Nessie UI (to generate some traces): http://localhost:19120</li> <li>Jaeger UI (to retrieve the traces): http://localhost:16686/search</li> </ul> <p>To kill the port forwarding processes, run:</p> <pre><code>killall -9 kubectl\n</code></pre>"},{"location":"guides/minikube/#custom-docker-images-for-nessie-with-minikube","title":"Custom Docker images for Nessie with Minikube","text":"<p>You can modify Nessie\u2019s code and deploy it to Minikube.</p> <p>Once you\u2019ve satisfied with your changes, build the project with:</p> <pre><code>./gradlew :nessie-quarkus:quarkusBuild\n</code></pre> <p>Then, build the Docker image and install it in your Minikube node as follows:</p> <pre><code>eval $(minikube docker-env)\ndocker build -f ./tools/dockerbuild/docker/Dockerfile-server -t nessie-test:latest ./servers/quarkus-server\n</code></pre> <p>By running <code>eval $(minikube docker-env)</code>, you are setting the Docker environment variables to point to the Minikube Docker daemon. This means that when you run <code>docker build</code>, the image will be built in the Minikube Docker daemon, and not in your local Docker daemon, and it will be available to Minikube immediately.</p> <p>Then deploy Nessie with the custom Docker image:</p> <pre><code>helm install nessie -n nessie-ns helm/nessie \\\n  --set image.repository=nessie-test \\ \n  --set image.tag=latest\n</code></pre>"},{"location":"guides/nessie_vs_git/","title":"Nessie vs Git","text":"<p>Git is awesome. Nessie was inspired by Git but makes very different tradeoffs. Nessie focuses  on the specific use case of data version control. By narrowing our focus, we were able to better  serve the needs of the data ops experience while continuing to support a general git  metaphor. The key difference between the two is that Nessie does not support disconnected  copies. This allows several other dimensions to be substantially more powerful. </p>"},{"location":"guides/nessie_vs_git/#key-differences","title":"Key differences","text":"Dimension Git Nessie Rationale Clones Allowed Not Allowed This is the biggest difference between Nessie and Git. Git is a distributed version control system, Nessie is not. This is appropriate in the context of Nessie\u2019s role as an RPS. When talking about Cloud Data ops, everyone does not get their own copy of data\u2013the datasets are typically large and centralized. Because Nessie is layered on top of those shared datasets, clones make less sense. In the Nessie world, using personal branches provides a similar mechanism while keeping a shared world view of what can be managed for GC policies, etc. Speed (commits/second)) &lt;1 hundreds to thousands When we started working on Nessie, we actually tried to use Git. We evaluated Git directly, implemented a version that used JGit (used by tools like Gerrit and Eclipse) as well as explored the capabilities of GitHub, Azure Git and AWS Git. What we saw was a fairly expensive operation. Typically, a single commit operation took on the order of a few seconds. Scale 100s MB Unconstrained While there are multiple examples of larger or higher performance Git implementations (1, 2) , in general Git repositories are fairly small in size. Things like Git LFS were created to help accommodate this but given the nature of clones, large repositories are frowned upon. Because Nessie provides a centralized repository, typical repository constraints do not apply. History Thousands Billions Nessie supports optional garbage collection of historical commits based on user-defined rules to reduce storage consumption. Committer Human Human &amp; Machine Git was designed for human time: commits happen 100-1000s of times a day, not 100x per second. Data in many systems is changing very frequently (especially when you consider a large number of tables). These systems are frequently driven by automated scripts Objects Files Tables &amp; Views Nessie is focused on tracking versions of tables using a well-known set of open table formats. Nessie\u2019s capabilities are integrated into the particular properties of these formats. Git is focused on the generalized problem of tracking arbitrary files (buckets of bytes)."},{"location":"guides/nessie_vs_git/#nessie-on-git","title":"Nessie on Git?","text":"<p>While we describe the reasoning and differences above, we actually support running  Nessie on top of Git. In fact, the first version of Nessie was built on top of Git.  Once implemented, we then evaluated it against one of our key design criteria. This  design criterion was to support commits in the situation where there are 100,000 tables and each table is changing every 5 minutes. (For reference, the  5 minutes comes from community guidance on commit speed per table for Iceberg. The  100,000 tables comes from various users we\u2019ve worked with before.) The math for this  comes out to ~333 commits/second.</p>"},{"location":"guides/nessie_vs_git/#333-commitssecond","title":"333 Commits/second?","text":"<p>Using the design goal above, we looked at the major Git service providers to evaluate  their performance. We saw an average commit turn-around speed of 1-5/s for most  services (GitHub, Azure Git, AWS Git, etc.). Worse case commit latency were &gt;20s for  a single commit.</p> <p>Given this initial result, things were not looking good. We took one more attempt to  try to achieve the performance requirements using Git. We built a custom storage mechanism  for the awesome JGit library. This showed better promise,  providing up to 20/commits second when run against DynamoDB. However, it was still  insufficient. As such, we ultimately built our own commit kernel to power Nessie.</p> <p>In Nessie, we do not continue to include a backing store built on top of JGit. </p>"},{"location":"guides/nessie_vs_git/#so-which-is-better","title":"So Which is Better","text":"<p>Like all engineering solutions, this isn\u2019t about what is better, only what is better for  a certain use case. Git is good at generalized version control. Nessie is good  at data version control.</p>"},{"location":"guides/nessie_vs_git/#nessie-vs-dvc","title":"Nessie vs DVC","text":"<p>DVC is a popular package within ML community that is described  as \u201cVersion Control System for Machine Learning Projects\u201d it presents. While both Nessie  and DVC are focused on data, DVC is focused on smaller datasets and maintaining the  distributed capabilities of Git. This works great for individual projects that are  typically run on single workstations where datasets can be replicated. Nessie works  at a table and metadata level specifically focused on data management problems.</p>"},{"location":"guides/reverse-proxy/","title":"Nessie behind a Reverse Proxy","text":"<p>If you run Nessie behind a Reverse Proxy like istio or nginx, both the reverse proxy and Nessie/Quarkus need to be configured accordingly.</p> <p>Warn</p> <p>Make sure to understand what the configuration options mean, before you apply those to any production setup!</p> <p>Info</p> <p>Passing the <code>X-Forwarded-Proto</code>, <code>X-Forwarded-Host</code> and <code>X-Forwarded-Port</code> headers to Nessie works for Iceberg REST.</p> <p>The follow config options are mentioned only for documentation purposes. Consult the Quarkus documentation for \u201cRunning behind a reverse proxy\u201d and configure those depending on your actual needs.</p> Application PropertiesHelm Chart <pre><code>quarkus.http.proxy.proxy-address-forwarding=true\nquarkus.http.proxy.allow-x-forwarded=true\nquarkus.http.proxy.enable-forwarded-host=true\nquarkus.http.proxy.enable-forwarded-prefix=true\n</code></pre> <pre><code>advancedConfig:\n  quarkus:\n    http:\n      proxy:\n        proxy-address-forwarding: \"true\"\n        allow-x-forwarded: \"true\"\n        enable-forwarded-host: \"true\"\n        enable-forwarded-prefix: \"true\"\n</code></pre> <p>Warn</p> <p>Do NOT enable the above options unless your reverse proxy (for example istio or nginx) is properly setup to set these headers but also filter those from incoming requests.</p> <p>Info</p> <p>The newer <code>Forwarded</code> header is the newer version of the older <code>X-Forwarded-For</code>, <code>X-Forwarded-Host</code>, <code>X-Forwarded-Proto</code> headers, but does not provide a replacement for the <code>X-Forwarded-Prefix</code> header, which is needed in some scenarios.</p>"},{"location":"guides/reverse-proxy/#using-path-prefixes-on-the-ingress-reverse-proxy","title":"Using path prefixes on the ingress / reverse proxy","text":"<p>Usually, all HTTP requests to the reverse proxy are passed down to Nessie. If only requests that start with for example <code>/nessie/</code> to be proxied to Nessie, then the reverse proxy needs to be configured to also pass the <code>X-Forwarded-Prefix</code> with the that prefix and Quarkus must be configured to respect the <code>X-Forwarded-Prefix</code> header using the <code>quarkus.http.proxy.enable-forwarded-prefix=true</code> configuration.</p>"},{"location":"guides/reverse-proxy/#verifying-the-reverse-proxy-configuration","title":"Verifying the reverse proxy configuration","text":"<p>Nessie returns URLs via the Iceberg REST config endpoint and in the returned table/view metadata. These URLs are used by Iceberg clients to issue follow-up requests.</p> <p>Assuming the setup of the below Docker Compose example, issue the following <code>curl</code> command against your ingress (reverse proxy).</p> <pre><code>curl https://nessie-nginx.localhost.localdomain:8443/nessie/iceberg/v1/config\n</code></pre> <p>It should yield the configuration. Look for the <code>nessie.iceberg-base-uri</code> property: <pre><code>{\n  \"defaults\": {\n    // ...\n  },\n  \"overrides\": {\n    // ...\n    \"nessie.iceberg-base-uri\": \"https://nessie-nginx.localhost.localdomain:8443/nessie/iceberg/\"\n    // ...\n  }\n}\n</code></pre></p> <p>Make sure that the scheme (<code>https</code> in this case), the hostname (<code>nessie-nginx.localhost.localdomain</code> in this case), the port (<code>8443</code> in this case) and the prefix (<code>/nessie/</code> in this case) match the expected values. Differences will result in Iceberg REST request failures.</p> <p>Consult the documentation of your ingress/reverse-proxy for details how to set those up to meet the requirements of your particular environment.</p> Istio / EnvoyNginx <p>Related istio/envoy documentation pages:</p> <ul> <li>Configuring X-Forwarded-For</li> <li>Header manipulation</li> </ul> <p>Related nginx documentation pages:</p> <ul> <li>Reverse Proxy</li> <li>Proxy module</li> <li>Load Balancer</li> </ul> <p>See the example configuration from the Docker Compose example below: <pre><code>events {\n  worker_connections 1024;\n}\n\nhttp {\n  # Redirect non-HTTPS to HTTPS\n  server {\n    listen 8080;\n    server_name nessie-nginx.localhost.localdomain;\n    return 301 https://$host$request_uri;\n  }\n\n  server {\n    listen 8443 ssl;\n    server_name nessie-nginx.localhost.localdomain;\n\n    ssl_certificate /etc/nginx/certs/nessie-nginx.localhost.localdomain+3.pem;\n    ssl_certificate_key /etc/nginx/certs/nessie-nginx.localhost.localdomain+3-key.pem;\n    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;\n    ssl_ciphers HIGH:!aNULL:!MD5;\n\n    # This example uses /nessie/ as the path-prefix. It is not mandatory to do this.\n    # To use no prefix and route all requests to Nessie, set '/' as the 'location' and\n    # remove the 'proxy_set_header X-Forwarded-Prefix' line.\n    location /nessie/ {\n      proxy_buffering off;\n      # The X-Forwarded-* headers needed by Quarkus\n      proxy_set_header X-Forwarded-Proto $scheme;\n      proxy_set_header X-Forwarded-Host $host;\n      proxy_set_header X-Forwarded-Port $server_port;\n      # X-Forwarded-Prefix is needed when the ingress shall use prefixes. Must set\n      # quarkus.http.proxy.enable-forwarded-prefix=true for Nessie/Quarkus in that case.\n      proxy_set_header X-Forwarded-Prefix /nessie/;\n\n      proxy_pass http://nessie:19120;\n    }\n  }\n}\n</code></pre></p>"},{"location":"guides/reverse-proxy/#docker-compose-example","title":"Docker Compose example","text":"<p>Info</p> <p>Either docker-compose or podman-compose are needed.</p> <p>We have a Docker Compose example to illustrate how to use Nessie behind a reverse proxy, using nginx in this example. You need a local clone of the Nessie source repository to run this example.</p> <p>Create a self-signed certificate first, we use the <code>mkcert</code> tool. <pre><code># Install the mkcert tool\nsudo apt install mkcert\n# Run the setup script - it will generate the necessary SSL certificate.\ndocker/catalog-nginx-https/setup.sh\n</code></pre></p> <p>Build Nessie Docker image <pre><code>tools/dockerbuild/build-push-images.sh --gradle-project :nessie-quarkus  --project-dir servers/quarkus-server --local localhost/projectnessie/nessie\n</code></pre></p> <p>Then start the Docker compose:</p> DockerPodman <pre><code>docker-compose -f docker/catalog-nginx-https/docker-compose.yml up\n</code></pre> <pre><code>podman-compose -f docker/catalog-nginx-https/docker-compose.yml up\n</code></pre> <p>Run <code>spark-sql</code> against Nessie running behind the reverse proxy: <pre><code>catalog/bin/spark-sql.sh --no-nessie-start --aws --iceberg https://nessie-nginx.localhost.localdomain:8443/nessie/iceberg/main\n</code></pre></p> <p>Within the Spark SQL shell: <pre><code>CREATE NAMESPACE nessie.sales;\nUSE nessie.sales;\nCREATE TABLE city (C_CITYKEY BIGINT, C_NAME STRING, N_NATIONKEY BIGINT, C_COMMENT STRING) USING iceberg PARTITIONED BY (bucket(16, N_NATIONKEY));\n</code></pre></p> <p>Exit the Spark SQL shell.</p> <p>Run the Nessie CLI: <pre><code>./gradlew :nessie-cli:clean :nessie-cli:jar\n\njava -jar cli/cli/build/libs/nessie-cli-*-SNAPSHOT.jar\n</code></pre></p> <p>In Nessie CLI: <pre><code>CONNECT TO https://nessie-nginx.localhost.localdomain:8443/nessie/api/v2\n</code></pre> It will print the following informational messages: <pre><code>Connecting to https://nessie-nginx.localhost.localdomain:8443/nessie/api/v2 ...\nSuccessfully connected to Iceberg REST at https://nessie-nginx.localhost.localdomain:8443/nessie/iceberg/\nSuccessfully connected to Nessie REST at https://nessie-nginx.localhost.localdomain:8443/nessie/api/v2/ - Nessie API version 2, spec version 2.1.0\n</code></pre></p> <p>More CLI commands to try out: <pre><code>LIST CONTENTS\nSHOW TABLE sales.city\n</code></pre></p>"},{"location":"guides/spark-s3/","title":"Accessing data in S3 with Apache Spark","text":"<p>In this guide we walk through the process of configuring an Apache Spark session to work  with data files stored in Amazon S3 and version history in a local Nessie Server.</p> <p>Docker is used at the runtime environments for Nessie. Spark is assumed to be installed locally.</p>"},{"location":"guides/spark-s3/#setting-up-nessie-server","title":"Setting up Nessie Server","text":"<p>Start the Nessie server container from the <code>projectnessie/nessie</code> Docker image in default mode.</p> <pre><code>docker run -p 19120:19120 ghcr.io/projectnessie/nessie:latest\n</code></pre> <p>Note: this example will run the Nessie Server using in-memory storage for table metadata. If/when the container is deleted, Nessie data about table changes will be lost, yet the data files in S3 will remain.</p>"},{"location":"guides/spark-s3/#setting-up-spark-session-for-amazon-s3","title":"Setting up Spark Session for Amazon S3","text":"<p>Configure AWS SDK credentials in any way appropriate for the default AWS SDK Credentials Provider Chain.</p> <p>In this guide we assume an AWS profile (e.g. called <code>demo</code>) in defined in <code>~/.aws/credentials</code> (or other location appropriate for your OS) and contains S3 credentials. Make this profile active for CLI tools by exporting its name in the <code>AWS_PROFILE</code> environment variable. For example:</p> <pre><code>export AWS_PROFILE=demo\n</code></pre> <p>Create an Amazon S3 bucket of your own. This guide uses the bucket name <code>spark-demo1</code>.</p> <p>Start a Spark session:</p> <pre><code>spark-sql \\\n --packages \\\norg.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,\\\nsoftware.amazon.awssdk:bundle:2.20.131,\\\nsoftware.amazon.awssdk:url-connection-client:2.20.131 \\\n --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions  \\\n --conf spark.sql.catalog.nessie=org.apache.iceberg.spark.SparkCatalog \\\n --conf spark.sql.catalog.nessie.warehouse=s3://spark-demo1 \\\n --conf spark.sql.catalog.nessie.catalog-impl=org.apache.iceberg.nessie.NessieCatalog \\\n --conf spark.sql.catalog.nessie.io-impl=org.apache.iceberg.aws.s3.S3FileIO \\\n --conf spark.sql.catalog.nessie.uri=http://localhost:19120/api/v1 \\\n --conf spark.sql.catalog.nessie.ref=main \\\n --conf spark.sql.catalog.nessie.cache-enabled=false\n</code></pre> <p>Note: <code>spark-demo1</code> is the name of the S3 bucket that will hold table data files.</p> <p>Note: the <code>--packages</code> option lists modules required for Iceberg to write data files into S3. Please refer to Iceberg documentation for the most up-to-date information on how to connect Iceberg to S3.</p> <p>Note: the word <code>nessie</code> in configuration property names is the name of the Nessie catalog in the Spark session. A different name can be chosen according the user\u2019s liking.</p>"},{"location":"guides/spark-s3/#setting-up-spark-session-for-minio","title":"Setting up Spark Session for Minio","text":"<p>Using Minio is mostly the same as using Amazon S3 except that the S3 endpoint and credentials are different.</p> <p>For this example, start a local Minio server using Docker:</p> <pre><code>docker run -p 9000:9000 -p 9001:9001 --name minio \\\n -e \"MINIO_ROOT_USER=datauser\" -e \"MINIO_ROOT_PASSWORD=minioSecret\" \\\n quay.io/minio/minio:latest server /data --console-address \":9001\"\n</code></pre> <p>Configure AWS SDK crendetials the same way you would configure them for Amazon S3 (refer to the section above) but use relevant Minio credentials. In this example the credentials for the Minio server running in Docker are: <code>aws_access_key_id = datauser</code>, <code>aws_secret_access_key = minioSecret</code>. Assuming the credentials are stored in an AWS profile named <code>demo</code>, export the profile name in the <code>AWS_PROFILE</code> environment variable. For example:</p> <pre><code>export AWS_PROFILE=demo\n</code></pre> <p>Create a Minio bucket of your own. This guide uses the bucket name <code>spark1</code>.</p> <p>Start a Spark session:</p> <pre><code>spark-sql \\\n --packages \\\norg.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,\\\nsoftware.amazon.awssdk:bundle:2.20.131,\\\nsoftware.amazon.awssdk:url-connection-client:2.20.131 \\\n --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions  \\\n --conf spark.sql.catalog.nessie=org.apache.iceberg.spark.SparkCatalog \\\n --conf spark.sql.catalog.nessie.warehouse=s3://minio/spark1 \\\n --conf spark.sql.catalog.nessie.s3.endpoint=http://localhost:9000\n --conf spark.sql.catalog.nessie.catalog-impl=org.apache.iceberg.nessie.NessieCatalog \\\n --conf spark.sql.catalog.nessie.io-impl=org.apache.iceberg.aws.s3.S3FileIO \\\n --conf spark.sql.catalog.nessie.uri=http://localhost:19120/api/v1 \\\n --conf spark.sql.catalog.nessie.ref=main \\\n --conf spark.sql.catalog.nessie.cache-enabled=false\n</code></pre> <p>Note the <code>s3.endpoint</code> catalog property. it should point to the appropriate Minio endpoint. In this example it points  to the local Minio server running in Docker.</p>"},{"location":"guides/spark-s3/#running-ddl-and-dml-in-spark-sql-shell","title":"Running DDL and DML in Spark SQL Shell","text":"<p>Once the Spark session initializes, issue a <code>use</code> statement to make <code>nessie</code> the current catalog: <pre><code>spark-sql&gt; use nessie\n</code></pre></p> <p>This command will establish a connection to the Nessie Server. When it is done, it will be possible to create tables and run DML. For example:</p> <pre><code>spark-sql&gt; CREATE TABLE demo (id bigint, data string);\nTime taken: 1.615 seconds\nspark-sql&gt; show tables;\ndemo\nTime taken: 0.425 seconds, Fetched 1 row(s)\nspark-sql&gt; INSERT INTO demo (id, data) VALUES (1, 'a');\nTime taken: 4.017 seconds\nspark-sql&gt; SELECT * FROM demo;\n1   a\nTime taken: 3.225 seconds, Fetched 1 row(s)\n</code></pre> <p>Branches, merges and other git-like commands can be run as well, as explained in the  Getting Started guide. </p> <p>Note: The above example uses the <code>spark-sql</code> shell, but the same configuration options apply to <code>spark-shell</code>.</p>"},{"location":"guides/spark-s3/#authentication","title":"Authentication","text":"<p>This example uses implicit AWS authentication via credentials configured in a credentials file plus the <code>AWS_PROFILE</code> environment variable.</p> <p>The Nessie Server in this example does not require authentication.</p> <p>If the Nessie Server runs with authentication enabled, additional configuration parameters will be required in the Spark session. Please refer to the Authentication in Tools section for details.</p>"},{"location":"guides/sql/","title":"Nessie Spark SQL Extensions","text":"<p>Spark SQL extensions provide an easy way to execute common Nessie commands via SQL.</p>"},{"location":"guides/sql/#how-to-use-them","title":"How to use them","text":""},{"location":"guides/sql/#spark","title":"Spark","text":"<p>Choose your Spark, Iceberg and Scala version below:</p> Spark 3.5Spark 3.4 Iceberg 1.10.1Iceberg 1.10.0Iceberg 1.9.2Iceberg 1.9.1Iceberg 1.9.0Iceberg 1.7.2Iceberg 1.8.1Iceberg 1.8.0Iceberg 1.7.1Iceberg 1.7.0Iceberg 1.6.1Iceberg 1.6.0Iceberg 1.5.2Iceberg 1.5.1Iceberg 1.5.0Iceberg 1.4.3Iceberg 1.4.2Iceberg 1.4.1Iceberg 1.4.0 Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.10.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.104.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.10.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.104.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.10.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.104.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.10.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.104.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.10.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.104.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.10.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.104.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.10.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.104.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.10.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.104.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.2</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.103.3</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.103.3\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.9.2</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.103.3</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.9.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.103.3\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.103.3</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.103.3\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.9.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.103.3</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.9.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.103.3\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.103.3</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.103.3\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.9.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.103.3</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.9.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.103.3\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.2</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.102.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.102.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.7.2</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.102.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.7.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.102.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.8.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.102.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.8.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.102.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.8.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.102.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.8.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.102.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.8.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.102.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.8.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.102.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.8.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.102.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.8.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.102.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.99.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.99.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.7.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.99.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.7.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.99.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.99.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.99.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.7.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.99.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.7.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.99.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.92.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.92.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.6.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.92.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.6.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.92.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.92.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.92.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.6.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.92.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.6.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.92.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.5.2</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.77.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.5.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.77.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.5.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.77.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.5.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.77.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.5.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.77.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.5.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.77.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.71.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.71.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.4.3</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.71.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.4.3,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.71.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.71.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.71.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.4.2</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.71.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.4.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.71.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.71.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.71.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.4.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.71.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.4.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.71.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.71.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.71.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.5.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.4.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.71.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.4.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.13:0.71.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Iceberg 1.10.1Iceberg 1.10.0Iceberg 1.9.2Iceberg 1.9.1Iceberg 1.9.0Iceberg 1.7.2Iceberg 1.8.1Iceberg 1.8.0Iceberg 1.7.1Iceberg 1.7.0Iceberg 1.6.1Iceberg 1.6.0Iceberg 1.5.2Iceberg 1.5.1Iceberg 1.5.0Iceberg 1.4.3Iceberg 1.4.2Iceberg 1.4.1Iceberg 1.4.0 Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.10.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.104.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.10.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.104.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.10.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.104.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.10.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.104.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.10.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.104.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.10.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.104.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.10.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.104.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.10.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.104.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.9.2</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.103.3</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.9.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.103.3\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.9.2</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.103.3</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.9.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.103.3\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.9.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.103.3</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.9.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.103.3\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.9.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.103.3</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.9.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.103.3\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.9.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.103.3</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.9.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.103.3\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.9.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.103.3</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.9.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.103.3\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.7.2</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.102.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.7.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.102.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.7.2</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.102.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.7.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.102.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.8.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.102.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.8.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.102.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.8.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.102.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.8.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.102.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.8.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.102.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.8.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.102.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.8.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.102.5</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.8.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.102.5\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.7.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.99.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.7.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.99.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.7.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.99.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.7.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.99.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.7.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.99.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.7.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.99.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.7.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.99.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.7.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.99.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.6.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.92.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.6.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.92.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.6.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.92.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.6.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.92.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.6.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.92.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.6.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.92.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.6.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.92.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.6.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.92.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.5.2</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.77.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.5.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.77.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.5.2</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.77.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.5.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.77.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.5.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.77.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.5.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.77.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.5.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.77.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.5.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.77.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.5.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.77.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.5.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.77.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.5.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.77.1</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.5.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.77.1\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.3</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.71.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.3,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.71.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.4.3</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.71.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.4.3,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.71.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.2</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.71.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.71.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.4.2</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.71.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.4.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.71.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.71.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.71.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.4.1</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.71.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.4.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.71.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> Scala 2.12Scala 2.13 <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.71.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.71.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>In order to be able to use Nessie\u2019s custom Spark SQL extensions with Spark 3.4.x, one needs to configure <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.4.0</code> along with <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.71.0</code></p> <p>Here\u2019s an example of how this is done when starting the <code>spark-sql</code> shell:</p> <pre><code>bin/spark-sql \n  --packages \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.13:1.4.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.13:0.71.0\"\n  --conf spark.sql.extensions=\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n  --conf &lt;other settings&gt;\n</code></pre> <p>Additional configuration details can be found in the Spark via Iceberg docs.</p> <p>Warn</p> <p>Use the version of the Nessie Spark SQL extensions that matches the Nessie version included in the Iceberg version you want to use! See Nessie Spark SQL Extensions page for details.</p>"},{"location":"guides/sql/#grammar-since-nessie-0950","title":"Grammar since Nessie 0.95.0","text":"<p>See reference docs for the Spark SQL syntax for Nessie Spark Extensions v0.95.0 or newer.</p>"},{"location":"guides/sql/#grammar-before-nessie-0950","title":"Grammar before Nessie 0.95.0","text":"<p>The grammar is shown below:</p> <pre><code>: CREATE (BRANCH|TAG) (IF NOT EXISTS)? reference=identifier (IN catalog=identifier)? (FROM fromRef=identifier)?\n| DROP (BRANCH|TAG) (IF EXISTS)? identifier (IN catalog=identifier)?\n| USE REFERENCE reference=identifier (AT tsOrHash=identifier)?  (IN catalog=identifier)?\n| LIST REFERENCES (IN catalog=identifier)?\n| SHOW REFERENCE (IN catalog=identifier)?\n| MERGE BRANCH (reference=identifier)? (INTO toRef=identifier)?  (IN catalog=identifier)?\n| SHOW LOG (reference=identifier)? (AT tsOrHash=identifier)? (IN catalog=identifier)?\n| ASSIGN (BRANCH|TAG) (reference=identifier)? (TO toRef=identifier)? (IN catalog=identifier)?\n</code></pre>"},{"location":"guides/sql/#creating-branchestags","title":"Creating Branches/Tags","text":"<p>Creating a branch <code>dev</code> in the <code>nessie</code> catalog (in case it doesn\u2019t already exist):</p> <ul> <li><code>CREATE BRANCH IF NOT EXISTS dev IN nessie</code></li> </ul> <p>Creating a tag <code>devTag</code> in the <code>nessie</code> catalog (in case it doesn\u2019t already exist):</p> <ul> <li><code>CREATE TAG IF NOT EXISTS devTag IN nessie</code></li> </ul> <p>Creating a branch <code>dev</code> in the <code>nessie</code> catalog off of an existing branch/tag <code>base</code>:</p> <ul> <li><code>CREATE BRANCH IF NOT EXISTS dev IN nessie FROM base</code></li> </ul> <p>Note that in case <code>base</code> doesn\u2019t exist, Nessie will fall back to the default branch (<code>main</code>).</p>"},{"location":"guides/sql/#dropping-branchestags","title":"Dropping Branches/Tags","text":"<p>Dropping a branch <code>dev</code> in the <code>nessie</code> catalog (in case it exists):</p> <ul> <li><code>DROP BRANCH IF EXISTS dev IN nessie</code></li> </ul> <p>Dropping a tag <code>devTag</code> in the <code>nessie</code> catalog (in case it exists):</p> <ul> <li><code>DROP TAG IF EXISTS devTag IN nessie</code></li> </ul> <p>Note: the <code>IF EXISTS</code> clause is optional and is only supported for Nessie SQL Extensions 0.72 or  higher.</p>"},{"location":"guides/sql/#switching-to-a-branchtag","title":"Switching to a Branch/Tag","text":"<p>In order to switch to the HEAD of the branch/tag <code>ref</code> in the <code>nessie</code> catalog:</p> <ul> <li><code>USE REFERENCE ref IN nessie</code></li> </ul> <p>It is also possible to switch to a specific timestamp on a given branch/tag:</p> <ul> <li><code>USE REFERENCE ref AT `2021-10-06T08:50:37.157602` IN nessie</code></li> </ul> <p>Additionally, one can switch to a specific hash on a given branch/tag:</p> <ul> <li><code>USE REFERENCE ref AT dd8d46a3dd5478ce69749a5455dba29d74f6d1171188f4c21d0e15ff4a0a9a9b IN nessie</code></li> </ul>"},{"location":"guides/sql/#listing-available-branchestags","title":"Listing available Branches/Tags","text":"<p>One can list available branches/tags in the <code>nessie</code> catalog via:</p> <ul> <li><code>LIST REFERENCES IN nessie</code></li> </ul>"},{"location":"guides/sql/#showing-details-of-the-current-branchtag","title":"Showing details of the current Branch/Tag","text":"<p>One can see details about the current branch/tag in the <code>nessie</code> catalog via:</p> <ul> <li><code>SHOW REFERENCE IN nessie</code></li> </ul>"},{"location":"guides/sql/#showing-the-commit-log-of-the-current-branchtag","title":"Showing the Commit Log of the current Branch/Tag","text":"<p>It is possible to look at the commit log of a particular branch/tag in the <code>nessie</code> catalog via:</p> <ul> <li><code>SHOW LOG dev IN nessie</code></li> </ul> <p>Additionally, one can show log of a specific hash on a given branch/tag:</p> <ul> <li><code>SHOW LOG dev AT dd8d46a3dd5478ce69749a5455dba29d74f6d1171188f4c21d0e15ff4a0a9a9b IN nessie</code></li> </ul>"},{"location":"guides/sql/#assigning-branchestags","title":"Assigning Branches/Tags","text":"<p>Assigning a branch <code>dev</code> to <code>base</code> in catalog <code>nessie</code> can be done via:</p> <ul> <li><code>ASSIGN BRANCH dev TO base IN nessie</code></li> </ul> <p>Assigning a tag <code>devTag</code> to <code>base</code> in catalog <code>nessie</code> can be done via:</p> <ul> <li><code>ASSIGN TAG devTag TO base IN nessie</code></li> </ul> <p>Note that in case <code>base</code> doesn\u2019t exist, Nessie will fall back to the default branch (<code>main</code>).</p> <p>It is also possible to assign a branch/tag to a <code>base</code> at a particular <code>hash</code>:</p> <ul> <li><code>ASSIGN TAG devTag TO base AT dd8d46a3dd5478ce69749a5455dba29d74f6d1171188f4c21d0e15ff4a0a9a9b IN nessie</code></li> </ul>"},{"location":"guides/sql/#merging-a-branch-into-another-branch","title":"Merging a Branch into another Branch","text":"<p>Merging branch <code>dev</code> into <code>base</code> for the <code>nessie</code> catalog can be done via:</p> <ul> <li><code>MERGE BRANCH dev INTO base IN nessie</code></li> </ul> <p>Note that in case <code>base</code> doesn\u2019t exist, Nessie will fall back to the default branch (<code>main</code>).</p>"},{"location":"guides/tables-views/","title":"Overview","text":"<p>Nessie is designed to work with table formats that support a write-once, immutable asset  and metadata model. These types of formats rely on a transaction arbitrator to decide  the order of operations within a table. Nessie developers have named this operation a \u201croot  pointer store\u201d (or RPS). This is because these formats all have the same need of  determining what is the \u201clatest\u201d version of data. This decision needs to be maintained  via a check-and-set operation about what the current state of a table is.</p>"},{"location":"guides/tables-views/#root-pointer-store","title":"Root Pointer Store","text":"<p>Each table format provides at least one RPS facility. Existing RPS models include:</p> <ul> <li>RPS by convention: E.g. \u201conly one writer is allowed\u201d</li> <li>RPS by consistent fileSystem: E.g. one file can be created with a certain name</li> <li>RPS by external locking: E.g. calling Hive Metastore lock apis</li> </ul> <p>Nessie formalizes and extends the concept of an RPS. It adds two main types  of operations: coordination of multiple per-table root pointers and historical versioning  across changes. This allows users to combine the rich capabilities of existing table  formats with the Nessie capabilities around versioning and transactions.</p>"},{"location":"guides/tables-views/#table-formats","title":"Table Formats","text":"<p>Nessie currently works with the Apache Iceberg table format.</p> <p>We expect that Nessie will continue to add table formats as more are created.</p>"},{"location":"guides/tables-views/#iceberg-views","title":"Iceberg Views","text":"<p>In addition to table formats, Nessie also supports storing Iceberg views within the Nessie  repository. This allows tools working in tandem with Nessie to provide very powerful versioned,  semantic-layering system. See more in our documentation on Iceberg Views.</p>"},{"location":"guides/tables-views/#other-object-types","title":"Other Object Types","text":"<p>There has been discussion about adding additional types of objects to Nessie for the  purpose of creating a consistent repository between input assets (jobs, models, etc.)  and output assets. This is something that will be evaluated based on demand. There are  currently three options being considered: </p> <ul> <li>more structured object types (such as Spark job)</li> <li>blob types</li> <li>support for git sub-modules (where Nessie offers a new object type that refers to a particular commit within a git repository)</li> </ul> <p>If you have more thoughts on this, please provide feedback on the mailing list.</p>"},{"location":"guides/tls/","title":"TLS Ingress with Minikube","text":"<p>In this guide we walk through the process of configuring a Nessie Service with secure HTTPS transport in Minikube.</p> <p>Please familiarize yourself with the Nessie Helm chart and with our general-purpose Minikube guide before proceeding.</p>"},{"location":"guides/tls/#setting-up-minikube","title":"Setting up Minikube","text":"<ol> <li>Start Minikube cluster: <code>minikube start</code></li> <li>Enable NGINX Ingress controller: <code>minikube addons enable ingress</code></li> </ol>"},{"location":"guides/tls/#creating-certificates","title":"Creating Certificates","text":"<p>If you have your own trusted certificate you can use it to sign a new certificate for the Nessie service.</p> <p>Otherwise, the rest of this section shows how can create a self-signed certificate and add it the list of trusted certificates in the client OS.</p> <p>To generate a fresh pair of a key and a (self-signed) certificate:</p> <pre><code>openssl req -new -subj \"/CN=Nessie\" -addext \"subjectAltName = DNS:nessie.local\" \\\n  -newkey rsa:2048 -keyout nessie-key.pem -out nessie.crt -x509 -nodes\n</code></pre> <p>Note the <code>-nodes</code> option. It is used only for the sake of simplicity of this example deployment. Also, newer <code>openssl</code> versions deprecated it in favour of <code>-noenc</code>.</p> <p>Add the new certificate to the local set of CA certificates on the client host (where the Nessie clients, such as Nessie CLI / <code>curl</code> / browser, are going to run).</p> <p>The following example is for Linux:</p> <ol> <li><code>sudo cp nessie.crt /usr/local/share/ca-certificates/nessie.crt</code></li> <li><code>sudo update-ca-certificates</code></li> </ol> <p>This should output something like \u201cCertificate added: CN=Nessie\u201d.</p> <p>For macOS, use the following command:</p> <pre><code>sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain nessie.crt\n</code></pre> <p>You will need to input your password both on the command line and in a pop-up window in order to add the certificate to the system keychain. To check that the certificate was added successfully, run the following command:</p> <pre><code>security find-certificate -c Nessie\n</code></pre> <p>The output should start with something like below:</p> <pre><code>keychain: \"/Library/Keychains/System.keychain\"\nversion: 256\nclass: 0x80001000 \nattributes:\n    \"alis\"&lt;blob&gt;=\"Nessie\"\n</code></pre> <p>Note: to remove the certificate from the system keychain when you are done with this guide, run the following command:</p> <pre><code>sudo security delete-certificate -c Nessie -t /Library/Keychains/System.keychain\n</code></pre> <p>Note: Some tools allow to input certificates on the command line (e.g. <code>curl --cacert nessie.crt https://$(minikube ip)/api/v2/config\" ...</code>) If you intend to use only those tools, then you don\u2019t need to add the certificate to the system keychain. However, browsers and Nessie clients both rely on the OS to trust the certificate, so to be able to use Nessie UI or CLI, the simplest approach is to add the certificate to the OS keychain as explained above.</p>"},{"location":"guides/tls/#creating-a-k8s-secret-for-the-nessie-certificate","title":"Creating a k8s Secret for the Nessie Certificate","text":"<p>Make sure a dedicated k8s namespace exists. This example uses the <code>nessie-ns</code> name. If it does not exist run the following command to create it:</p> <pre><code>kubectl create namespace nessie-ns\n</code></pre> <p>Create a TLS secret for Nessie:</p> <pre><code>kubectl -n nessie-ns create secret tls nessie-tls \\\n  --cert=nessie.crt --key=nessie-key.pem\n</code></pre>"},{"location":"guides/tls/#deploying-nessie-with-helm","title":"Deploying Nessie with Helm","text":"<p>Add the Nessie repository to Helm:</p> <pre><code>helm repo add nessie-helm https://charts.projectnessie.org\n</code></pre> <p>Install the Nessie helm chart:</p> <pre><code>helm install nessie -n nessie-ns nessie-helm/nessie \\\n  --set 'ingress.enabled=true' \\\n  --set 'ingress.hosts[0].host=nessie.local' \\\n  --set 'ingress.hosts[0].paths[0]=/' \\\n  --set 'ingress.tls[0].secretName=nessie-tls' \\\n  --set 'ingress.tls[0].hosts[0]=nessie.local'\n</code></pre> <p>The deployment process may take some time. Use the following command to check its status and get the ingress IP address.</p> <pre><code>kubectl get ingress -n nessie-ns\n</code></pre> <p>Wait until you see something like below (note the <code>ADDRESS</code> column with a non-empty value):</p> <pre><code>NAME     CLASS   HOSTS          ADDRESS        PORTS     AGE\nnessie   nginx   nessie.local   192.168.49.2   80, 443   33s\n</code></pre> <p>Note: in minikube, the ingress IP address is the same as the minikube IP address. You can get it by running <code>minikube ip</code> as well.</p>"},{"location":"guides/tls/#activating-minikubes-ssh-tunnel","title":"Activating minikube\u2019s SSH tunnel","text":"<p>For macOS users only, an extra step is required. Run the following command after installing the Nessie helm chart:</p> <pre><code>minikube tunnel\n</code></pre> <p>This will create an SSH tunnel to services deployed with type LoadBalancer and set their Ingress to their ClusterIP. This is required because Docker on macOS does not expose ingress ports on the host machine. </p> <p>You should see something like below:</p> <pre><code>\u2705  Tunnel successfully started\n\ud83d\udccc  NOTE: Please do not close this terminal as this process must stay alive for the tunnel to be accessible ...\n\u2757  The service/ingress nessie requires privileged ports to be exposed: [80 443]\n\ud83d\udd11  sudo permission will be asked for it.\n\ud83c\udfc3  Starting tunnel for service nessie.\nPassword:\n</code></pre> <p>Input your password, and do not close the terminal until you are done with the guide.</p>"},{"location":"guides/tls/#modifying-etchosts","title":"Modifying /etc/hosts","text":"<p>Add an entry in the local hosts file (e.g. <code>/etc/hosts</code>) mapping the ingress IP address to <code>nessie.local</code>, for example:</p> <pre><code>192.168.49.2    nessie.local\n</code></pre> <p>For macOS users, or users having executed the <code>minikube tunnel</code> step: you should use 127.0.0.1 instead:</p> <pre><code>127.0.0.1  nessie.local\n</code></pre> <p>Note: Some tools allow substituting host names with specific IP addresses on the command line (e.g. <code>curl --resolve \"nessie.local:443:$(minikube ip)\" ...</code>) If you intend to use only those tools, then modifying <code>/etc/hosts</code> is not necessary. However, Nessie python CLI and java client rely on the OS to be able to resolve <code>nessie.local</code>, so to be able to use them the simplest approach is to define <code>nessie.local</code> in <code>/etc/hosts</code>.</p>"},{"location":"guides/tls/#accessing-nessie-rest-api-over-https","title":"Accessing Nessie REST API over HTTPS","text":"<p>It\u2019s finally time to test Nessie\u2019s REST API. Use <code>curl</code> to verify that the server is accessible:</p> <pre><code>curl https://nessie.local/api/v2/config\n</code></pre> <p>The output should be similar to below:</p> <pre><code>{\n  \"defaultBranch\" : \"main\",\n  \"minSupportedApiVersion\" : 1,\n  \"maxSupportedApiVersion\" : 2,\n  \"actualApiVersion\" : 2,\n  \"specVersion\" : \"2.0.0\",\n  \"noAncestorHash\" : \"2e1cfa82b035c26cbbbdae632cea070514eb8b773f616aaeaf668e2f0be8f10d\",\n  \"repositoryCreationTimestamp\" : \"2023-06-02T16:46:01.307506877Z\",\n  \"oldestPossibleCommitTimestamp\" : \"2023-06-02T16:46:01.307506877Z\"\n}\n</code></pre>"},{"location":"guides/tls/#accessing-nessie-ui-over-https","title":"Accessing Nessie UI over HTTPS","text":"<p>Point your browser to https://nessie.local.</p> <p>Please note that most browsers do not trust self-signed certificates, so an exception will have to be manually granted to the \u201cNessie\u201d certificate.</p> <p>Once the browser is told to trust the certificate, you should see the UI landing page under  https://nessie.local/tree/main.</p>"},{"location":"guides/transactions/","title":"Transactions","text":"<p>Nessie extends existing table formats to provide a single serial view of transaction  history. This is enabled across an unlimited number of tables. A user can view a commit log either  through the UI or by using the Nessie CLI. Operations against each table are listed along  with timestamp, user and helpful information about the operation. </p>"},{"location":"guides/transactions/#cross-table-transactions","title":"Cross-Table Transactions","text":"<p>Nessie is the first technology to provide an open facility for cross-table transactions  within your data lake. There are two ways that this can be accomplished:</p> <ul> <li>Via Branches: Because Nessie allows branches to be created and then reassigned, a    sequence of commits on one branch can be exposed as a single consistent view to other    users through the use of a merge operation. This allows use of systems that don\u2019t have internal    cross-table transaction support to still control the visibility of connected changes. </li> <li>Single Commits: Nessie allows a single commit operation to include many object changes    simultaneously. While Nessie operations internally use this capability, tools will need to be enhanced to take advantage of this powerful    new capability.</li> </ul>"},{"location":"guides/transactions/#start-transaction-commit","title":"START TRANSACTION.. COMMIT","text":"<p>The Nessie community is working with tool developers to introduce traditional data  warehouse cross-table transactions. Nessie\u2019s catalog implementations already support  the underlying capability of multi-table transactions.</p>"},{"location":"guides/transactions/#isolation-levels","title":"Isolation Levels","text":"<p>Nessie exposes APIs to support three-levels of isolation: Read Committed,  Repeated Read and Serialized. By supporting the recording of reads as part of a commit (via the Unchanged  operation), tools can introduce full per operation serialized isolation. This is a  transaction mode that has been traditionally limited to OLTP systems and unavailable  to OLAP systems and data warehouses. </p> <p>Info</p> <p>While Nessie exposes the necessary primitives to support configurable isolation, work still needs to be  done with tool developers to ensure those tools expose these capabilities. As those  integrations progress, we\u2019ll include more information about them here. </p> <p>At the moment, most tools operate  in either Read Committed (Iceberg when calling refresh) or Repeated Read (Iceberg when avoiding calls to refresh).</p>"},{"location":"guides/transactions/#read-committed-optimistic","title":"Read Committed (Optimistic)","text":"Read Each time metadata for a table is retrieved, the latest version of that ref for the that current branch is exposed. Ownership A transaction only needs to be created on the client. There is not concept of a long-lived transaction. Write Safe writes are allowed. How Client goes to server to retrieve latest version of data for each operation"},{"location":"guides/transactions/#repeated-read-optimistic","title":"Repeated Read (Optimistic)","text":"Read When a transaction is started, a ref is turned into a specific commit id. All metadata retrieved is locked to this hash or later, as long as future hashes have not changed any table already read. Ownership A transaction only needs to be created on the client. There is no concept of a long-lived transaction on the server. Write Safe writes are allowed. Unsafe writes fail. How Client resolves commit hash on first read and uses that for all subsequent operations. <p>Note: this is stricter than the formal definition of repeatable read since that will allow new records to also be viewed on a second operation within the same transaction. However, both implementations are of similar complexity and a stricter form of repeated read seems easier to understand.</p>"},{"location":"guides/transactions/#serializable-optimistic","title":"Serializable (Optimistic)","text":"Read When a transaction is started, a ref is turned into a specific commit id. During the transaction, a recording of all read tables is recorded. Ownership A transaction only needs to be created on the client. There is no concept of a long-lived transaction on the server. Write All tables touched as part of the read operations must be in the same state when the commit operation is attempted. If they are not, the write operation is rejected. This is done internally via the Unchanged operation. How Client resolves commit hash on first read and uses that for all subsequent operations. <p>Serializable transactions allow one to do guaranteed exactly once operations. For example-  move all this data from table1 to table2. At the end of this operation there is a guarantee  that any operations done against table1 will either show up in table2 or fail to apply  to table1 (true before &amp; after).</p>"},{"location":"guides/transactions/#pessimistic-locking","title":"Pessimistic Locking","text":"<p>Currently, all three isolation models are supported only via optimistic  locking. In the future, it is likely that we will also add support for pessimistic  locking. To support pessimistic locking, transaction state must be held by the Nessie  service as opposed to Nessie clients requiring a more formal protocol around start  transaction, commit transaction with relevant timeouts, deadlock detection and  cancellation capabilities.</p>"},{"location":"guides/transactions/#lock-coarseness-and-resolution","title":"Lock Coarseness and Resolution","text":"<p>Nessie maintains state and locks at table granularity. If a conflict is found at the  table level, Nessie will either reject the operation or delegate the operation to the  underlying table format to see if further conflict resolution can occur.</p>"},{"location":"guides/try-nessie/","title":"Getting Started with Nessie and Iceberg REST","text":"<p>Nessie supports Apache Iceberg REST! Some of the features are</p> <ul> <li>All clients that \u201cspeak\u201d Iceberg REST can work with Nessie, whether it\u2019s running under Java, Scala, Python or Rust.</li> <li>Nessie supports S3 and compatible object stores like MinIO, and supports both request signing and   session tokens.</li> <li>(Currently experimental) support for Google Cloud Storage and ADLS Gen2.</li> <li>Plus all the existing Nessie features.</li> </ul> <p>To give Nessie with Iceberg REST a try, we prepared a Docker/Podman compose file for you. Running Nessie on your laptop and accessing it using Spark SQL and Iceberg REST is not difficult. The following starts Nessie with Minio and a predefined bucket.</p> <pre><code>git clone projectnessie/nessie\ncd nessie/docker\ndocker-compose -f catalog-auth-s3/docker-compose.yml up\n# or use podman-compose, if you're using Podman\n</code></pre> <p>Once all the containers are running, you can run Spark-SQL:</p> <pre><code>spark-sql  \\\n  --packages \"org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.106.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.10.1,org.apache.iceberg:iceberg-aws-bundle:1.10.1\" \\\n  --conf spark.sql.extensions=org.projectnessie.spark.extensions.NessieSparkSessionExtensions,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\\n  --conf spark.sql.catalogImplementation=in-memory \\\n  --conf \"spark.sql.catalog.nessie.scope=catalog sign\" \\\n  --conf spark.sql.catalog.nessie.oauth2-server-uri=http://127.0.0.1:8080/realms/iceberg/protocol/openid-connect/token \\\n  --conf spark.sql.catalog.nessie.credential=client1:s3cr3t \\\n  --conf spark.sql.catalog.nessie.uri=http://127.0.0.1:19120/iceberg/main/ \\\n  --conf spark.sql.catalog.nessie.type=rest \\\n  --conf spark.sql.catalog.nessie=org.apache.iceberg.spark.SparkCatalog\n</code></pre> <p>Note</p> <p>With Nessie, all necessary configuration about the object store (in the above example it is S3 via MinIO) is pushed from Nessie to the Iceberg client used by Spark. Requests from Spark/Iceberg to the object store are \u201csecured\u201d via Nessie, in the above example using S3 request signing. This means, that you do not need to configure object store credentials on your Iceberg REST clients.</p> <p>And that\u2019s basically all to connect to Nessie using Iceberg REST and try it out locally - from there it just works.</p> <p>The above docker/podman-compose configuration does a bunch of things for the convenience of a demo/try-out. A production-like setup would need a couple configuration settings. In other words, Nessie needs to know a few things, before it can serve Iceberg REST:</p> <ul> <li>The name and (object store) location of your warehouse, see configuration reference</li> <li>Configuration of the object store (S3/GCS/ADLS, cloud region and credentials), see configuration reference</li> </ul>"},{"location":"guides/ui/","title":"Web UI","text":"<p>Nessie comes with a simple web UI that allows you to understand what is in your Nessie repository. You can view existing tags and branches as well as the content within them.</p> <p>The UI automatically runs as part of starting the Nessie service. If running locally,  you can find the UI at localhost:19120.</p> <p></p>"},{"location":"iceberg/flink/","title":"Flink via Iceberg","text":"<p>Note</p> <p>Detailed steps on how to set up Pyspark + Iceberg + Flink + Nessie with Python is available on Binder</p> <p>In order to use Flink with Python API, you will need to make sure <code>pyflink</code> have access to all Hadoop JARs as mentioned in these docs. After that, you will need to make sure <code>iceberg-flink-runtime</code> is added to Flink. This can be done by adding the iceberg JAR to <code>pyflink</code> via <code>env.add_jar</code>, e.g: <code>env.add_jars(\"file://path/to/jar/iceberg-flink-runtime-1.10.1.jar\")</code>. This can be shown below:</p> <pre><code>import os\n\nfrom pyflink.datastream import StreamExecutionEnvironment\n\nenv = StreamExecutionEnvironment.get_execution_environment()\niceberg_flink_runtime_jar = os.path.join(os.getcwd(), \"iceberg-flink-runtime-1.10.1.jar\")\n\nenv.add_jars(\"file://{}\".format(iceberg_flink_runtime_jar))\n</code></pre> <p>Once we have added <code>iceberg-flink-runtime</code> JAR to <code>pyflink</code>, we can then create <code>StreamTableEnvironment</code> and execute Flink SQL statements. This can be shown in the following example:</p> <pre><code>from pyflink.table import StreamTableEnvironment\n\ntable_env = StreamTableEnvironment.create(env)\n\ntable_env.execute_sql(\n        \"\"\"CREATE CATALOG &lt;catalog_name&gt; WITH (\n        'type'='iceberg',\n        'catalog-impl'='org.apache.iceberg.nessie.NessieCatalog',\n        'uri'='http://localhost:19120/api/v1',\n        'ref'='main',\n        'warehouse' = '/path/to/flink/warehouse')\"\"\"\n    )\n</code></pre> <p>With the above statement, we have created a Nessie catalog (via Iceberg) that Flink will use to manage the tables.</p> <p>For more general information about Flink and Iceberg, refer to Iceberg and Flink documentation.</p>"},{"location":"iceberg/flink/#configuration","title":"Configuration","text":"<p>To use Nessie Catalog in Flink via Iceberg, we will need to create a catalog in Flink through <code>CREATE CATALOG</code> SQL statement (replace <code>&lt;catalog_name&gt;</code> with the name of your catalog), example:</p> <pre><code>table_env.execute_sql(\n        \"\"\"CREATE CATALOG &lt;catalog_name&gt; WITH (\n        'type'='iceberg',\n        'catalog-impl'='org.apache.iceberg.nessie.NessieCatalog',\n        'uri'='http://localhost:19120/api/v1',\n        'ref'='main',\n        'warehouse' = '/path/to/flink/warehouse')\"\"\"\n    )\n</code></pre> <p>The following properties are required in Flink when creating the Nessie Catalog:</p> <ul> <li><code>type</code>: This must be <code>iceberg</code> for iceberg table format.</li> <li><code>catalog-impl</code>: This must be <code>org.apache.iceberg.nessie.NessieCatalog</code> in order to tell Flink to use Nessie catalog implementation.</li> <li><code>uri</code>: The location of the Nessie server.</li> <li><code>ref</code>: The Nessie ref/branch we want to use.</li> <li><code>warehouse</code>: The location where to store Iceberg tables managed by Nessie catalog.</li> <li><code>authentication.type</code>: The authentication type to be used, please refer to the authentication docs for more info.</li> </ul>"},{"location":"iceberg/flink/#create-tables","title":"Create tables","text":"<p>To create tables in Flink that are managed by Nessie/Iceberg, you will need to specify the catalog name in addition to the database whenever you issue <code>CREATE TABLE</code> statement, e.g:</p> <pre><code>CREATE DATABASE `&lt;catalog_name&gt;`.`&lt;database_name&gt;`;\n\nCREATE TABLE `&lt;catalog_name&gt;`.`&lt;database_name&gt;`.`&lt;table_name&gt;` (\n    id BIGINT COMMENT 'unique id',\n    data STRING\n);\n</code></pre>"},{"location":"iceberg/flink/#reading-tables","title":"Reading tables","text":"<p>To read tables in Flink, this can be done with a typical SQL <code>SELECT</code> statement, however as the same with creating tables, you will need to make sure to specify the catalog name in addition to the database. e.g:</p> <pre><code>SELECT * FROM `&lt;catalog_name&gt;`.`&lt;database_name&gt;`.`&lt;table_name&gt;`;\n</code></pre> <p>As well, similar to Spark, you can read tables from specific branches or hashes from within a <code>SELECT</code> statement. The general pattern is <code>&lt;table_name&gt;@&lt;branch&gt;</code> or <code>&lt;table&gt;#&lt;hash&gt;</code> or <code>&lt;table&gt;@&lt;branch&gt;#&lt;hash&gt;</code> (e.g: <code>salaries@main</code>):</p> <pre><code>SELECT * FROM `&lt;catalog_name&gt;`.`&lt;database_name&gt;`.`&lt;table_name&gt;@&lt;branch&gt;`;\nSELECT * FROM `&lt;catalog_name&gt;`.`&lt;database_name&gt;`.`&lt;table_name&gt;#&lt;hash&gt;`;\nSELECT * FROM `&lt;catalog_name&gt;`.`&lt;database_name&gt;`.`&lt;table_name&gt;@&lt;branch&gt;#&lt;hash&gt;`;\n</code></pre>"},{"location":"iceberg/flink/#other-ddl-statements","title":"Other DDL statements","text":"<p>To read and write into tables that are managed by Iceberg and Nessie, typical Flink SQL queries can be used. Refer to this documentation here for more information.</p>"},{"location":"iceberg/hive/","title":"Hive via Iceberg","text":"<p>Note</p> <p>Detailed steps on how to set up Pyspark + Iceberg + Hive + Nessie with Python is available on Binder</p> <p>To access Hive via Iceberg, you will need to make sure <code>iceberg-hive-runtime</code> is added to Hive. This can be done either by adding the JAR file to <code>auxlib</code> folder in Hive home directory, by adding the JAR file to <code>hive-site.xml</code> file or via Hive shell, e.g: <code>add jar /path/to/iceberg-hive-runtime.jar;</code>. Nessie\u2019s Iceberg module is already included with <code>iceberg-hive-runtime</code> JAR distribution.</p> <p>For more general information about Hive and Iceberg, refer to Iceberg and Hive documentation.</p>"},{"location":"iceberg/hive/#configuration","title":"Configuration","text":"<p>To configure a Nessie Catalog in Hive, first it needs to be registered in Hive, this can be done by configuring the following properties in Hive (Replace <code>&lt;catalog_name&gt;</code> with the name of your catalog):</p> <pre><code>SET iceberg.catalog.&lt;catalog_name&gt;.catalog-impl=org.apache.iceberg.nessie.NessieCatalog\nSET iceberg.catalog.&lt;catalog_name&gt;.&lt;nessie_config_property&gt;=&lt;config&gt;\n</code></pre> <p>To use Nessie Catalog in Hive via Iceberg, the following properties are required within Hive:</p> <ul> <li> <p><code>iceberg.catalog.&lt;catalog_name&gt;.warehouse</code> : The location where to store Iceberg tables managed by Nessie catalog. This will be the same location that is used to create an Iceberg table as it shown below.</p> </li> <li> <p><code>iceberg.catalog.&lt;catalog_name&gt;.ref</code> : The current Nessie branch. Note that Hive doesn\u2019t support the notation of <code>table@branch</code>, therefore everytime you want to execute against a specific branch, you will need to set this property to point to the working branch, e.g: <code>SET iceberg.catalog.&lt;catalog_name&gt;.ref=main</code>.</p> </li> <li> <p><code>iceberg.catalog.&lt;catalog_name&gt;.uri</code>: The location of the Nessie server.</p> </li> <li> <p><code>iceberg.catalog.&lt;catalog_name&gt;.authentication.type</code>: The authentication type to be used, please refer to the authentication docs for more info.</p> </li> </ul> <p>For example:</p> <pre><code>SET iceberg.catalog.&lt;catalog_name&gt;.warehouse=/home/user/notebooks/nessie_warehouse;\nSET iceberg.catalog.&lt;catalog_name&gt;.ref=dev;\nSET iceberg.catalog.&lt;catalog_name&gt;.catalog-impl=org.apache.iceberg.nessie.NessieCatalog;\nSET iceberg.catalog.&lt;catalog_name&gt;.uri=http://localhost:19120/api/v1;\n</code></pre>"},{"location":"iceberg/hive/#create-tables","title":"Create tables","text":"<p>Whenever Hive creates an Iceberg table, it will create it as external table that is managed by Iceberg catalog (in this case Nessie Catalog), thus, some properties need to be provided in order to create an Iceberg tables in Hive:</p> <p><pre><code>CREATE TABLE database_a.table_a (\n  id bigint, name string\n) PARTITIONED BY (\n  dept string\n) STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler'\n  LOCATION '/path_nessie_warehouse/database_a/salaries\n  TBLPROPERTIES ('iceberg.catalog'='&lt;catalog_name&gt;', 'write.format.default'='parquet');\n</code></pre> Whereby the above properties are explained as below:</p> <ul> <li> <p><code>STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler'</code> : Since Hive doesn\u2019t use a custom global catalog, we tell Hive here that individual table will be managed by Iceberg\u2019s catalog through <code>org.apache.iceberg.mr.hive.HiveIcebergStorageHandler</code>.</p> </li> <li> <p><code>LOCATION '/path_nessie_warehouse/database_a/salaries</code> : As mentioned before, Hive will create Iceberg tables as external tables and thus, location of the data files needs to be provided. This location is the same location that is provided in <code>iceberg.catalog.&lt;catalog_name&gt;.warehouse</code> in addition to the database and table names.</p> </li> <li> <p><code>'iceberg.catalog'='&lt;catalog_name&gt;'</code> : The custom Iceberg catalog to be used to manage this table.</p> </li> <li> <p><code>'write.format.default'='parquet'</code> : The format that is used to store the data, this could be anything that is supported by Iceberg, e.g: ORC.</p> </li> </ul>"},{"location":"iceberg/hive/#writing-and-reading-tables","title":"Writing and reading tables","text":"<p>To read and write into tables that are managed by Iceberg and Nessie, typical Hive SQL queries can be used. Refer to this documentation here for more information.</p> <p>Note: Hive doesn\u2019t support the notation of <code>&lt;table&gt;@&lt;branch&gt;</code>, therefore everytime you want to execute against a specific branch, you will need to set this property to point to the working branch, e.g: <code>SET iceberg.catalog.&lt;catalog_name&gt;.ref=main</code>. E.g: <pre><code>SET iceberg.catalog.&lt;catalog_name&gt;.ref=dev\n\nSELECT * FROM database_a.table_a;\n</code></pre></p>"},{"location":"iceberg/iceberg/","title":"Apache Iceberg","text":"<p>Apache Iceberg is an Apache Software Foundation project that provides a rich, relatively new table format. It provides:</p> <ul> <li>Single table ACID transactions</li> <li>Scalable metadata</li> <li>Appends via file addition</li> <li>Updates, deletes and merges via single record operations</li> </ul> <p>Nessie works seamlessly with Iceberg in Spark3. Nessie is implemented as a custom Iceberg catalog and therefore supports all features available to any Iceberg client. This includes Spark structured streaming, Presto, Trino, Flink and Hive. See the Iceberg docs for more info. Current Nessie Iceberg integration includes the following:</p> <ul> <li>Spark via Iceberg</li> <li>Flink via Iceberg</li> <li>Hive via Iceberg</li> <li>Trino via Iceberg</li> </ul>"},{"location":"iceberg/iceberg/#iceberg-extension-points","title":"Iceberg Extension Points","text":"<p>Iceberg exposes two primary classes for working with datasets. These are Catalog and TableOperations. Nessie implements each. These classes are available in the Iceberg source code and are available directly in Iceberg releases (eg <code>spark-runtime</code>, <code>spark3-runtime</code>, <code>flink-runtime</code>).</p>"},{"location":"iceberg/iceberg/#iceberg-snapshots","title":"Iceberg Snapshots","text":"<p>Iceberg supports the concept of snapshots. Snapshots are point in time versions of a table and are managed as part of each commit operation. Snapshots are limited to single table versioning. Nessie versions and commits provide a broader set of snapshot capabilities as they support multiple tables. Nessie is happy to coexist with Iceberg Snapshots. When working with Nessie, Iceberg snapshots will also be versioned along the rest of Iceberg metadata within the Nessie commit model.</p>"},{"location":"iceberg/iceberg/#automatic-snapshot-import","title":"Automatic Snapshot Import","text":"<p>We are exploring the creation of a tool where a user can import table snapshots across multiple Iceberg tables into a single Nessie repository to capture historical data snapshots (interleaved across time).</p>"},{"location":"iceberg/spark/","title":"Spark via Iceberg","text":"<p>Note</p> <p>Detailed steps on how to set up Pyspark + Iceberg + Nessie with Python is available on Binder</p> <p>To access Nessie on Iceberg from a spark cluster make sure the <code>spark.jars</code> spark option is set to include a jar of the iceberg spark runtime, or the <code>spark.jars.packages</code> spark option is set to include a Maven coordinate of the iceberg spark runtime.</p> <code>iceberg-spark-runtime</code> (required) <code>nessie-spark-extensions</code> (optional) Spark 3.5, Scala 2.12: <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:2.12</code>(All, Latest) <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:2.12</code>(All, Latest) Spark 3.5, Scala 2.13: <code>org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:2.13</code>(All, Latest) <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:2.13</code>(All, Latest) Spark 3.4, Scala 2.12: <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:2.12</code>(All, Latest) <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:2.12</code>(All, Latest) Spark 3.4, Scala 2.13: <code>org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:2.13</code>(All, Latest) <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:2.13</code>(All, Latest) Spark 3.3, Scala 2.12: <code>org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:2.12</code>(All, Latest) <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.3_2.12:2.12</code>(All, Latest) Spark 3.3, Scala 2.13: <code>org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:2.13</code>(All, Latest) <code>org.projectnessie.nessie-integrations:nessie-spark-extensions-3.3_2.12:2.13</code>(All, Latest) <p>The <code>iceberg-spark-runtime</code> fat jars are distributed by the Apache Iceberg project and contains all Apache Iceberg libraries required for operation, including the built-in Nessie Catalog.</p> <p>The <code>nessie-spark-extensions</code> jars are distributed by the Nessie project and contain SQL extensions that allow you to manage your tables with nessie\u2019s git-like syntax.</p> <p>In pyspark, usage would look like\u2026</p> Python <pre><code>SparkSession.builder\n    .config('spark.jars.packages',\n            'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.10.1')\n    ... rest of spark config\n    .getOrCreate()\n</code></pre> <p>\u2026or if using the nessie extensions\u2026</p> Python <pre><code>SparkSession.builder\n    .config('spark.jars.packages',\n            'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.10.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.104.5')\n    ... rest of spark config\n    .getOrCreate()\n</code></pre> <p>Note</p> <p>The Spark config parameter <code>spark.jars.packages</code> uses Maven coordinates to pull the given dependencies and all transitively required dependencies as well. Dependencies are resolved via the local Ivy cache, the local Maven repo and then against Maven Central. The config parameter <code>spark.jars</code> only takes a list of jar files and does not resolve transitive dependencies.</p> <p>The docs for the Java API in Iceberg explain how to use a <code>Catalog</code>. The only change is that a Nessie catalog should be instantiated</p> JavaPython <pre><code>Catalog catalog = new NessieCatalog(spark.sparkContext().hadoopConfiguration())\n</code></pre> <pre><code>catalog = jvm.NessieCatalog(sc._jsc.hadoopConfiguration())\n</code></pre> <p>Note</p> <p>Iceberg\u2019s python libraries are still under active development. Actions against catalogs in pyspark still have to go through the jvm objects. See the demo directory for details.</p>"},{"location":"iceberg/spark/#configuration","title":"Configuration","text":"<p>The Nessie Catalog needs the following parameters set in the Spark/Hadoop config.</p> <p>These are set as follows in code (or through other methods as described here)</p> <p>In these examples, <code>spark.jars.packages</code> is configured for Spark 3.3.x.  Consult the table above to find the version of that correspond to your Spark deployment.</p> JavaPython <pre><code>// Full url of the Nessie API endpoint to nessie\nString url = \"http://localhost:19120/api/v1\";\n// Where to store nessie tables\nString fullPathToWarehouse = ...;\n// The ref or context that nessie will operate on\n// (if different from default branch).\n// Can be the name of a Nessie branch or tag name.\nString ref = \"main\";\n// Nessie authentication type (NONE, BEARER, OAUTH2 or AWS)\nString authType = \"NONE\";\n\n    // for a local spark instance\n    conf.set(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.10.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.104.5\")\n        .set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\")\n        .set(\"spark.sql.catalog.nessie.uri\", url)\n        .set(\"spark.sql.catalog.nessie.ref\", ref)\n        .set(\"spark.sql.catalog.nessie.authentication.type\", authType)\n        .set(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n        .set(\"spark.sql.catalog.nessie.warehouse\", fullPathToWarehouse)\n        .set(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\");\n    spark = SparkSession.builder()\n                        .master(\"local[2]\")\n                        .config(conf)\n                        .getOrCreate();\n</code></pre> <pre><code># Full url of the Nessie API endpoint to nessie\nurl = \"http://localhost:19120/api/v1\"\n# Where to store nessie tables\nfull_path_to_warehouse = ...\n# The ref or context that nessie will operate on (if different from default branch).\n# Can be the name of a Nessie branch or tag name.\nref = \"main\"\n# Nessie authentication type (NONE, BEARER, OAUTH2 or AWS)\nauth_type = \"NONE\"\n\n    spark = SparkSession.builder \\\n            .config(\"spark.jars.packages\",\"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.10.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.104.5\") \\\n            .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\") \\\n            .config(\"spark.sql.catalog.nessie.uri\", url) \\\n            .config(\"spark.sql.catalog.nessie.ref\", ref) \\\n            .config(\"spark.sql.catalog.nessie.authentication.type\", auth_type) \\\n            .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\") \\\n            .config(\"spark.sql.catalog.nessie.warehouse\", full_path_to_warehouse) \\\n            .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n            .getOrCreate()\n</code></pre> <p>All configuration for the Nessie catalog exists below this <code>spark.sql.catalog.nessie</code> configuration namespace. The catalog name is not important, it is important that the required options are all given below the catalog name.</p> <p>The following properties are required in Spark when creating the Nessie Catalog (replace <code>&lt;catalog_name&gt;</code> with the name of your catalog):</p> <ul> <li><code>spark.sql.catalog.&lt;catalog_name&gt;.uri</code> : The location of the Nessie server.</li> <li><code>spark.sql.catalog.&lt;catalog_name&gt;.ref</code> : The default Nessie branch that the iceberg   catalog will use.</li> <li><code>spark.sql.catalog.&lt;catalog_name&gt;.authentication.type</code> : The authentication type to be used, set to <code>NONE</code> by default. Please refer to the Configuration and authentication in Tools docs for more info.</li> <li><code>spark.sql.catalog.&lt;catalog_name&gt;.catalog-impl</code> : This must be <code>org.apache.iceberg.nessie.NessieCatalog</code> in order to tell Spark to use Nessie catalog implementation.</li> <li><code>spark.sql.catalog.&lt;catalog_name&gt;.warehouse</code> : The location where to store Iceberg tables managed by Nessie catalog.</li> <li><code>spark.sql.catalog.&lt;catalog_name&gt;</code> : This must be <code>org.apache.iceberg.spark.SparkCatalog</code>. This is a Spark   option to set the catalog <code>&lt;catalog_name&gt;</code> to be managed by Nessie\u2019s Catalog implementation.</li> </ul> <p>Note</p> <p>An example of configuring Spark with Iceberg and an S3 bucket for the <code>warehouse</code> location is available in the Guides section.</p>"},{"location":"iceberg/spark/#writing","title":"Writing","text":"<p>Iceberg Catalog APIs can be used for creating the table as follows:</p> JavaPython <pre><code>// first instantiate the catalog\nNessieCatalog catalog = new NessieCatalog();\ncatalog.setConf(sc.hadoopConfiguration());\n// other catalog properties can be added based on the requirement. For example, \"io-impl\",\"authentication.type\", etc.\ncatalog.initialize(\"nessie\", ImmutableMap.of(\n    \"ref\", ref,\n    \"uri\", url,\n    \"warehouse\", pathToWarehouse));\n\n// Creating table by first creating a table name with namespace\nTableIdentifier region_name = TableIdentifier.parse(\"testing.region\");\n\n// next create the schema\nSchema region_schema = Schema([\n  Types.NestedField.optional(1, \"R_REGIONKEY\", Types.LongType.get()),\n  Types.NestedField.optional(2, \"R_NAME\", Types.StringType.get()),\n  Types.NestedField.optional(3, \"R_COMMENT\", Types.StringType.get()),\n]);\n\n// and the partition\nPartitionSpec region_spec = PartitionSpec.unpartitioned();\n\n// finally create the table\ncatalog.createTable(region_name, region_schema, region_spec);\n</code></pre> <pre><code>sc = spark.sparkContext\njvm = sc._gateway.jvm\n\n# import jvm libraries for iceberg catalogs and schemas\njava_import(jvm, \"org.projectnessie.iceberg.NessieCatalog\")\njava_import(jvm, \"org.apache.iceberg.catalog.TableIdentifier\")\njava_import(jvm, \"org.apache.iceberg.Schema\")\njava_import(jvm, \"org.apache.iceberg.types.Types\")\njava_import(jvm, \"org.apache.iceberg.PartitionSpec\")\n\n# first instantiate the catalog\ncatalog = jvm.NessieCatalog()\ncatalog.setConf(sc._jsc.hadoopConfiguration())\n# other catalog properties can be added based on the requirement. For example, \"io-impl\",\"authentication.type\", etc.\ncatalog.initialize(\"nessie\", {\"ref\": ref,\n    \"uri\": url,\n    \"warehouse\": pathToWarehouse})\n\n# Creating table by first creating a table name with namespace\nregion_name = jvm.TableIdentifier.parse(\"testing.region\")\n\n# next create the schema\nregion_schema = jvm.Schema([\n  jvm.Types.NestedField.optional(\n    1, \"R_REGIONKEY\", jvm.Types.LongType.get()\n  ),\n  jvm.Types.NestedField.optional(\n    2, \"R_NAME\", jvm.Types.StringType.get()\n  ),\n  jvm.Types.NestedField.optional(\n    3, \"R_COMMENT\", jvm.Types.StringType.get()\n  ),\n])\n\n# and the partition\nregion_spec = jvm.PartitionSpec.unpartitioned()\n\n# finally create the table\nregion_table = catalog.createTable(region_name, region_schema, region_spec)\n</code></pre> <p>When looking at the Python code above, lines 1-11 are importing jvm objects into pyspark. Lines 12-25 create the table name, schema and partition spec. These actions will be familiar to seasoned iceberg users and are wholly iceberg operations. Line 29 is where our initial iceberg metadata is finally written to disk and a commit takes place on Nessie.</p> <p>Now that we have created an Iceberg table in nessie we can write to it. The iceberg <code>DataSourceV2</code> allows for either <code>overwrite</code> or <code>append</code> mode in a standard <code>spark.write</code>.</p> <p>Spark support is constantly evolving. See the iceberg docs for an up-to-date support table.</p>"},{"location":"iceberg/spark/#spark3","title":"Spark3","text":"<p>Spark3 table creation/insertion is as follows:</p> JavaPythonSQL <pre><code>regionDf = spark.read().load('data/region.parquet');\n//create\nregionDf.writeTo(\"nessie.testing.region\").create();\n//append\nregionDf.writeTo(\"nessie.testing.region\").append();\n//overwrite partition\nregionDf.writeTo(\"nessie.testing.region\").overwritePartitions();\n</code></pre> <pre><code>region_df = spark.read.load(\"data/region.parquet\")\nregion_df.write.format(\"iceberg\").mode(\"overwrite\") \\\n    .save(\"nessie.testing.region\")\n</code></pre> <pre><code>CREATE NAMESPACE nessie.testing;\n\nCREATE TABLE nessie.testing.city (\n    C_CITYKEY BIGINT, C_NAME STRING, N_NATIONKEY BIGINT, C_COMMENT STRING\n) USING iceberg PARTITIONED BY (N_NATIONKEY)\n-- AS SELECT .. can be added to the sql statement to perform a CTAS\n\nINSERT INTO nessie.testing.city VALUES (1, 'a', 1, 'comment')\n</code></pre> <p>The full list of operations can be found here. Everything that Iceberg supports the Nessie Iceberg Catalog also supports.</p>"},{"location":"iceberg/spark/#reading","title":"Reading","text":"<p>To read a Nessie table in iceberg simply:</p> JavaPythonSQL <pre><code>regionDf = spark.table(\"nessie.testing.region\");\n</code></pre> <pre><code>region_df = spark.read.format(\"iceberg\").load(\"nessie.testing.region\")\n</code></pre> <p><pre><code>SELECT * FROM nessie.testing.city\n</code></pre> <pre><code>-- Read from the `etl` branch\nSELECT * FROM nessie.testing.`city@etl`\n</code></pre></p> <p>The examples above all use the default branch defined on initialisation. There are several ways to reference specific branches or hashes from within a read statement. We will take a look at a few now from pyspark3, the rules are the same across all environments though. The general pattern is <code>&lt;table&gt;@&lt;branch&gt;</code> or <code>&lt;table&gt;#&lt;hash&gt;</code> or <code>&lt;table&gt;@&lt;branch&gt;#&lt;hash&gt;</code>. Table must be present and either branch and/or hash are optional. We will throw an error if branch or hash don\u2019t exist. Branch or hash references in the table name will override passed <code>option</code>s and the settings in the Spark/Hadoop configs.</p> <pre><code># read from branch dev\nspark.read().format(\"iceberg\").load(\"testing.region@dev\")\n# read specifically from hash\nspark.read().format(\"iceberg\").load(\"testing.region#&lt;hash&gt;\")\n# read specifically from hash in dev branch\nspark.read().format(\"iceberg\").load(\"testing.region@dev#&lt;hash&gt;\")\n\nspark.sql(\"SELECT * FROM nessie.testing.`region@dev`\")\nspark.sql(\"SELECT * FROM nessie.testing.`region#&lt;hash&gt;`\")\nspark.sql(\"SELECT * FROM nessie.testing.`region@dev#&lt;hash&gt;`\")\n</code></pre> <p>Notice in the SQL statements the <code>&lt;table&gt;@&lt;branch&gt;</code> or <code>&lt;table&gt;#&lt;hash&gt;</code> or <code>&lt;table&gt;@&lt;branch&gt;#&lt;hash&gt;</code> must be escaped separately from namespace or catalog arguments.</p> <p>Future versions may add the ability to specify a timestamp to query the data at a specific point in time (time-travel). In the meantime the history can be viewed on the command line or via the python client and a specific hash based on commit time can be extracted for use in the spark catalog. It is recommended to use the time-travel features of Nessie over the Iceberg features as Nessie history is consistent across the entire database.</p>"},{"location":"iceberg/trino/","title":"Trino via Iceberg","text":"<p>Trino can be deployed for use with Nessie by any of the methods mentioned in the Trino Installation Guide. To access Iceberg tables, one needs to configure the Iceberg connector. Refer to the Trino Iceberg Connector Documentation for more information. Nessie catalog properties required for the Iceberg connector can be found at https://trino.io/docs/current/object-storage/metastores.html#nessie-catalog.</p> <p>Sample Nessie configuration for Iceberg connector in <code>etc/catalog/iceberg.properties</code>: <pre><code>connector.name=iceberg\niceberg.catalog.type=nessie\niceberg.nessie-catalog.uri=http://localhost:19120/api/v2\niceberg.nessie-catalog.ref=main\niceberg.nessie-catalog.default-warehouse-dir=s3://warehouse\nfs.native-s3.enabled=true\ns3.endpoint=https://&lt;s3_url&gt;\ns3.region=&lt;aws_region&gt;\ns3.aws-access-key=&lt;aws_access_key&gt;\ns3.aws-secret-key=&lt;aws_secret_key&gt;\n</code></pre></p> <p>Note</p> <ol> <li>It is recommended to use Trino version 443 and above, which includes Iceberg 1.5.0 with bug fixes for Nessie integration.</li> <li>Trino currently lacks SQL support for managing catalog-level branches and tags related to Nessie.    a. To create branches and tags, use the Nessie CLI available here.    b. To switch references (branch or tag), update the catalog property <code>iceberg.nessie-catalog.ref</code>.</li> </ol> <p>Nessie can also be accessed via REST catalog path. Sample Nessie-REST configuration for Iceberg connector in <code>etc/catalog/iceberg.properties</code>: <pre><code>connector.name=iceberg\niceberg.catalog.type=rest\niceberg.rest-catalog.uri=http://localhost:19120/iceberg\niceberg.rest-catalog.prefix=main\n</code></pre></p>"},{"location":"iceberg/views/","title":"Apache Iceberg Views","text":"<p>Nessie supports versioning Iceberg views. A view is composed of the following properties:</p> <ul> <li>Metadata Location</li> <li>Version ID</li> <li>Schema ID</li> <li>SQL Text</li> <li>Dialect (such as Hive, Spark, Dremio, Presto) </li> </ul> <p>This enables Iceberg views to be versioned along with underlying datasets to provide a  complete place for logical and physical experimentation. Because SQL dialects differ  by system, Nessie does not parse or understand SQL. It relies on the creator of SQL statements  to validate the provided SQL before being stored in Nessie.</p> <p>Additional information about Iceberg Views can be found in the View Spec</p>"},{"location":"nessie-latest/","title":"Nessie 0.106.0","text":"<p>Older release downloads are available via GitHub.</p> <p>Download options for this Nessie 0.106.0 release:</p> <ul> <li>Nessie Server</li> <li>CLI &amp; REPL</li> <li>GC Tool</li> <li>Server Admin Tool</li> <li>REST OpenAPI</li> <li>Maven Central</li> <li>License Reports</li> </ul> <p>GitHub release page for 0.106.0 with release artifacts.</p>"},{"location":"nessie-latest/#nessie-server","title":"Nessie Server","text":"<p>The main Nessie server serves the Nessie repository using the Iceberg REST API and Nessie\u2019s native REST API.</p> Docker ImageHelm ChartStandalone Jar <p>Docker images are multiplatform images for amd64, arm64, ppc64le and s390x. They are available from the following repositories:</p> <ul> <li>GitHub Container Registry:</li> </ul> <pre><code>docker pull ghcr.io/projectnessie/nessie:0.106.0\ndocker run -p 19120:19120 -p 9000:9000 ghcr.io/projectnessie/nessie:0.106.0\n</code></pre> <ul> <li>Quay.io:</li> </ul> <pre><code>docker pull quay.io/projectnessie/nessie:0.106.0\ndocker run -p 19120:19120 -p 9000:9000 quay.io/projectnessie/nessie:0.106.0\n</code></pre> <p>Nessie 0.106.0 Helm chart is available from the following locations:</p> <ul> <li>Nessie Helm Repo:</li> </ul> <pre><code>helm repo add nessie https://charts.projectnessie.org/\nhelm repo update\nhelm install my-nessie nessie/nessie --version \"0.106.0\"\n</code></pre> <ul> <li>Artifact Hub.</li> <li>Nessie 0.106.0 Helm Chart Tarball.</li> </ul> <p>Java version: minimum 17, 21 recommended, supported operating systems</p> <pre><code>curl -L -o nessie-quarkus-0.106.0-runner.jar \\\n  https://github.com/projectnessie/nessie/releases/download/nessie-0.106.0/nessie-quarkus-0.106.0-runner.jar\njava -jar nessie-quarkus-0.106.0-runner.jar\n</code></pre>"},{"location":"nessie-latest/#nessie-cli-repl","title":"Nessie CLI &amp; REPL","text":"<p>Nessie CLI is both a command-line interface but primarily a REPL.</p> Docker ImageStandalone Jar <p>Docker images are multiplatform images for amd64, arm64, ppc64le, s390x.</p> <p>They are available from the following repositories:</p> <ul> <li>GitHub Container Registry:</li> </ul> <pre><code>docker pull ghcr.io/projectnessie/nessie-cli:0.106.0\ndocker run -it ghcr.io/projectnessie/nessie-cli:0.106.0 \n</code></pre> <ul> <li>Quay.io:</li> </ul> <pre><code>docker pull quay.io/projectnessie/nessie-cli:0.106.0\ndocker run -it quay.io/projectnessie/nessie-cli:0.106.0\n</code></pre> <p>Requires Java 11, Java 21 recommended.</p> <pre><code>curl -L -o nessie-cli-0.106.0.jar \\\n  https://github.com/projectnessie/nessie/releases/download/nessie-0.106.0/nessie-cli-0.106.0.jar\njava -jar nessie-cli-0.106.0.jar\n</code></pre>"},{"location":"nessie-latest/#nessie-gc-tool","title":"Nessie GC Tool","text":"<p>Nessie GC allows mark and sweep data files based on flexible expiration policies.</p> Docker ImageStandalone Jar <p>Docker images are multiplatform images for amd64, arm64, ppc64le, s390x. They are available from the following repositories:</p> <ul> <li>GitHub Container Registry:</li> </ul> <pre><code>docker pull ghcr.io/projectnessie/nessie-gc:0.106.0\ndocker run ghcr.io/projectnessie/nessie-gc:0.106.0 --help\n</code></pre> <ul> <li>Quay.io:</li> </ul> <pre><code>docker pull quay.io/projectnessie/nessie-gc:0.106.0\ndocker run quay.io/projectnessie/nessie-gc:0.106.0 --help\n</code></pre> <p>Requires Java 17, Java 21 recommended, supported operating systems</p> <pre><code>curl -L -o nessie-gc-0.106.0.jar \\\n  https://github.com/projectnessie/nessie/releases/download/nessie-0.106.0/nessie-gc-0.106.0.jar\njava -jar nessie-gc-0.106.0.jar\n</code></pre>"},{"location":"nessie-latest/#nessie-server-admin-tool","title":"Nessie Server Admin Tool","text":"<p>Nessie\u2019s Server Admin Tool allows migration (export/import) of a Nessie repository.</p> Docker ImageStandalone Jar <p>Docker images are multiplatform images for amd64, arm64, ppc64le, s390x. They are available from the following repositories:</p> <ul> <li>GitHub Container Registry:</li> </ul> <pre><code>docker pull ghcr.io/projectnessie/nessie-server-admin:0.106.0\ndocker run ghcr.io/projectnessie/nessie-server-admin:0.106.0 --help\n</code></pre> <ul> <li>Quay.io:</li> </ul> <pre><code>docker pull quay.io/projectnessie/nessie-server-admin:0.106.0\ndocker run quay.io/projectnessie/nessie-server-admin:0.106.0 --help\n</code></pre> <p>Java version: minimum 17, 21 recommended, supported operating systems</p> <pre><code>curl -L -o nessie-server-admin-tool-0.106.0-runner.jar \\\n  https://github.com/projectnessie/nessie/releases/download/nessie-0.106.0/nessie-server-admin-tool-0.106.0-runner.jar\njava -jar nessie-server-admin-tool-0.106.0-runner.jar\n</code></pre>"},{"location":"nessie-latest/#nessie-rest-api","title":"Nessie REST API","text":"Download <p>OpenAPI Download</p>"},{"location":"nessie-latest/#nessie-artifacts-on-maven-central","title":"Nessie artifacts on Maven Central","text":"<p>Artifacts are available in two groups: <code>org.projectnessie.nessie</code> and <code>org.projectnessie.nessie-integrations</code>. Most users will only need the <code>org.projectnessie.nessie</code> group, which contains the Nessie server and CLI. The <code>org.projectnessie.nessie-integrations</code> group contains additional tools and integrations:</p> <ul> <li>Spark extensions</li> <li>Nessie GC tool</li> </ul> <p>Useful links:</p> <ul> <li>Nessie 0.106.0 BOM (Bill of Materials)</li> <li>Nessie 0.106.0 <code>org.projectnessie.nessie</code> artifacts</li> <li>Nessie 0.106.0 <code>org.projectnessie.nessie-integrations</code> artifacts</li> </ul> <p>The following examples show how to add the Nessie BOM to your build configuration:</p> MavenGradle (Kotlin) <p>In your Maven <code>pom.xml</code> add the Nessie BOM as a dependency: <pre><code>&lt;dependencyManagement&gt;\n  &lt;dependencies&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;org.projectnessie.nessie&lt;/groupId&gt;\n      &lt;artifactId&gt;nessie-bom&lt;/artifactId&gt;\n      &lt;version&gt;0.106.0&lt;/version&gt;\n      &lt;type&gt;pom&lt;/type&gt;\n      &lt;scope&gt;import&lt;/scope&gt;\n    &lt;/dependency&gt;\n  &lt;/dependencies&gt;\n&lt;/dependencyManagement&gt;\n</code></pre> Then you can use all Nessie artifacts like this: <pre><code>&lt;dependencies&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;org.projectnessie.nessie&lt;/groupId&gt;\n    &lt;artifactId&gt;nessie-client&lt;/artifactId&gt;\n  &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre></p> <p>In your Gradle project\u2019s <code>build.gradle.kts</code> add the Nessie BOM as an enforced platform: <pre><code>dependencies {\n  enforcedPlatform(\"org.projectnessie.nessie:nessie-bom:0.106.0\")\n}\n</code></pre> A full example using the <code>nessie-client</code> artifact: <pre><code>dependencies {\n  enforcedPlatform(\"org.projectnessie.nessie:nessie-bom:0.106.0\")\n  implementation(\"org.projectnessie.nessie:nessie-client\")\n}\n</code></pre></p>"},{"location":"nessie-latest/#license-reports","title":"License Reports","text":"<p>License reports for this release are available via this link (zip file).</p>"},{"location":"nessie-latest/authentication/","title":"Authentication","text":"<p>By default, Nessie servers run with authentication disabled and all requests are processed under the \u201canonymous\u201d user identity. In Nessie clients this authentication type is known as <code>NONE</code>.</p> <p>When a Nessie server runs as an AWS Lambda, access to its API is controlled by AWS authentication settings. In this case there is no need to configure any additional authentication in the Nessie server. In Nessie clients this authentication type is known as <code>AWS</code>.  </p> <p>When a Nessie API is exposed to clients without any external authentication layer, the server itself can be configured to authenticate clients using OpenID tokens as described in the section below. On the client side this authentication type is known as <code>BEARER</code> authentication.</p> <p>For client-side authentication settings refer to the following pages:</p> <ul> <li>Nessie CLI</li> <li>Java Client</li> <li>Authentication in Tools</li> </ul>"},{"location":"nessie-latest/authentication/#openid-bearer-tokens","title":"OpenID Bearer Tokens","text":"<p>Nessie supports bearer tokens and uses OpenID Connect for validating them.</p> <p>To enable bearer authentication the following configuration properties need to be set  for the Nessie Server process:</p> <ul> <li><code>nessie.server.authentication.enabled=true</code></li> <li><code>quarkus.oidc.auth-server-url=&lt;OpenID Server URL&gt;</code></li> <li><code>quarkus.oidc.client-id=&lt;Client ID&gt;</code></li> </ul> <p>If you also want to use authentication for the Nessie Web UI, the following properties need to be set as well:</p> <ul> <li><code>quarkus.oidc.ui-app.auth-server-url=&lt;OpenID Server URL&gt;</code></li> <li><code>quarkus.oidc.ui-app.client-id=&lt;Client ID&gt;</code></li> <li><code>quarkus.oidc.ui-app.application-type=web-app</code></li> <li><code>quarkus.oidc.ui-app.tenant-paths=/,/tree*,/content*,/notfound*,/commits*</code></li> </ul> <p>When using Nessie Docker images, the authentication options can be specified on the <code>docker</code> command line as environment variables, for example:</p> <pre><code>$ docker run -p 19120:19120 \\\n  -e QUARKUS_OIDC_CLIENT_ID=$YOUR_OIDC_Client_ID \\\n  -e QUARKUS_OIDC_AUTH_SERVER_URL=$YOUR_OpenID_Server_URL \\\n  -e NESSIE_SERVER_AUTHENTICATION_ENABLED=true \\\n  --network host \\\n  ghcr.io/projectnessie/nessie\n</code></pre> <p>Note the use of the <code>host</code> Docker network. In this example, it is assumed that the Open ID Server is available on the host network. More advanced network setup is possible, of course.</p>"},{"location":"nessie-latest/authorization/","title":"Authorization (Access Control)","text":""},{"location":"nessie-latest/authorization/#authorization-scope","title":"Authorization scope","text":"<p>It is important to note that Nessie does not store data directly but only data location and other metadata. </p> <p>As a consequence, the Nessie authorization layer can only really control access to metadata, but might not prevent data itself to be accessed directly without interacting with Nessie.  It is then expected that another system can control access to data itself to make sure unauthorized access isn\u2019t possible.</p> <p>The same is true for access to historical data, which is one of Nessie\u2019s main features. For example, while it might seem safe committing a change that removes undesired sensitive data and restricting access to only the latest version of the dataset, the truth is that the sensitive data may still exist on the data lake and be accessed by other means  (similar to how redacting a PDF by adding black boxes on top of sensitive information does not prevent people to read what is written beneath in most cases).  The only safe way to remove this data is to remove it from the table (e.g. via <code>DELETE</code> statements) and then run the Garbage Collection algorithm to ensure the data has been removed from Nessie history and deleted on the data lake.</p>"},{"location":"nessie-latest/authorization/#stories","title":"Stories","text":"<p>Here\u2019s a list of common authorization scenarios:</p> <ul> <li>Alice attempts to execute a query against the table <code>Foo</code> on branch <code>prod</code>. As she has read access to the table on this branch, Nessie allows the execution engine to get the table details.</li> <li>Bob attempts to execute a query against the table <code>Foo</code> on branch <code>prod</code>. However, Bob does not have read access to the table. Nessie returns an authorization error, and the execution engine refuses to execute the query.</li> <li>Carol has access to the content on branch <code>prod</code>, but not to the table <code>Foo</code> on this branch. Carol creates a new reference named <code>carol-branch</code> with the same hash as <code>prod</code>, and attempts to change permissions on table <code>Foo</code>. However, request is denied and Carol cannot access the content of <code>Foo</code>.</li> <li>Dave has access to the content on branch <code>prod</code>, and wants to update the content of the table <code>Foo</code>. He creates a new reference named <code>dave-experiment</code>, and executes several queries against this branch to modify table <code>Foo</code>. Each modification is a commit done against <code>dave-experiment</code> branch which is approved by the Nessie server. When all the desired modifications are done, Dave attempts to merge the changes back to the <code>prod</code> branch. However, Dave doesn\u2019t have the rights to modify the <code>prod</code> branch, causing Nessie to deny the request.</li> </ul>"},{"location":"nessie-latest/authorization/#access-control-model","title":"Access control model","text":"<p>Any object in Nessie can be designated by a pair of coordinates (reference, path), therefore access control is also designed around those two concepts.</p>"},{"location":"nessie-latest/authorization/#access-control-against-references","title":"Access control against references","text":"<p>References can be designated by their name (branches and tags) and there are several operations that can be exercised:</p> <ul> <li>view/list available references</li> <li>create a new named reference</li> <li>assign a hash to a reference</li> <li>delete a reference</li> <li>list objects present in the tree</li> <li>read objects content in the tree</li> <li>commit a change against the reference</li> </ul> <p>Note that a user needs to be able to view a reference in order to list objects on that reference.</p>"},{"location":"nessie-latest/authorization/#access-control-against-paths","title":"Access control against paths","text":"<p>For a specific reference, an entity is designated by its path which is why a simple way of performing access control can be done by applying restrictions on path.</p> <p>Several operations can be exercised against an entity:</p> <ul> <li>create a new entity</li> <li>delete an entity</li> <li>update entity\u2019s content</li> </ul> <p>Note that those operations combine themselves with the reference operations. For example to actually be able to update the content of an entity, user needs both permission to do the update AND to commit the change against the reference where the change will be stored</p>"},{"location":"nessie-latest/authorization/#service-provider-interface","title":"Service Provider Interface","text":"<p>The SPI is named AccessChecker and uses AccessContext, which carries information about the overall context of the operation. Implementers of <code>AccessChecker</code> are completely free to define their own way of creating/updating/checking authorization rules.</p>"},{"location":"nessie-latest/authorization/#contentid-usage","title":"ContentId Usage","text":"<p>Note that there is a <code>contentId</code> parameter in some methods of the AccessChecker, which allows checking specific rules for a given entity at a given point in time. The <code>contentId</code> parameter refers to the ID of a <code>Content</code> object and its contract is defined here.</p> <p>One can think of this similar to how permissions are defined in Google Docs. There are some permissions that are specific to the parent folder and to the doc itself. When a Doc is moved from one folder to another, it inherits the permissions of the parent folder. However, the doc-specific permissions are carried over with the doc and still apply. The same is true in the context of entities. There are some rules that apply to an entity in a global fashion, and then there\u2019s the possibility to define rules specific to the <code>contentId</code> of an entity.</p>"},{"location":"nessie-latest/authorization/#reference-implementation-for-metadata-authorization","title":"Reference implementation for Metadata Authorization","text":"<p>The reference implementation allows defining authorization rules via application.properties and is therefore dependent on Quarkus. Nessie\u2019s metadata authorization can be enabled via <code>nessie.server.authorization.enabled=true</code>.</p>"},{"location":"nessie-latest/authorization/#authorization-rules","title":"Authorization Rules","text":"<p>Authorization rule definitions are using a Common Expression Language (CEL) expression (an intro to CEL can be found at https://github.com/google/cel-spec/blob/master/doc/intro.md).</p> <p>Rule definitions are of the form <code>nessie.server.authorization.rules.&lt;ruleId&gt;=&lt;rule_expression&gt;</code>, where <code>&lt;ruleId&gt;</code> is a unique identifier for the rule.</p> <p><code>&lt;rule_expression&gt;</code> is basically a CEL expression string, which allows lots of flexibility on a given set of variables. </p>"},{"location":"nessie-latest/authorization/#variables","title":"Variables","text":"<p>Certain variables are available within the <code>&lt;rule_expression&gt;</code> depending on context:</p> <ul> <li>op - refers to the type name (string) of the operation that is being authorized.   See BatchAccessChecker   and Check types for details.</li> <li>role - refers to the user\u2019s primary role and can be any string.</li> <li>roles - refers to the list of all known user roles.</li> <li>ref - refers to a string representing a branch/tag name or <code>DETATCHED</code> for direct access to a commit id.</li> <li>path - refers to the URI path representation (<code>ContentKey.toPathString()</code>) of the content key for the object related to the authorization check.</li> <li>contentType - refers to a (possibly empty) string representing the name of the object\u2019s <code>Content.Type</code>.</li> <li>type - refers to the repository config type to be retrieved or updated.</li> <li>api - contains information about the receiving API. This is a composite object with two properties:</li> <li>apiName the name of the API, can be <code>Nessie</code> or <code>Iceberg</code></li> <li>apiVersion the version of the API - for <code>Nessie</code> it can be 1 or 2, for <code>Iceberg</code> currently 1</li> <li>actions a list of actions (strings), available for some Iceberg endpoints.</li> </ul>"},{"location":"nessie-latest/authorization/#actions","title":"Actions","text":"<p>The list of <code>actions</code> (strings) is available for some Iceberg endpoints that perform changes against an entity (table, view, namespace). The list of actions is empty for the Nessie REST API.</p> <p>Catalog operations</p> <p>Available for all updating Iceberg endpoints for the <code>Check</code> types <code>CREATE_ENTITY</code>, <code>UPDATE_ENTITY</code> and <code>DELETE_ENTITY</code>.</p> <ul> <li><code>CATALOG_CREATE_ENTITY</code> - create a table/view/namespace</li> <li><code>CATALOG_UPDATE_ENTITY</code> - update a table/view/namespace</li> <li><code>CATALOG_DROP_ENTITY</code> - dropping a table/view/namespace</li> <li><code>CATALOG_RENAME_ENTITY_FROM</code> - renaming a table (from)</li> <li><code>CATALOG_RENAME_ENTITY_TO</code> - renaming a table (to)</li> <li><code>CATALOG_REGISTER_ENTITY</code> - registering a table (from)</li> <li><code>CATALOG_UPDATE_MULTIPLE</code> - update multiple tables</li> <li><code>CATALOG_S3_SIGN</code> - S3 request signing</li> </ul> <p>Iceberg metadata updates</p> <p>Available for Iceberg endpoints that update entities, represents the kinds of metadata updates, for the <code>Check</code> types <code>CREATE_ENTITY</code>, <code>UPDATE_ENTITY</code>.</p> <ul> <li><code>META_ADD_VIEW_VERSION</code></li> <li><code>META_SET_CURRENT_VIEW_VERSION</code></li> <li><code>META_SET_STATISTICS</code></li> <li><code>META_REMOVE_STATISTICS</code></li> <li><code>META_SET_PARTITION_STATISTICS</code></li> <li><code>META_REMOVE_PARTITION_STATISTICS</code></li> <li><code>META_ASSIGN_UUID</code></li> <li><code>META_ADD_SCHEMA</code></li> <li><code>META_SET_CURRENT_SCHEMA</code></li> <li><code>META_ADD_PARTITION_SPEC</code></li> <li><code>META_SET_DEFAULT_PARTITION_SPEC</code></li> <li><code>META_ADD_SNAPSHOT</code></li> <li><code>META_ADD_SORT_ORDER</code></li> <li><code>META_SET_DEFAULT_SORT_ORDER</code></li> <li><code>META_SET_LOCATION</code></li> <li><code>META_SET_PROPERTIES</code></li> <li><code>META_REMOVE_PROPERTIES</code></li> <li><code>META_REMOVE_LOCATION_PROPERTY</code></li> <li><code>META_SET_SNAPSHOT_REF</code></li> <li><code>META_REMOVE_SNAPSHOT_REF</code></li> <li><code>META_UPGRADE_FORMAT_VERSION</code></li> </ul> <p>from Iceberg\u2019s snapshot summary</p> <p>Available for Iceberg updates that add a snapshot, for the <code>Check</code> types <code>CREATE_ENTITY</code>, <code>UPDATE_ENTITY</code>.</p> <ul> <li><code>SNAP_ADD_DATA_FILES</code></li> <li><code>SNAP_DELETE_DATA_FILES</code></li> <li><code>SNAP_ADD_DELETE_FILES</code></li> <li><code>SNAP_ADD_EQUALITY_DELETE_FILES</code></li> <li><code>SNAP_ADD_POSITION_DELETE_FILES</code></li> <li><code>SNAP_REMOVE_DELETE_FILES</code></li> <li><code>SNAP_REMOVE_EQUALITY_DELETE_FILES</code></li> <li><code>SNAP_REMOVE_POSITION_DELETE_FILES</code></li> <li><code>SNAP_ADDED_RECORDS</code></li> <li><code>SNAP_DELETED_RECORDS</code></li> <li><code>SNAP_ADDED_POSITION_DELETES</code></li> <li><code>SNAP_DELETED_POSITION_DELETES</code></li> <li><code>SNAP_ADDED_EQUALITY_DELETES</code></li> <li><code>SNAP_DELETED_EQUALITY_DELETES</code></li> <li><code>SNAP_REPLACE_PARTITIONS</code></li> <li><code>SNAP_OP_APPEND</code></li> <li><code>SNAP_OP_REPLACE</code></li> <li><code>SNAP_OP_OVERWRITE</code></li> <li><code>SNAP_OP_DELETE</code></li> </ul>"},{"location":"nessie-latest/authorization/#checks-for-reference-operations","title":"Checks for Reference operations","text":"<p>Applicable <code>op</code> types:</p> <ul> <li><code>VIEW_REFERENCE</code></li> <li><code>CREATE_REFERENCE</code></li> <li><code>DELETE_REFERENCE</code></li> <li><code>ASSIGN_REFERENCE_TO_HASH</code></li> <li><code>DELETE_REFERENCE</code></li> <li><code>READ_ENTRIES</code></li> <li><code>LIST_COMMIT_LOG</code></li> <li><code>COMMIT_CHANGE_AGAINST_REFERENCE</code></li> </ul> <p>Available variables:</p> <ul> <li><code>role</code></li> <li><code>roles</code></li> <li><code>ref</code></li> <li><code>api</code></li> </ul>"},{"location":"nessie-latest/authorization/#checks-for-content-operations","title":"Checks for Content operations","text":"<p>Applicable <code>op</code> types:</p> <ul> <li><code>READ_CONTENT_KEY</code></li> <li><code>READ_ENTITY_VALUE</code></li> <li><code>CREATE_ENTITY</code></li> <li><code>UPDATE_ENTITY</code></li> <li><code>DELETE_ENTITY</code></li> </ul> <p>Available variables:</p> <ul> <li><code>role</code></li> <li><code>roles</code></li> <li><code>ref</code></li> <li><code>path</code></li> <li><code>contentType</code></li> <li><code>api</code></li> <li><code>actions</code> (for <code>CREATE_ENTITY</code>, <code>UPDATE_ENTITY</code>, <code>DELETE_ENTITY</code> against Iceberg REST)</li> </ul>"},{"location":"nessie-latest/authorization/#checks-for-repository-config-operations","title":"Checks for Repository Config operations","text":"<p>Applicable <code>op</code> types:</p> <ul> <li><code>READ_REPOSITORY_CONFIG</code></li> <li><code>UPDATE_REPOSITORY_CONFIG</code></li> </ul> <p>Available variables:</p> <ul> <li><code>role</code></li> <li><code>roles</code></li> <li><code>type</code></li> <li><code>api</code></li> </ul>"},{"location":"nessie-latest/authorization/#relevant-cel-features","title":"Relevant CEL features","text":"<p>Since all available authorization rule variables are strings, the relevant CEL-specific things that are worth mentioning are shown below:</p> <ul> <li>equality and inequality</li> <li>regular expressions</li> <li>operators &amp; functions</li> </ul>"},{"location":"nessie-latest/authorization/#example-authorization-rules","title":"Example authorization rules","text":"<p>Below are some basic examples that show how to give a permission for a particular operation. In reality, one would want to keep the number of authorization rules for a single user/role low and grant permissions for all required operations through as few rules as possible.</p> <ul> <li> <p>allows viewing the branch/tag starting with the name <code>allowedBranch</code> for the role that starts with the name <code>test_</code>: <pre><code>nessie.server.authorization.rules.allow_branch_listing=\\\n  op=='VIEW_REFERENCE' &amp;&amp; role.startsWith('test_') &amp;&amp; ref.startsWith('allowedBranch')\n</code></pre></p> </li> <li> <p>allows creating branches/tags that match the regex <code>.*allowedBranch.*</code> for the role <code>test_user</code>: <pre><code>nessie.server.authorization.rules.allow_branch_creation=\\\n  op=='CREATE_REFERENCE' &amp;&amp; role=='test_user' &amp;&amp; ref.matches('.*allowedBranch.*')\n</code></pre></p> </li> <li> <p>allows deleting branches/tags that end with <code>allowedBranch</code> for the role named <code>test_user123</code>: <pre><code>nessie.server.authorization.rules.allow_branch_deletion=\\\n  op in ['VIEW_REFERENCE', 'DELETE_REFERENCE'] &amp;&amp; role=='test_user123' &amp;&amp; ref.endsWith('allowedBranch')\n</code></pre></p> </li> <li> <p>allows listing the commit log for all branches/tags starting with <code>dev</code>: <pre><code>nessie.server.authorization.rules.allow_listing_commitlog=\\\n  op in ['VIEW_REFERENCE', 'LIST_COMMIT_LOG'] &amp;&amp; ref.startsWith('dev')\n</code></pre></p> </li> <li> <p>allows reading the entity value where teh <code>path</code> starts with <code>allowed.</code> for the role <code>test_user</code>: <pre><code>nessie.server.authorization.rules.allow_reading_entity_value=\\\n  op in ['VIEW_REFERENCE', 'READ_ENTITY_VALUE'] &amp;&amp; role=='test_user' &amp;&amp; path.startsWith('allowed.')\n</code></pre></p> </li> <li> <p>allows deleting the entity where the <code>path</code> starts with <code>dev.</code> for all roles: <pre><code>nessie.server.authorization.rules.allow_deleting_entity=\\\n  op in ['VIEW_REFERENCE', 'DELETE_ENTITY'] &amp;&amp; path.startsWith('dev.')\n</code></pre></p> </li> <li> <p>allows listing reflog for the role <code>admin_user</code>: <pre><code>nessie.server.authorization.rules.allow_listing_reflog=\\\n  op=='VIEW_REFLOG' &amp;&amp; role=='admin_user'\n</code></pre></p> </li> </ul>"},{"location":"nessie-latest/authorization/#example-authorization-rules-from-stories-section","title":"Example authorization rules from Stories section","text":"<p>As mentioned in the Stories section, a few common scenarios that are possible are:</p> <ul> <li>Alice attempts to execute a query against the table <code>Foo</code> on branch <code>prod</code>. As she has read access to the table on this branch, Nessie allows the execution engine to get the table details.</li> <li>Bob attempts to execute a query against the table <code>Foo</code> on branch <code>prod</code>. However, Bob does not have read access to the table. Nessie returns an authorization error, and the execution engine refuses to execute the query.</li> <li>Carol has access to the content on branch <code>prod</code>, but not to the table <code>Foo</code> on this branch. Carol creates a new reference named <code>carol-branch</code> with the same hash as <code>prod</code>, and attempts to change permissions on table <code>Foo</code>. However, request is denied and Carol cannot access the content of <code>Foo</code>.</li> <li>Dave has access to the content on branch <code>prod</code>, and wants to update the content of the table <code>Foo</code>. He creates a new reference named <code>dave-experiment</code>, and executes several queries against this branch to modify table <code>Foo</code>. Each modification is a commit done against <code>dave-experiment</code> branch which is approved by the Nessie server. When all the desired modifications are done, Dave attempts to merge the changes back to the <code>prod</code> branch. However, Dave doesn\u2019t have the rights to modify the <code>prod</code> branch, causing Nessie to deny the request.</li> </ul> <p>Below are the respective authorization rules for these scenarios: <pre><code># read access for all on the prod branch\nnessie.server.authorization.rules.prod=\\\n  op in ['VIEW_REFERENCE'] &amp;&amp; ref=='prod' &amp;&amp; role in ['Alice', 'Bob', 'Carol', 'Dave']\n\n# alice &amp; dave can read Foo  \nnessie.server.authorization.rules.reading_foo_on_prod=\\\n  op in ['READ_ENTITY_VALUE'] &amp;&amp; ref=='prod' &amp;&amp; path=='Foo' &amp;&amp; role in ['Alice', 'Dave']\n\n# specific rules for carol on her branch\nnessie.server.authorization.rules.carol-branch=\\\n  op in ['VIEW_REFERENCE', 'CREATE_REFERENCE', 'DELETE_REFERENCE', 'COMMIT_CHANGE_AGAINST_REFERENCE'] &amp;&amp; ref=='carol-branch' &amp;&amp; role=='Carol'\n\n# specific rules for dave on his branch\nnessie.server.authorization.rules.dave-experiment=\\\n  op in ['VIEW_REFERENCE', 'CREATE_REFERENCE', 'DELETE_REFERENCE', 'COMMIT_CHANGE_AGAINST_REFERENCE'] &amp;&amp; ref=='dave-experiment' &amp;&amp; role=='Dave'\n\n# bob can read/update/delete BobsBar only\nnessie.server.authorization.rules.bob=\\\n  op in ['READ_ENTITY_VALUE', 'UPDATE_ENTITY', 'DELETE_ENTITY'] &amp;&amp; path=='BobsBar` &amp;&amp; role=='Bob')\n\n# carol can read/update/delete CarolsSecret\nnessie.server.authorization.rules.carol=\\\n  op in ['READ_ENTITY_VALUE', 'UPDATE_ENTITY', 'DELETE_ENTITY'] &amp;&amp; path=='CarolsSecret` &amp;&amp; role=='Alice')\n\n# dave can read/update/delete DavesHiddenX\nnessie.server.authorization.rules.dave=\\\n  op in ['READ_ENTITY_VALUE', 'UPDATE_ENTITY', 'DELETE_ENTITY'] &amp;&amp; path=='DavesHiddenX` &amp;&amp; role=='Dave')\n</code></pre></p>"},{"location":"nessie-latest/cli/","title":"Nessie CLI","text":"<p>The Nessie CLI is an easy way to get started with Nessie. It supports multiple branch  and tag management capabilities.</p> <p>Nessie CLI is designed to be usable as an interactive REPL supporting auto-completion, highlighting where appropriate and has built-in help. Long outputs, like a commit log, are automatically paged like the Unix <code>less</code> command.</p> <p></p>"},{"location":"nessie-latest/cli/#installation","title":"Installation","text":"<p>Nessie CLI is available as a standalone uber jar, or as a Docker image. See download options in the Nessie download page or in the releases page on GitHub.</p>"},{"location":"nessie-latest/cli/#usage","title":"Usage","text":"<p>Use <code>CONNECT TO http://127.0.0.1:19120/iceberg</code> to connect to a locally running Nessie instance with Iceberg REST. Use <code>CONNECT TO http://127.0.0.1:19120/api/v2</code> for Nessie\u2019s native REST API.</p> <p>Use <code>CONNECT TO https://app.dremio.cloud/repositories/&lt;project-id&gt;/api/v2</code> to connect to your Dremio cloud instance using Nessie\u2019s native REST API.</p> <p>See <code>CONNECT</code> statement below.</p>"},{"location":"nessie-latest/cli/#command-line-options","title":"Command line options","text":"<pre><code>Usage: nessie-cli.jar [-hHqV] [--no-up-to-date-check] [--non-ansi] [-j=&lt;historyFile&gt;] [[-K] [-E]\n                      [-s=&lt;scriptFile&gt; | -c[=&lt;commands&gt;...] [-c[=&lt;commands&gt;...]]...]] [-u=&lt;uri&gt;\n                      [--client-name=&lt;clientName&gt;] [-o[=&lt;String=String&gt;[,\n                      &lt;String=String&gt;...]...]]... [-r=&lt;initialReference&gt;]]\n\nThe Nessie CLI\nSee https://projectnessie.org/nessie-latest/cli/ for documentation.\n\n  -h, --help                Show this help message and exit.\n  -H, --[no-]history        Allows disabling the command history file in the REPL.\n                            Default is to save the command history.\n  -j, --history-file=&lt;historyFile&gt;\n                            Specify an alternative history file for the REPL.\n                              Default: ~/.nessie/nessie-cli.history\n      --no-up-to-date-check Optionally disable the up-to-date check.\n                            Only effective if --quiet is not specified.\n      --non-ansi            Allows disabling the (default) ANSI mode. Disabling ANSI support can be\n                              useful in non-interactive scripts.\n  -q, --quiet               Quiet option - omit the welcome and exit output.\n  -V, --version             Print version information and exit.\n\nStatements to execute before or without running the REPL\n========================================================\n\n  -c, --command[=&lt;commands&gt;...]\n                            Nessie CLI commands to run. Each value represents one command.\n                            The process will exit once all specified commands have been executed.\n                              To keep the REPL running in case of errors, specify the\n                              --keep-running option.\n  -E, --continue-on-error   When running commands via the --command or --run-script option the\n                              process will stop/exit when a command could not be parsed or ran into\n                              an error.\n                            Specifying this option lets the REPL continue executing the remaining\n                              commands after parse or runtime errors.\n  -K, --keep-running        When running commands via the --command or --run-script option the\n                              process will exit once the commands have been executed.\n                            To keep the REPL running, specify this option.See the\n                              --continue-on-error option.\n  -s, --run-script=&lt;scriptFile&gt;\n                            Run the commands in the Nessie CLI script referenced by this option.\n                            Possible values are either a file path or use the minus character ('-')\n                              to read the script from stdin.\n\nConnect options\n===============\n\n      --client-name=&lt;clientName&gt;\n                            Name of the client implementation to use, defaults to HTTP suitable for\n                              Nessie REST API.\n                            See https://projectnessie.org/nessie-latest/client_config/ for the\n                              'nessie.client-builder-name' option.\n  -o, --client-option[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                            Parameters to configure the REST client.\n                            See https://projectnessie.org/nessie-latest/client_config/\n  -r, --initial-reference=&lt;initialReference&gt;\n                            Name of the Nessie reference to use.\n  -u, --uri=&lt;uri&gt;           REST API endpoint URI to connect to.\n                            See 'HELP CONNECT' in the REPL.\n</code></pre>"},{"location":"nessie-latest/cli/#repl-commands","title":"REPL Commands","text":"<p>The following syntax descriptions illustrate how commands are used and the order of the clauses.</p> <p>Info</p> <p><code>CODE</code> style means the term is a keyword.</p> <p>BoldTerms mean variable input, see Descripton of Command Parts below</p> <p>Square brackets <code>[</code> <code>]</code> mean that the contents are optional (0 or 1 occurrence).</p> <p>Curly brackets <code>{</code> <code>}</code> mean that the contents can be repeated 0 or more times.</p>"},{"location":"nessie-latest/cli/#connect","title":"<code>CONNECT</code>","text":"<p><code>CONNECT</code> <code>TO</code> Uri        [ <code>ON</code> ReferenceName ]        [ <code>USING</code> ParamKey <code>=</code> Value { <code>AND</code> ParamKey <code>=</code> Value } ]</p> <p>Connect to a Nessie repository using the Nessie REST API base URI specified via <code>Uri</code>.</p> <p>When using Nessie with Iceberg REST, it is recommended to use the Iceberg REST base URI instead of the Nessie REST base URI.</p> <p>Nessie client options can be specified using the <code>ParamKey = Value</code> pairs, see https://projectnessie.org/nessie-latest/client_config/,</p> <p>If the Nessie CLI is already connected to a Nessie repository, that connection will be closed.</p> <p>Tip: If your connect string contains sensitive information, like a password or an access token, put a space before the <code>CONNECT</code> statement, which instructs the Nessie CLI to not record the statement in the history file.</p> <p>Examples:</p> <ul> <li>Local Nessie with Iceberg REST, using the default branch:   <code>CONNECT TO http://127.0.0.1:19120/iceberg/</code> for a locally running Nessie instance with   Iceberg REST (on your machine) without authentication enabled.</li> <li>Nessie with Iceberg REST, using another branch:   <code>CONNECT TO http://127.0.0.1:19120/iceberg ON myBranch</code> for a locally running Nessie   instance with Iceberg REST (on your machine) without authentication enabled, using <code>myBranch</code>   as the initial reference name. Alternatively, you can add the initial reference name to   the URI like this: <code>CONNECT TO http://127.0.0.1:19120/iceberg/myBranch</code>.</li> <li>Local Nessie:   <code>CONNECT TO http://127.0.0.1:19120/api/v2</code> for a locally running Nessie instance (on your   machine) without authentication enabled.</li> <li>Local Nessie:   <code>CONNECT TO http://127.0.0.1:19120/api/v2 ON myBranch</code> for a locally running Nessie instance   (on your machine) without authentication enabled, but using <code>myBranch</code> as the initial reference.</li> <li>Dremio Cloud:   <code>CONNECT to https://app.dremio.cloud/repositories/beeff00d-1122-1234-4242-feedcafebabe/api/v2     USING \"nessie.authentication.type\" = BEARER     AND \"nessie.authentication.token\" = \"your-personal-bearer-token\"</code>   to connect to Dremio Cloud. (The statement must not contain line breaks.)   Replace <code>beeff00d-1122-1234-4242-feedcafebabe</code> with your project ID. The complete endpoint   URI of your Dremio Cloud catalog can be found in the catalog settings   as Catalog Endpoint under General Information.   Also replace <code>your-personal-bearer-token</code> with a personal token, which can be created in   your account settings under Personal Access Tokens. Do never share a personal access token!</li> </ul> <p>When using \u201cinteractive\u201d OAuth2 device-code or authorization-code flows, you can abort the login by pressing Ctrl-C.</p>"},{"location":"nessie-latest/cli/#create-branch-tag","title":"<code>CREATE BRANCH</code> / <code>TAG</code>","text":"<p><code>CREATE</code> ReferenceType        [ <code>IF</code> <code>NOT</code> <code>EXISTS</code> ]  ReferenceName        [ <code>FROM</code> ExistingReference ]        [ <code>AT</code> [ <code>TIMESTAMP</code> | <code>COMMIT</code> ] TimestampOrCommit ]</p> <p>Creates a new Nessie branch or tag using the name specified using the <code>ReferenceName</code> parameter. The reference type is specified using the <code>ReferenceType</code> parameter.</p> <p>By default, the new branch or tag is created from the latest commit on the current reference of the Nessie CLI (see <code>USE</code> statement). Another source reference name can use specified using the <code>FROM</code> clause. The optional <code>AT</code> clause allows specifying a different commit ID (hash) to create the new reference from.</p> <p>This command will fail, if a references with the name <code>ReferenceName</code> already exists, unless the optional <code>IF NOT EXISTS</code> is specified.</p>"},{"location":"nessie-latest/cli/#create-namespace","title":"<code>CREATE NAMESPACE</code>","text":"<p><code>CREATE</code> <code>NAMESPACE</code> ContentKey        [ <code>ON</code> [ ReferenceType ] ExistingReference ]        [ <code>SET</code> ParamKey <code>=</code> Value { <code>AND</code> ParamKey <code>=</code> Value } ]</p> <p>Creates a new namespace in the current branch, or in the branch specified using the <code>IN</code> clause.</p> <p>Namespace properties can be set using the key/value pairs in the <code>SET</code> clause.</p>"},{"location":"nessie-latest/cli/#drop-branch-tag","title":"<code>DROP BRANCH</code> / <code>TAG</code>","text":"<p><code>DROP</code> ReferenceType [ <code>IF</code> <code>EXISTS</code> ] ExistingReference</p> <p>Drops a new Nessie branch or tag using the name specified using the <code>ReferenceName</code> parameter. The reference type is specified using the <code>ReferenceType</code> parameter.</p> <p>This command will fail, if a references with the name <code>ReferenceName</code> does not exist, unless the optional <code>IF EXISTS</code> is specified.</p>"},{"location":"nessie-latest/cli/#drop-table-view-namespace","title":"<code>DROP TABLE</code> / <code>VIEW</code> / <code>NAMESPACE</code>","text":"<p><code>DROP</code> ContentKind ContentKey        [ <code>ON</code> [ ReferenceType ] ExistingReference ]</p> <p>Drops a table or view in the current branch, or in the branch specified using the <code>IN</code> clause.</p> <p>Note that dropping namespaces must be performed using the <code>DROP NAMESPACE</code> command.</p>"},{"location":"nessie-latest/cli/#assign-branch-tag","title":"<code>ASSIGN BRANCH</code> / <code>TAG</code>","text":"<p><code>ASSIGN</code> ReferenceType        [ ExistingReference ]        [ <code>TO</code> ExistingReference [ <code>AT</code> [ <code>TIMESTAMP</code> | <code>COMMIT</code> ] TimestampOrCommit ] ]</p> <p>Assigns a Nessie branch or tag using the name specified using the <code>ReferenceName</code> parameter to another commit. The reference type is specified using the <code>ReferenceType</code> parameter.</p> <p>By default, the branch or tag is updated to the latest commit on the current reference of the Nessie CLI (see <code>USE</code> statement). Another target reference name can use specified using the <code>TO</code> clause. The optional <code>AT</code> clause allows specifying a different commit ID (hash) to assign the reference to.</p>"},{"location":"nessie-latest/cli/#alter-namespace","title":"<code>ALTER NAMESPACE</code>","text":"<p><code>ALTER</code> <code>NAMESPACE</code> ContentKey        [ <code>ON</code> [ ReferenceType ] ExistingReference ]        [ <code>SET</code> ParamKey <code>=</code> Value { <code>AND</code> ParamKey <code>=</code> Value } ]        [ <code>REMOVE</code> ParamKey { <code>AND</code> ParamKey } ]</p> <p>Updates a namespace in the current branch, or in the branch specified using the <code>IN</code> clause.</p> <p>New namespace properties can be set using the key/value pairs in the <code>SET</code> clause.</p> <p>Existing namespace properties can be updated using the key/value pairs in the <code>SET</code> clause.</p> <p>Existing namespace properties can be removed using the keys specified in the <code>REMOVE</code> clause.</p>"},{"location":"nessie-latest/cli/#list-contents","title":"<code>LIST CONTENTS</code>","text":"<p><code>LIST</code> <code>CONTENTS</code>        [ <code>ON</code> [ ReferenceType ] ExistingReference ]        [ <code>AT</code> [ <code>TIMESTAMP</code> | <code>COMMIT</code> ] TimestampOrCommit ]        [ <code>FILTER</code> Value       | [ <code>STARTING</code> <code>WITH</code> Value ] [ <code>CONTAINING</code> Value ]       ]</p> <p>Lists all tables, views and namespaces either in the current reference of the Nessie CLI, or in the branch or tab specified using the <code>IN</code> clause. By default entities on the latest commit of the branch or tag will be listed, which can be overridden using the <code>AT</code> clause.</p> <p>An optional CEL-filter can be specified, which is evaluated on the server side.</p> <p>The optional <code>STARTING WITH</code> clause starts the output at the content-key with the given value.</p> <p>The optional <code>CONTAINING</code> clause only outputs entities with a content-key that contain the given value.</p>"},{"location":"nessie-latest/cli/#list-references","title":"<code>LIST REFERENCES</code>","text":"<p><code>LIST</code> <code>REFERENCES</code>        [ <code>FILTER</code> Value       | [ <code>STARTING</code> <code>WITH</code> Value ] [ <code>CONTAINING</code> Value ]       ]</p> <p>Lists all named references.</p> <p>An optional CEL-filter can be specified, which is evaluated on the server side.</p> <p>The optional <code>STARTING WITH</code> clause starts the output at the content-key with the given value.</p> <p>The optional <code>CONTAINING</code> clause only outputs entities with a content-key that contain the given value.</p>"},{"location":"nessie-latest/cli/#merge-branch","title":"<code>MERGE BRANCH</code>","text":"<p><code>MERGE</code>        [ <code>DRY</code> ]        [ ReferenceType ]  ExistingReference        [ <code>AT</code> [ <code>TIMESTAMP</code> | <code>COMMIT</code> ] TimestampOrCommit ]        [ <code>INTO</code> ExistingReference ]        [ <code>BEHAVIOR</code> MergeBehaviorKind ]        [ <code>BEHAVIORS</code> ContentKey <code>=</code> MergeBehaviorKind { <code>AND</code> ContentKey <code>=</code> MergeBehaviorKind } ]</p> <p>Merges a branch or tag into another branch, supporting manual conflict resolution.</p> <p>The optional <code>DRY</code> keyword defines that Nessie shall simulate a merge operation. This is useful to check whether a merge operation would succeed.</p> <p>Specifying the name of the \u201cfrom\u201d reference is mandatory. By default, the latest commit of the \u201cfrom\u201d branch or tag will be merged, which can be overridden using the <code>AT</code> clause.</p> <p>By default, <code>MERGE</code> uses the CLI\u2019s current reference as the target branch. The <code>INTO</code> clause can be used to specify another target branch.</p> <p>Nessie merge operations currently support three different merge behaviors:</p> <ul> <li><code>NORMAL</code>: a merge succeeds, if the content does not have a conflicting change in the target branch.</li> <li><code>FORCE</code>: a merge always succeeds, the content from the \u201cfrom\u201d reference will be applied onto the target branch.</li> <li><code>DROP</code>: like <code>NORMAL</code>, but does not cause a conflict, so does not fail the whole merge operation.</li> </ul> <p>The merge behavior for all contents defaults to <code>NORMAL</code> and can be changed using the <code>BEHAVIOR</code> clause.</p> <p>Specific merge behaviors can be specified using the <code>BEHAVIORS</code> clause for individual content keys.</p>"},{"location":"nessie-latest/cli/#revert-content","title":"<code>REVERT CONTENT</code>","text":"<p><code>REVERT</code> <code>CONTENT</code>        [ <code>DRY</code> ]  <code>OF</code> ContentKey        { <code>AND</code> ContentKey }        [ <code>ON</code> [ ReferenceType ] ExistingReference ]  <code>TO</code> <code>STATE</code>        ( <code>ON</code> [ ReferenceType ] ExistingReference [ <code>AT</code> [ <code>TIMESTAMP</code> | <code>COMMIT</code> ] TimestampOrCommit ]       | <code>AT</code> [ <code>TIMESTAMP</code> | <code>COMMIT</code> ] TimestampOrCommit       )        [ <code>ALLOW</code> <code>DELETES</code> ]</p> <p>Creates a new commit in Nessie to set the state of the given content-keys to the same state as those content keys existed on the <code>TO STATE</code> reference.</p> <p>This command is for example useful when another system has created a Nessie commit but the referenced Iceberg metadata object was not written.</p> <p>It is recommended to use the <code>DRY</code> keyword to investigate the changes before actually committing those to Nessie.</p> <p>Contents that exist on the <code>TO STATE</code> are updated in the new Nessie commit to that state. Contents that do not exist on the <code>TO STATE</code> are deleted only if the <code>ALLOW DELETES</code> clause is specified, otherwise the command will fail.</p> <p>Note that this command will always create a new commit, even if the actual contents at the \u201ctip\u201d of the target branch and the <code>TO STATE</code> are equal.</p>"},{"location":"nessie-latest/cli/#show-log","title":"<code>SHOW LOG</code>","text":"<p><code>SHOW</code> <code>LOG</code>        [ [ <code>ON</code> [ ReferenceType ] ] ExistingReference ]        [ <code>AT</code> [ <code>TIMESTAMP</code> | <code>COMMIT</code> ] TimestampOrCommit ]        [ <code>LIMIT</code> PositiveInt ]</p> <p>Shows the Nessie commit log.</p> <p>By default, the commit log fetched for the current reference of the Nessie CLI, or in the branch or tab specified using the <code>IN</code> clause. By default entities on the latest commit of the branch or tag will be listed, which can be overridden using the <code>AT</code> clause.</p> <p>The output can be limited using the <code>LIMIT</code> clause. It is safe to omit the <code>LIMIT</code> clause for ANSI terminals, because the commit log will be safely paged with neither overloading the Nessie CLI or Nessie server.</p>"},{"location":"nessie-latest/cli/#show-table-view-namespace","title":"<code>SHOW TABLE</code> / <code>VIEW</code> / <code>NAMESPACE</code>","text":"<p><code>SHOW</code> ContentKind        [ <code>ON</code> [ ReferenceType ] ExistingReference ]        [ <code>AT</code> [ <code>TIMESTAMP</code> | <code>COMMIT</code> ] TimestampOrCommit ]  ContentKey</p> <p>Shows the content of a table, view or namespaces that is stored in Nessie.</p> <p>By default, the content will be looked up in the current reference of the Nessie CLI, or in the branch or tab specified using the <code>IN</code> clause. By default entities on the latest commit of the branch or tag will be listed, which can be overridden using the <code>AT</code> clause.</p>"},{"location":"nessie-latest/cli/#show-reference","title":"<code>SHOW REFERENCE</code>","text":"<p><code>SHOW</code> <code>REFERENCE</code>        [ ExistingReference ]        [ <code>AT</code> [ <code>TIMESTAMP</code> | <code>COMMIT</code> ] TimestampOrCommit ]</p> <p>Shows information about the current or given reference.</p> <p>If no reference is specified, information about the current reference of the Nessie CLI is shown, otherwise information about the given reference. By default, entities information of latest commit of the branch or tag will be shown, which can be overridden using the <code>AT</code> clause.</p>"},{"location":"nessie-latest/cli/#use","title":"<code>USE</code>","text":"<p><code>USE</code>        ( ReferenceType       | <code>REFERENCE</code>       )  ExistingReference        [ <code>AT</code> [ <code>TIMESTAMP</code> | <code>COMMIT</code> ] TimestampOrCommit ]</p> <p>Changes the current reference in the Nessie CLI to the branch or tag name in <code>ExistingReference</code>.</p>"},{"location":"nessie-latest/cli/#help","title":"<code>HELP</code>","text":"<p><code>HELP</code>        [ <code>USE</code>       | <code>CONNECT</code>       | <code>CREATE</code> [ ( <code>BRANCH</code> | <code>TAG</code> | <code>NAMESPACE</code> ) ]       | <code>ALTER</code> [ <code>NAMESPACE</code> ]       | <code>DROP</code> [ <code>BRANCH</code> | <code>TAG</code> | <code>NAMESPACE</code> | <code>TABLE</code> | <code>VIEW</code> ]       | <code>LIST</code> [ <code>CONTENTS</code> | <code>REFERENCES</code> ]       | <code>SHOW</code> [ <code>LOG</code> | <code>TABLE</code> | <code>VIEW</code> | <code>NAMESPACE</code> | <code>REFERENCE</code> ]       | <code>ASSIGN</code> [ <code>BRANCH</code> | <code>TAG</code> ]       | <code>MERGE</code>       | <code>REVERT</code>       | <code>HELP</code>       | <code>EXIT</code>       | <code>LICENSE</code>       ]</p> <p>Shows available commands or detailed information about a specific command.</p> <p><code>HELP</code> shows all available command.</p> <p><code>HELP</code> plus another command shows information about that specific command. Some commands require multiple keywords, for example <code>HELP CREATE</code> and <code>HELP CREATE BRANCH</code> or <code>HELP CREATE NAMESPACE</code>.</p>"},{"location":"nessie-latest/cli/#exit","title":"<code>EXIT</code>","text":"<p><code>EXIT</code></p> <p>Exits the Nessie CLI.</p>"},{"location":"nessie-latest/cli/#command-parts","title":"Command parts","text":""},{"location":"nessie-latest/cli/#referencetype","title":"ReferenceType","text":"<p><code>BRANCH</code> | <code>TAG</code></p>"},{"location":"nessie-latest/cli/#contentkind","title":"ContentKind","text":"<p><code>TABLE</code> | <code>VIEW</code> | <code>NAMESPACE</code></p>"},{"location":"nessie-latest/cli/#existingreference","title":"ExistingReference","text":"<p>Name of an existing reference in Nessie.</p>"},{"location":"nessie-latest/cli/#referencename","title":"ReferenceName","text":"<p>Nessie reference name.</p>"},{"location":"nessie-latest/cli/#timestamporcommit","title":"TimestampOrCommit","text":"<p>Either a Nessie commit ID (hash) or a timestamp in ISO format. Examples:</p> <ul> <li><code>2024-04-26T10:31:05.277650575Z</code> is a valid ISO timestamp</li> <li><code>fa32a50d5303a53826f65649277561f5c6772eba019e7f1e01a359becb764877</code> is a valid Nessie commit ID (hash)</li> </ul>"},{"location":"nessie-latest/client_config/","title":"Client Configuration","text":"<p>When Nessie is integrated into a broader data processing environment, authentication settings need to be provided in a way specific to the tool used.</p>"},{"location":"nessie-latest/client_config/#nessie-client-configuration-options","title":"Nessie client configuration options","text":"<p>See also Authentication Settings below.</p>"},{"location":"nessie-latest/client_config/#common-settings","title":"Common settings","text":"Property Description <code>nessie.uri</code> Config property name (\u201cnessie.uri\u201d) for the Nessie service URL. <code>nessie.authentication.type</code> ID of the authentication provider to use, default is no authentication. Valid values are <code>BASIC</code>, <code>BEARER</code>, <code>OAUTH2</code> and <code>AWS</code>.   The value is matched against the values returned as the supported auth-type by  implementations of (<code>NessieAuthenticationProvider</code>) across all available authentication  providers.   Note that \u201cbasic\u201d HTTP authentication is not considered secure, use <code>BEARER</code> instead. <code>nessie.ref</code> Name of the initial Nessie reference, usually <code>main</code>. <code>nessie.ref.hash</code> Commit ID (hash) on \u201cnessie.ref\u201d, usually not specified. <code>nessie.tracing</code> Enable adding the HTTP headers of an active OpenTracing span to all Nessie requests. Disabled  by default. <code>nessie.client-builder-name</code> Name of the Nessie client to use. If not specified, the implementation prefers the new Java  HTTP client ( <code>JavaHttp</code>), if running on Java 11 or newer, or the Java <code>URLConnection</code> client. The Apache HTTP client ( <code>ApacheHttp</code>) can be used, if it has been  made available on the classpath. <code>nessie.client-builder-impl</code> Similar to \u201cnessie.client-builder-name\u201d, but uses a class name. Deprecated Prefer using Nessie client implementation names, configured via \u201cnessie.client-builder-name\u201d. <code>nessie.enable-api-compatibility-check</code> Enables API compatibility check when creating the Nessie client. The default is <code>true</code>.   You can also control this setting by setting the system property <code>nessie.client.enable-api-compatibility-check</code> to <code>true</code> or <code>false</code>. <code>nessie.client-api-version</code> Explicitly specify the Nessie API version number to use. The default for this setting depends  on the client being used. <code>nessie.commit-meta.message</code> Override all commit messages with the configured value. The corresponding HTTP header is <code>Nessie-Commit-Message</code> . <code>nessie.commit-meta.authors</code> Set/override the author of all commits. The corresponding HTTP header is <code>Nessie-Commit-Authors</code> . Multiple authors can be specified, separated with <code>,</code>. <code>nessie.commit-meta.signed-off-by</code> Set/override the signed-off-by of all commits. The corresponding HTTP header is <code>Nessie-Commit-SignedOffBy</code> . Multiple signed-off-by can be specified, separated with <code>,</code>."},{"location":"nessie-latest/client_config/#network-settings","title":"Network settings","text":"Property Description <code>nessie.transport.read-timeout</code> Network level read timeout in milliseconds. When running with Java 11, this becomes a request  timeout. Default is 25000 ms. <code>nessie.transport.connect-timeout</code> Network level connect timeout in milliseconds, default is 5000. <code>nessie.transport.disable-compression</code> Config property name (\u201cnessie.transport.disable-compression\u201d) to disable compression on the  network layer, if set to <code>true</code>. <code>nessie.ssl.no-certificate-verification</code> Optional, disables certificate verifications, if set to <code>true</code>. Can be useful for testing  purposes, not recommended for production systems. <code>nessie.ssl.cipher-suites</code> Optional, list of comma-separated cipher suites for SSL connections. This parameter only works on Java 11 and newer with the Java HTTP client. <code>nessie.ssl.protocols</code> Optional, list of comma-separated protocols for SSL connections. This parameter only works on Java 11 and newer with the Java HTTP client. <code>nessie.ssl.sni-hosts</code> Optional, comma-separated list of SNI host names for SSL connections. This parameter only works on Java 11 and newer with the Java HTTP client. <code>nessie.ssl.sni-matcher</code> Optional, a single SNI matcher for SSL connections. Takes a single SNI hostname matcher, a regular expression representing the SNI  hostnames to match.   This parameter only works on Java 11 and newer with the Java HTTP client."},{"location":"nessie-latest/client_config/#http-settings","title":"HTTP settings","text":"Property Description <code>nessie.http2-upgrade</code> Optional, allow HTTP/2 upgrade, if set to <code>true</code>. This parameter only works on Java 11 and newer with the Java HTTP client. <code>nessie.http-redirects</code> Optional, specify how redirects are handled.  * <code>NEVER</code>: Never redirect.     * <code>ALWAYS</code>: Always redirect.     * <code>NORMAL</code>: Always redirect, except from HTTPS URLs to HTTP URLs.  This parameter only works on Java 11 and newer with the Java HTTP client."},{"location":"nessie-latest/client_config/#bearer-authentication-settings","title":"Bearer authentication settings","text":"<p>See also Authentication Settings below.</p> Property Description <code>nessie.authentication.token</code> Token used for <code>BEARER</code> authentication."},{"location":"nessie-latest/client_config/#oauth2-settings","title":"OAuth2 settings","text":"<p>General OAuth2 settings. See also Authentication Settings below.</p> Property Description <code>nessie.authentication.oauth2.issuer-url</code> OAuth2 issuer URL. The root URL of the OpenID Connect identity issuer provider, which will be used for  discovering supported endpoints and their locations. For Keycloak, this is typically the realm  URL: <code>https://&lt;keycloak-server&gt;/realms/&lt;realm-name&gt;</code>.   Endpoint discovery is performed using the OpenID Connect Discovery metadata published by the  issuer. See OpenID Connect  Discovery 1.0  for more information.   Either this property or (<code>nessie.authentication.oauth2.token-endpoint</code>) must be set. <code>nessie.authentication.oauth2.token-endpoint</code> URL of the OAuth2 token endpoint. For Keycloak, this is typically <code>https://&lt;keycloak-server&gt;/realms/&lt;realm-name&gt;/protocol/openid-connect/token</code> .   Either this property or (<code>nessie.authentication.oauth2.issuer-url</code>) must be set. In case it is  not set, the token endpoint will be discovered from the issuer URL (<code>nessie.authentication.oauth2.issuer-url</code>), using the OpenID Connect Discovery metadata published by the issuer. <code>nessie.authentication.oauth2.grant-type</code> The grant type to use when authenticating against the OAuth2 server. Valid values are:    * \u201cclient_credentials\u201d  * \u201cpassword\u201d  * \u201cauthorization_code\u201d  * \u201cdevice_code\u201d  * \u201ctoken_exchange\u201d Optional, defaults to \u201cclient_credentials\u201d. <code>nessie.authentication.oauth2.client-id</code> Client ID to use when authenticating against the OAuth2 server. Required if using OAuth2  authentication, ignored otherwise. <code>nessie.authentication.oauth2.client-secret</code> Client secret to use when authenticating against the OAuth2 server. Required if using OAuth2  authentication, ignored otherwise. <code>nessie.authentication.oauth2.extra-params</code> Extra parameters to include in each request to the token endpoint. This is useful for custom  parameters that are not covered by the standard OAuth2.0 specification. Optional, defaults to  empty.   The format of this field is a comma-separated list of key-value pairs, separated by an equal  sign. The values must NOT be URL-encoded. Example:   <code>nessie.authentication.oauth2.extra-params = \"custom_param1=custom_value1,custom_param2=custom_value2\"</code> For example, Auth0 requires the <code>audience</code> parameter to be set to the API identifier.  This can be done by setting the following configuration:   <code>nessie.authentication.oauth2.extra-params = \"audience=https://nessie-catalog/api\"</code> <code>nessie.authentication.oauth2.client-scopes</code> Space-separated list of scopes to include in each request to the OAuth2 server. Optional,  defaults to empty (no scopes).   The scope names will not be validated by the Nessie client; make sure they are valid  according to RFC 6749  Section 3.3 ."},{"location":"nessie-latest/client_config/#oauth2-resource-owner-password-credentials-settings","title":"OAuth2 Resource Owner Password Credentials settings","text":"<p>OAuth2 settings relevant when using the <code>password</code> grant type. See below for details.</p> Property Description <code>nessie.authentication.oauth2.username</code> Username to use when authenticating against the OAuth2 server. Required if using OAuth2  authentication and \u201cpassword\u201d grant type, ignored otherwise. <code>nessie.authentication.oauth2.password</code> Password to use when authenticating against the OAuth2 server. Required if using OAuth2  authentication and the \u201cpassword\u201d grant type, ignored otherwise."},{"location":"nessie-latest/client_config/#oauth2-authorization-code-grant-settings","title":"OAuth2 Authorization Code Grant settings","text":"<p>OAuth2 settings relevant when using the <code>authorization_code</code> grant type. See below for details.</p> Property Description <code>nessie.authentication.oauth2.auth-endpoint</code> URL of the OAuth2 authorization endpoint. For Keycloak, this is typically <code>https://&lt;keycloak-server&gt;/realms/&lt;realm-name&gt;/protocol/openid-connect/auth</code> .   If using the \u201cauthorization_code\u201d grant type, either this property or (<code>nessie.authentication.oauth2.issuer-url</code>) must be set. In case it is not set, the authorization endpoint  will be discovered from the issuer URL (<code>nessie.authentication.oauth2.issuer-url</code>), using the OpenID  Connect Discovery metadata published by the issuer. <code>nessie.authentication.oauth2.auth-code-flow.web-port</code> Port of the OAuth2 authorization code flow web server. When running a client inside a container make sure to specify a port and forward the port to  the container host.   The port used for the internal web server that listens for the authorization code callback.  This is only used if the grant type to use is \u201cauthorization_code\u201d.   Optional; if not present, a random port will be used. <code>nessie.authentication.oauth2.auth-code-flow.timeout</code> Defines how long the client should wait for the authorization code flow to complete. This is  only used if the grant type to use is \u201cauthorization_code\u201d. Optional, defaults to \u201cPT5M\u201d."},{"location":"nessie-latest/client_config/#oauth2-device-authorization-grant-settings","title":"OAuth2 Device Authorization Grant settings","text":"<p>OAuth2 settings relevant when using the <code>device_code</code> grant type. See below for details.</p> Property Description <code>nessie.authentication.oauth2.device-auth-endpoint</code> URL of the OAuth2 device authorization endpoint. For Keycloak, this is typically <code>http://&lt;keycloak-server&gt;/realms/&lt;realm-name&gt;/protocol/openid-connect/auth/device</code> .   If using the \u201cDevice Code\u201d grant type, either this property or (<code>nessie.authentication.oauth2.issuer-url</code>) must be set. <code>nessie.authentication.oauth2.device-code-flow.timeout</code> Defines how long the client should wait for the device code flow to complete. This is only used  if the grant type to use is \u201cdevice_code\u201d. Optional,  defaults to \u201cPT5M\u201d. <code>nessie.authentication.oauth2.device-code-flow.poll-interval</code> Defines how often the client should poll the OAuth2 server for the device code flow to  complete.  This is only used if the grant type to use is \u201cdevice_code\u201d. Optional, defaults to \u201cPT5S\u201d."},{"location":"nessie-latest/client_config/#oauth2-token-exchange-grant-settings","title":"OAuth2 Token Exchange Grant settings","text":"<p>OAuth2 settings relevant when using the <code>token_exchange</code> grant type. See below for details.</p> <p>Warning</p> <p>The feature is experimental and subject to change.</p> Property Description <code>nessie.authentication.oauth2.token-exchange.resource</code> For token exchanges only. A URI that indicates the target service or resource where the client  intends to use the requested security token. Optional. <code>nessie.authentication.oauth2.token-exchange.audience</code> For token exchanges only. The logical name of the target service where the client intends to  use the requested security token. This serves a purpose similar to the resource parameter but  with the client providing a logical name for the target service. <code>nessie.authentication.oauth2.token-exchange.subject-token</code> For token exchanges only. The subject token to exchange. This can take 3 kinds of values:    * The value \u201ccurrent_access_token\u201d, if the client should use its current access        token;     * The value \u201ccurrent_refresh_token\u201d, if the client should use its current refresh        token (a refresh token must be available in this case);     * An arbitrary token: in this case, the client will always use the static token provided        here.  The default is to use the current access token. Note: when using token exchange as the initial  grant type, no current access token will be available: in this case, a valid, static subject  token to exchange must be provided via configuration. <code>nessie.authentication.oauth2.token-exchange.subject-token-type</code> For token exchanges only. The type of the subject token. Must be a valid URN. The default is  either <code>urn:ietf:params:oauth:token-type:access_token</code> or <code>urn:ietf:params:oauth:token-type:refresh_token</code> , depending on the value of \u201cnessie.authentication.oauth2.token-exchange.subject-token\u201d.   If the client is configured to use its access or refresh token as the subject token, please  note that if an incorrect token type is provided here, the token exchange could fail. <code>nessie.authentication.oauth2.token-exchange.actor-token</code> For token exchanges only. The actor token to exchange. This can take 4 kinds of values:    * The value \u201cno_token\u201d, if the client should not include any actor token in the        exchange request;     * The value \u201ccurrent_access_token\u201d, if the client should use its current access        token;     * The value \u201ccurrent_refresh_token\u201d, if the client should use its current refresh        token (if available);     * An arbitrary token: in this case, the client will always use the static token provided        here.  The default is to not include any actor token. <code>nessie.authentication.oauth2.token-exchange.actor-token-type</code> For token exchanges only. The type of the actor token. Must be a valid URN. The default is  either <code>urn:ietf:params:oauth:token-type:access_token</code> or <code>urn:ietf:params:oauth:token-type:refresh_token</code> , depending on the value of \u201cnessie.authentication.oauth2.token-exchange.actor-token\u201d.   If the client is configured to use its access or refresh token as the actor token, please  note that if an incorrect token type is provided here, the token exchange could fail."},{"location":"nessie-latest/client_config/#oauth2-impersonation-settings","title":"OAuth2 impersonation settings","text":"<p>OAuth2 settings relevant when using impersonation. See below for details.</p> <p>Warning</p> <p>The feature is experimental and subject to change.</p> Property Description <code>nessie.authentication.oauth2.impersonation.enabled</code> Whether to enable \u201cimpersonation\u201d mode. If enabled, each access token obtained from the OAuth2  server using the configured initial grant type will be exchanged for a new token, using the  token exchange grant type. <code>nessie.authentication.oauth2.impersonation.issuer-url</code> For impersonation only. The root URL of an alternate OpenID Connect identity issuer provider,  to use when exchanging tokens only.   If neither this property nor \u201cnessie.authentication.oauth2.impersonation.token-endpoint\u201d are  defined, the global token endpoint will be used. This means that the same authorization server  will be used for both the initial token request and the token exchange.   Endpoint discovery is performed using the OpenID Connect Discovery metadata published by the  issuer. See OpenID Connect  Discovery 1.0  for more information. <code>nessie.authentication.oauth2.impersonation.token-endpoint</code> For impersonation only. The URL of an alternate OAuth2 token endpoint to use when exchanging  tokens only.   If neither this property nor \u201cnessie.authentication.oauth2.impersonation.issuer-url\u201d are  defined, the global token endpoint will be used. This means that the same authorization server  will be used for both the initial token request and the token exchange. <code>nessie.authentication.oauth2.impersonation.client-id</code> For impersonation only. An alternate client ID to use. If not provided, the global client ID  will be used. If provided, and if the client is confidential, then its secret must be provided  as well with \u201cnessie.authentication.oauth2.impersonation.client-secret\u201d \u2013 the global client  secret will NOT be used. <code>nessie.authentication.oauth2.impersonation.client-secret</code> For impersonation only. The client secret to use, if \u201cnessie.authentication.oauth2.impersonation.client-id\u201d is defined and the token exchange client is  confidential. <code>nessie.authentication.oauth2.impersonation.scopes</code> For impersonation only. Space-separated list of scopes to include in each token exchange  request to the OAuth2 server. Optional. If undefined, the global scopes configured through  \u201cnessie.authentication.oauth2.client-scopes\u201d will be used. If defined and null or empty, no  scopes will be used.   The scope names will not be validated by the Nessie client; make sure they are valid  according to RFC 6749  Section 3.3 ."},{"location":"nessie-latest/client_config/#oauth2-token-refresh-settings","title":"OAuth2 token refresh settings","text":"<p>OAuth2 settings related to token refreshes. You should rarely need to change the defaults.</p> Property Description <code>nessie.authentication.oauth2.default-access-token-lifespan</code> Default access token lifespan; if the OAuth2 server returns an access token without specifying  its expiration time, this value will be used.  Optional, defaults to \u201cPT1M\u201d. Must be a valid ISO-8601 duration. <code>nessie.authentication.oauth2.default-refresh-token-lifespan</code> Default refresh token lifespan. If the OAuth2 server returns a refresh token without specifying  its expiration time, this value will be used.   Optional, defaults to \u201cPT30M\u201d. Must be a valid ISO-8601 duration. <code>nessie.authentication.oauth2.refresh-safety-window</code> Refresh safety window to use; a new token will be fetched when the current token\u2019s remaining  lifespan is less than this value.  Optional, defaults to \u201cPT10S\u201d. Must be a valid ISO-8601 duration. <code>nessie.authentication.oauth2.preemptive-token-refresh-idle-timeout</code> Defines for how long the OAuth2 provider should keep the tokens fresh, if the client is not  being actively used.  Setting this value too high may cause an excessive usage of network I/O  and thread resources; conversely, when setting it too low, if the client is used again, the  calling thread may block if the tokens are expired and need to be renewed synchronously.  Optional, defaults to \u201cPT30S\u201d. Must be a valid  ISO-8601 duration. <code>nessie.authentication.oauth2.background-thread-idle-timeout</code> Defines how long the background thread should be kept running if the client is not being  actively used, or no token refreshes are being executed.  Optional, defaults to \u201cPT30S\u201d. Setting this value too high will cause the background  thread to keep running even if the client is not used anymore, potentially leaking thread and  memory resources; conversely, setting it too low could cause the background thread to be  restarted too often. Must be a valid ISO-8601 duration."},{"location":"nessie-latest/client_config/#aws-authentication-settings","title":"AWS authentication settings","text":"<p>Additional AWS authentication configuration should be provided via standard AWS configuration files.</p> <p>See also Authentication Settings below.</p> Property Description <code>nessie.authentication.aws.region</code> AWS region used for <code>AWS</code> authentication, must be configured to the same region as the  Nessie setver. <code>nessie.authentication.aws.profile</code> AWS profile name used for <code>AWS</code> authentication (optional)."},{"location":"nessie-latest/client_config/#basic-authentication-settings","title":"Basic authentication settings","text":"<p>See also Authentication Settings below.</p> Property Description <code>nessie.authentication.username</code> Username used for the insecure <code>BASIC</code> authentication.  Deprecated \u201cbasic\u201d HTTP authentication is not considered secure. Use (<code>nessie.authentication.token</code>) instead. <code>nessie.authentication.password</code> Password used for the insecure <code>BASIC</code> authentication.  Deprecated \u201cbasic\u201d HTTP authentication is not considered secure. Use (<code>nessie.authentication.token</code>) instead."},{"location":"nessie-latest/client_config/#java-11-connection-pool-options","title":"Java 11 connection pool options","text":"<p>The Java 11 HTTP client can be configured using Java system properties. Since Java\u2019s <code>HttpClient</code> API does not support the configuration of these properties programmatically, Nessie cannot expose those via its configuration mechanism.</p> System property Meaning <code>jdk.httpclient.connectionPoolSize</code> The size of the HTTP connection pool.Defaults to <code>0</code>, which means the number of connections is unlimited. <code>jdk.httpclient.keepalive.timeout</code> Number of seconds an idle HTTP connection will be kept alive. Defaults is <code>1200</code> seconds. <code>jdk.httpclient.receiveBufferSize</code> Size of the network level receive buffer size. Defaults to <code>0</code>, which means the operating system defaults apply. <code>jdk.httpclient.sendBufferSize</code> Size of the network level send buffer size. Defaults to <code>0</code>, which means the operating system defaults apply. <p>Note</p> <p>See Javadoc of <code>javax.net.ssl.SSLParameters</code> for valid options/values for the configuration parameters starting with <code>nessie.ssl.</code>.</p> <p>Note</p> <p>See Javadoc of <code>org.projectnessie.client.NessieConfigConstants</code> as well.</p> <p>Note</p> <p>In case you run into issues with Nessie\u2019s new HTTP client for Java 11 and newer, you can try to use the legacy <code>URLConnection</code> based HTTP client by setting the system property or configuration option <code>nessie.client-builder-name</code> to <code>URLConnection</code>.</p>"},{"location":"nessie-latest/client_config/#spark","title":"Spark","text":"<p>When Nessie is used in Spark-based environments (with Iceberg  the Nessie authentication settings are configured via Spark session properties (Replace <code>&lt;catalog_name&gt;</code> with the name of your catalog).</p> JavaPython <pre><code>// local spark instance, assuming NONE authentication\nconf.set(\"spark.sql.catalog.&lt;catalog_name&gt;\", \"org.apache.iceberg.spark.SparkCatalog\")\n    .set(\"spark.sql.catalog.&lt;catalog_name&gt;.authentication.type\", \"NONE\")\n    .set(...);\nspark = SparkSession.builder()\n                    .master(\"local[2]\")\n                    .config(conf)\n                    .getOrCreate();\n</code></pre> <pre><code># local spark instance, assuming NONE authentication\nspark = SparkSession.builder \\\n        .config(\"spark.sql.catalog.&lt;catalog_name&gt;\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n        .config(\"spark.sql.catalog.&lt;catalog_name&gt;.authentication.type\", \"NONE\") \\\n        .config(...) \n        .getOrCreate()\n</code></pre>"},{"location":"nessie-latest/client_config/#property-prefixes","title":"Property Prefixes","text":"<p>The <code>spark.sql.catalog.&lt;catalog_name&gt;</code> prefix identifies properties for the Nessie catalog. The <code>&lt;catalog_name&gt;</code> part is just the name of the catalog in this case (not to be confused with the Nessie project name).</p> <p>Multiple Nessie catalogs can be configured in the same Spark environment, each with its own set of configuration properties and its own property name prefix.</p>"},{"location":"nessie-latest/client_config/#flink","title":"Flink","text":"<p>When Nessie is used in Flink with Iceberg, the Nessie authentication settings are configured when creating the Nessie catalog in Flink (Replace <code>&lt;catalog_name&gt;</code> with the name of your catalog):</p> <pre><code>table_env.execute_sql(\n        \"\"\"CREATE CATALOG &lt;catalog_name&gt; WITH (\n        'type'='iceberg',\n        'catalog-impl'='org.apache.iceberg.nessie.NessieCatalog',\n        'authentication.type'='NONE')\"\"\")\n</code></pre>"},{"location":"nessie-latest/client_config/#authentication-settings","title":"Authentication Settings","text":"<p>The sections below discuss specific authentication settings. The property names are shown without environment-specific prefixes for brevity. Nonetheless, in practice the property names should be given appropriate prefixes (as in the examples above) for them to be recognized by the tools and Nessie code.</p> <p>The value of the <code>authentication.type</code> property can be one of the following:</p> <ul> <li><code>NONE</code> (default)</li> <li><code>BEARER</code></li> <li><code>OAUTH2</code></li> <li><code>AWS</code></li> </ul>"},{"location":"nessie-latest/client_config/#authentication-type-none","title":"Authentication Type <code>NONE</code>","text":"<p>For the Authentication Type <code>NONE</code> only the <code>authentication.type</code> property needs to be set.</p> <p>This is also the default authentication type if nothing else is configured.</p>"},{"location":"nessie-latest/client_config/#authentication-type-bearer","title":"Authentication Type <code>BEARER</code>","text":"<p>For the <code>BEARER</code> Authentication Type the <code>authentication.token</code> property should be set to a valid OpenID token.</p> <p>This authentication type is recommended only when the issued access token has a lifespan large enough to cover the duration of the entire Nessie client\u2019s session. Once the token is expired, the Nessie client will not be able to refresh it and will have to be restarted, with a different token. If the token needs to be refreshed periodically, then the <code>OAUTH2</code> authentication type should be preferred to this one.</p>"},{"location":"nessie-latest/client_config/#authentication-type-oauth2","title":"Authentication Type <code>OAUTH2</code>","text":"<p>The <code>OAUTH2</code> Authentication Type is able to authenticate against an OAuth2 server and obtain a valid access token. Only Bearer access tokens are currently supported. The access token is then used to authenticate against Nessie. The client will automatically refresh the access token. This authentication type is recommended when the access token has a lifespan shorter than the Nessie client\u2019s session lifespan.</p> <p>Note that the Nessie server must be configured to accept OAuth2 tokens from the same server. For example, if the OAuth2 server is Keycloak, this can be done by defining the following properties in the <code>application.properties</code> file of the Nessie server:</p> <pre><code>nessie.server.authentication.enabled=true\nquarkus.oidc.auth-server-url=https://&lt;keycloak-server&gt;/realms/&lt;realm-name&gt;\n</code></pre> <p>OAuth is a complex framework and usually requires many configuration settings on the client side. The full list of available settings is shown above, but here are some general configuration guidelines:</p>"},{"location":"nessie-latest/client_config/#configuring-endpoints","title":"Configuring endpoints","text":"<p>The Nessie client interacts with the OAuth2 server by contacting its endpoints, in order to authenticate and obtain access tokens, using various grants. The main endpoint is the token endpoint and is always required, but other endpoints may also be required, depending on the grant type being used. </p> <p>The endpoints can be provided with the following properties:</p> <ul> <li>Token endpoint: <code>nessie.authentication.oauth2.token-endpoint</code> (always required);</li> <li>Authorization endpoint: <code>nessie.authentication.oauth2.auth-endpoint</code> (required when using the   <code>authorization_code</code> grant);</li> <li>Device authorization endpoint: <code>nessie.authentication.oauth2.device-auth-endpoint</code> (required when    using the <code>device_code</code> grant).</li> </ul> <p>However, instead of specifying the endpoints individually, it is recommended to use the all-in-one property <code>nessie.authentication.oauth2.issuer-url</code> whenever possible. When this property is provided, the client is capable of discovering all the required endpoints automatically by querying the authorization server well-known metadata endpoint.</p>"},{"location":"nessie-latest/client_config/#configuring-grant-types","title":"Configuring grant types","text":"<p>Another important property is <code>authentication.oauth2.grant-type</code>, which defines the grant type to use when authenticating against the OAuth2 server. Valid values are:</p> <ul> <li><code>client_credentials</code> : enables the Client Credentials grant (default);</li> <li><code>password</code> : enables the Resource Owner Password Credentials grant;</li> <li><code>authorization_code</code> : enables the Authorization Code grant;</li> <li><code>device_code</code> : enables the Device Authorization grant;</li> <li><code>token_exchange</code> : enables the Token Exchange grant.</li> </ul> <p>Note</p> <p>The Device Authorization grant can also be specified using its canonical URN:  <code>urn:ietf:params:oauth:grant-type:device_code</code>.</p> <p>Note</p> <p>The Token Exchange grant can also be specified using its canonical URN:  <code>urn:ietf:params:oauth:grant-type:token-exchange</code>.</p> <p>The <code>client_credentials</code> grant type is the simplest one, but it requires the client to be granted enough permissions to access the Nessie server on behalf of the user. This is not always possible, and should be avoided if the session is interactive (that is, when the client is being controlled by a human).</p> <p>For this grant type, the following properties must be provided:</p> <ul> <li><code>nessie.authentication.oauth2.issuer-url</code> or <code>nessie.authentication.oauth2.token-endpoint</code>;</li> <li><code>nessie.authentication.oauth2.client-id</code>;</li> <li><code>nessie.authentication.oauth2.client-secret</code> (unless the client is public).</li> </ul> <p>The <code>password</code> grant type is also simple, but it requires passing the user\u2019s password to the client, which may not be acceptable in some cases for security reasons. Many identity providers forbid its usage. </p> <p>All the properties required for <code>client_credentials</code> are also required for this grant type, as well as the following ones:</p> <ul> <li><code>nessie.authentication.oauth2.username</code>;</li> <li><code>nessie.authentication.oauth2.password</code>.</li> </ul> <p>For real users trying to authenticate within a terminal session, such as a Spark shell, the <code>authorization_code</code> grant type is recommended. It requires the user to authenticate in a browser window, thus sparing the need to provide the user\u2019s password directly to the client. The user will be prompted to authenticate in a separate browser window, and the Nessie client will be notified when the authentication is complete. </p> <p>All the properties required for <code>client_credentials</code> are also required for this grant type. As explained above, if <code>nessie.authentication.oauth2.issuer-url</code> is provided, then no further configuration is required. Otherwise, in addition to the token endpoint, the authorization endpoint must also be provided (<code>nessie.authentication.oauth2.auth-endpoint</code>).</p> <p>If the terminal session is running remotely however, on inside an embedded device, then the <code>authorization_code</code> grant type may not be suitable, as the browser and the terminal session must be running on the same machine. In this case, the <code>device_code</code> grant type is recommended. Similar to the <code>authorization_code</code> grant type, it requires the user to authenticate in a browser window, but it does not require the browser and the terminal session to be running on the same machine. The user will be prompted to authenticate in a local browser window, and the remote Nessie client will poll the OAuth2 server for the authentication status, until the authentication is complete. </p> <p>All the properties required for <code>client_credentials</code> are also required for this grant type. As explained above, if <code>nessie.authentication.oauth2.issuer-url</code> is provided, then no further configuration is required. Otherwise, in addition to the token endpoint, the device authorization endpoint must also be provided (<code>nessie.authentication.oauth2.device-auth-endpoint</code>).</p> <p>Finally, the <code>token_exchange</code> grant type is the most complex one. In-depth configuration of a token exchange grant is outside the scope of this document but in general, two use cases can be envisaged:</p> <ol> <li> <p>Initial token exchange: enabled when <code>authentication.oauth2.grant-type</code> is <code>token_exchange</code>. In    this scenario, the client will use token exchange as the primary grant. A subject token must be    provided with <code>nessie.authentication.oauth2.token-exchange.subject-token</code>. Other properties under    <code>nessie.authentication.oauth2.token-exchange.*</code> may also be required.</p> </li> <li> <p>Impersonation or delegation: this is the most typical usage, enabled when    <code>nessie.authentication.oauth2.impersonation.enabled</code> is <code>true</code>. Here, the client will first    obtain an initial token using another grant type, then exchange the received access token for    another access token, possibly from a second OAuth2 server. If a second OAuth2 server must be    contacted, use the properties under <code>nessie.authentication.oauth2.impersonation.*</code>. And finally,    since impersonation uses the token exchange grant type behind the scenes, properties under    <code>nessie.authentication.oauth2.token-exchange.*</code> may also be relevant.</p> </li> </ol> <p>Warning</p> <p>When using impersonation, the property <code>authentication.oauth2.grant-type</code> must be another grant type than <code>token_exchange</code>.</p> <p>Warning</p> <p>If a second OAuth2 server is required to perform impersonation, the admin user is responsible  for configuring the trust relationship between the two servers.</p>"},{"location":"nessie-latest/configuration/","title":"Server Configuration","text":"<p>The Nessie server is configurable via properties as listed in the application.properties file. </p> <p>These properties can be set when starting up the docker image in two different ways. For example, if  you want to set Nessie to use the <code>JDBC</code> version store and provide a JDBC connection URL, you can  either:</p> <ol> <li> <p>Set these values via the <code>JAVA_OPTS_APPEND</code> option in the Docker invocation. Each setting  should be inserted inside the variable\u2019s value as <code>-D&lt;name&gt;=&lt;value&gt;</code> pairs: </p> <pre><code>docker run  -p 19120:19120 \\\n  -e JAVA_OPTS_APPEND=\"-Dnessie.version.store.type=JDBC -Dquarkus.datasource.jdbc.url=jdbc:postgresql://host.com:5432/db\" \\\n  ghcr.io/projectnessie/nessie\n</code></pre> </li> <li> <p>Alternatively, set them via the <code>--env</code> (or <code>-e</code>) option in the Docker invocation. Each setting  must be provided separately as <code>--env NAME=value</code> options:</p> <pre><code>docker run -p 19120:19120 \\\n  --env NESSIE_VERSION_STORE_TYPE=JDBC \\\n  --env QUARKUS_DATASOURCE_JDBC_URL=\"jdbc:postgresql://host.com:5432/db\" \\\n  ghcr.io/projectnessie/nessie\n</code></pre> </li> </ol> <p>Note how the original property name is converted to an environment variable, e.g.  <code>nessie.version.store.type</code> becomes <code>NESSIE_VERSION_STORE_TYPE</code>. The conversion is done by replacing  all <code>.</code> with <code>_</code> and converting the name to upper case.  See here for more details.</p> <p>For more information on docker images, see Docker image options below.</p>"},{"location":"nessie-latest/configuration/#server-sizing","title":"Server sizing","text":"<p>The minimum resources for Nessie are 4 CPUs and 4 GB RAM.</p> <p>The recommended resources for Nessie depend on the actual use case and usage pattern(s). We recommend to try various configurations, starting with 8 CPUs and 8 GB RAM.</p> <p>The efficiency of Nessie\u2019s cache can be monitored using the metrics provided with the <code>cache=nessie-objects</code> tag, especially the <code>cache.gets</code> values for <code>hit</code>/<code>miss</code> and the <code>cause</code>s provided by <code>cache.evictions</code>.</p> <p>Note</p> <p>Nessie is a stateless service that heavily depends on the performance of the backend database (request duration and throughput) and works best with distributed key-value databases. Nessie has a built-in cache. Caches require memory, the more memory, the more efficient is the cache and the fewer operations need to be performed against the backend database.</p> <p>Tip</p> <p>You can set the the <code>nessie.version.store.persist.reference-cache-ttl</code> configuration option to further reduce the load against the backing database. See Version Store Advanced Settings below.</p> <p>Note</p> <p>Many things happen in parallel and some libraries that we have to depend on are not written in a \u201creactive way\u201d, especially with Iceberg REST. While the Iceberg REST parts in Nessie are built in a \u201creactive way\u201d, most Nessie core APIs are not.</p>"},{"location":"nessie-latest/configuration/#supported-operating-systems","title":"Supported operating systems","text":"Operating System Production Development &amp; prototyping Comments Linux Primarily supported operating systems, assuming recent kernel and distribution versions. macOS Supported for development and testing purposes. AIX Not tested, might work or not. Solaris Not tested, might work or not. Windows Not supported in any way. Nessie server and admin tool refuse to start."},{"location":"nessie-latest/configuration/#providing-secrets","title":"Providing secrets","text":"<p>Instead of providing secrets like passwords in clear text, you can also use a keystore. This functionality is provided natively via Quarkus.</p> <p>See also the secrets manager settings below for information about Hashicorp Vault, Google Cloud and Amazon Services Secrets Managers.</p>"},{"location":"nessie-latest/configuration/#core-nessie-configuration-settings","title":"Core Nessie Configuration Settings","text":""},{"location":"nessie-latest/configuration/#core-settings","title":"Core Settings","text":"<p>Nessie server configuration to be injected into the JAX-RS application.</p> Property Default Value Type Description <code>nessie.server.default-branch</code> <code>main</code> <code>string</code> The default branch to use if not provided by the user. <code>nessie.server.send-stacktrace-to-client</code> <code>false</code> <code>boolean</code> Whether stack traces should be sent to the client in case of error. The default is <code>false</code> to not expose internal details for security reasons. <code>nessie.server.access-checks-batch-size</code> <code>100</code> <code>int</code> The number of entity-checks that are grouped into a call to <code>BatchAccessChecker</code>. The default  value is quite conservative, it is the responsibility of the operator to adjust this value  according to the capabilities of the actual authz implementation. Note that the number of  checks can be slightly exceeded by the implementation, depending on the call site. <p>Related Quarkus settings:</p> Property Default values Type Description <code>quarkus.http.port</code> <code>19120</code> <code>int</code> Sets the HTTP port for the Nessie REST API endpoints. <code>quarkus.management.port</code> <code>9000</code> <code>int</code> Sets the HTTP port for management endpoints (health, metrics, Swagger) <p>Info</p> <p>A complete set of configuration options for Quarkus can be found on quarkus.io</p> <p>Info</p> <p>Reverse Proxy Settings</p> <p>These config options are mentioned only for documentation purposes. Consult the Quarkus documentation for \u201cRunning behind a reverse proxy\u201d and configure those depending on your actual needs.</p> <p>Do NOT enable these option unless your reverse proxy (for example istio or nginx) is properly setup to set these headers but also filter those from incoming requests.</p>"},{"location":"nessie-latest/configuration/#catalog-and-iceberg-rest-settings","title":"Catalog and Iceberg REST Settings","text":"Property Default Value Type Description <code>nessie.catalog.validate-secrets</code> <code>false</code> <code>boolean</code> Optional: validate at server startup that all referenced secrets can be resolved. Startup will  fail, it one or more secrets cannot be resolved at startup time, hence the default is <code>false</code>."},{"location":"nessie-latest/configuration/#warehouse-defaults","title":"Warehouse defaults","text":"Property Default Value Type Description <code>nessie.catalog.default-warehouse</code> <code>string</code> Name of the default warehouse. This one is used when a warehouse is not specified in a query.  If no default warehouse is configured and a request does not specify a warehouse, the request  will fail. <code>nessie.catalog.iceberg-config-defaults.</code><code>&lt;iceberg-property&gt;</code> <code>string</code> Iceberg config defaults applicable to all clients and warehouses. Any properties that are  common to all iceberg clients should be included here. They will be passed to all clients on  all warehouses as config defaults. These defaults can be overridden on a per-warehouse basis,  see <code>iceberg-config-defaults</code> in Warehouses. <code>nessie.catalog.iceberg-config-overrides.</code><code>&lt;iceberg-property&gt;</code> <code>string</code> Iceberg config overrides applicable to all clients and warehouses. Any properties that are  common to all iceberg clients should be included here. They will be passed to all clients on  all warehouses as config overrides. These overrides can be overridden on a per-warehouse basis,  see <code>iceberg-config-overrides</code> in Warehouses."},{"location":"nessie-latest/configuration/#warehouses","title":"Warehouses","text":"<p>Map of warehouse names to warehouse configurations.</p> Property Default Value Type Description <code>nessie.catalog.warehouses.</code><code>&lt;warehouse-name&gt;</code><code>.iceberg-config-defaults.</code><code>&lt;iceberg-property&gt;</code> <code>string</code> Iceberg config defaults specific to this warehouse, potentially overriding any defaults  specified in <code>iceberg-config-defaults</code> in Warehouse  defaults . <code>nessie.catalog.warehouses.</code><code>&lt;warehouse-name&gt;</code><code>.iceberg-config-overrides.</code><code>&lt;iceberg-property&gt;</code> <code>string</code> Iceberg config overrides specific to this warehouse. They override any overrides specified in  <code>iceberg-config-overrides</code> in Warehouse defaults. <code>nessie.catalog.warehouses.</code><code>&lt;warehouse-name&gt;</code><code>.location</code> <code>string</code> Location of the warehouse. Used to determine the base location of a table."},{"location":"nessie-latest/configuration/#s3-settings","title":"S3 settings","text":"<p>Configuration for S3 compatible object stores. </p> <p>Default settings to be applied to all buckets can be set in the <code>default-options</code> group.  Specific settings for each bucket can be specified via the <code>buckets</code> map.   </p> <p>All settings are optional. The defaults of these settings are defined by the AWSSDK Java  client.</p>"},{"location":"nessie-latest/configuration/#s3-default-bucket-settings","title":"S3 default bucket settings","text":"<p>Default bucket configuration, default/fallback values for all buckets are taken from this one.</p> Property Default Value Type Description <code>nessie.catalog.service.s3.default-options.endpoint</code> <code>uri</code> Endpoint URI, required for private (non-AWS) clouds, specified either per bucket or in the  top-level S3 settings.  If the endpoint URIs for the Nessie server and clients differ, this one defines the endpoint  used for the Nessie server. <code>nessie.catalog.service.s3.default-options.external-endpoint</code> <code>uri</code> When using a specific endpoint (<code>endpoint</code>) and the endpoint URIs for the Nessie server  differ, you can specify the URI passed down to clients using this setting.  Otherwise, clients  will receive the value from the <code>endpoint</code> setting. <code>nessie.catalog.service.s3.default-options.path-style-access</code> <code>boolean</code> Whether to use path-style access. If true, path-style access will be used, as in: <code>https://&lt;domain&gt;/&lt;bucket&gt;</code> . If false, a virtual-hosted style will be used instead, as in:  <code>https://&lt;bucket&gt;.&lt;domain&gt;</code>. If unspecified, the default will depend on the cloud  provider. <code>nessie.catalog.service.s3.default-options.access-point</code> <code>string</code> AWS Access point for this bucket. Access points can be used to perform S3 operations by  specifying a mapping of bucket to access points. This is useful for multi-region access,  cross-region access, disaster recovery, etc. See: Access      Points  <code>nessie.catalog.service.s3.default-options.allow-cross-region-access-point</code> <code>boolean</code> Authorize cross-region calls when contacting an <code>access-point</code>. By default, attempting to use an access point in a different region will throw an exception.  When enabled, this property allows using access points in other regions. <code>nessie.catalog.service.s3.default-options.region</code> <code>string</code> DNS name of the region, required for AWS. The region must be specified for AWS, either per  bucket or in the top-level S3 settings. <code>nessie.catalog.service.s3.default-options.auth-type</code> <code>APPLICATION_GLOBAL, STATIC</code> The authentication mode to use by the Catalog server. If not set, the default is <code>STATIC</code> . Depending on the authentication mode, other properties may be required.   Valid values are:    * <code>APPLICATION_GLOBAL</code>: Use the AWSSDK default        credentials provider .     * <code>STATIC</code>: Static credentials provided through the <code>access-key</code> option.   <code>nessie.catalog.service.s3.default-options.access-key</code> <code>uri</code> Name of the basic-credentials secret containing the access-key-id and secret-access-key, either  per bucket or in the top-level S3 settings.  Required when <code>auth-type</code> is <code>STATIC</code>.   For STS, this defines the Access Key ID and Secret Key ID to be used as a basic credential  for obtaining temporary session credentials. <code>nessie.catalog.service.s3.default-options.request-signing-enabled</code> <code>boolean</code> Optional parameter to disable S3 request signing. Default is to enable S3 request signing.   See <code>url-signing-expire</code>. <code>nessie.catalog.service.s3.default-options.url-signing-expire</code> <code>duration</code> Defines the validity of the signing endpoint returned to clients, defaults to 3 hours. See <code>request-signing-enabled</code>. <code>nessie.catalog.service.s3.default-options.chunked-encoding-enabled</code> <code>boolean</code> Controls whether the AWS SDK uses chunked transfer encoding for payload uploads. Disable chunked encoding for S3-compatible services such as Oracle Cloud Infrastructure  (OCI) that reject chunked payload signatures ( <code>x-amz-content-sha256</code>). Defaults to <code>true</code> to maintain AWS parity. <code>nessie.catalog.service.s3.default-options.sts-endpoint</code> <code>uri</code> The Security Token  Service  endpoint.  This parameter must be set when running in a private (non-AWS) cloud and the catalog is  configured to use S3 sessions (e.g. to use the \u201cassume role\u201d functionality). <code>nessie.catalog.service.s3.default-options.server-iam.enabled</code> <code>boolean</code> Optional parameter to enable assume role (vended credentials). Default is to disable assume  role. <code>nessie.catalog.service.s3.default-options.server-iam.policy</code> <code>string</code> IAM policy in JSON format to be used as an inline session  policy  (optional).  If specified, this policy will be used for all clients for all locations.   Related docs: S3  with IAM  and about  actions, resources, conditions  and policy  reference . <code>nessie.catalog.service.s3.default-options.server-iam.assume-role</code> <code>string</code> The ARN of  the role to assume for accessing S3 data.  This parameter is required for Amazon S3, but may not  be required for other storage providers (e.g. Minio does not use it at all).   If this option is defined, the server will attempt to assume the role at startup and cache  the returned session credentials. <code>nessie.catalog.service.s3.default-options.server-iam.role-session-name</code> <code>string</code> An identifier for the assumed role session. This parameter is most important in cases when the  same role is assumed by different principals in different use cases. <code>nessie.catalog.service.s3.default-options.server-iam.external-id</code> <code>string</code> An identifier for the party assuming the role. This parameter must match the external ID  configured in IAM rules that govern the assume role process for the specified <code>role-arn</code>.   This parameter is essential in preventing the Confused  Deputy  problem. <code>nessie.catalog.service.s3.default-options.server-iam.session-duration</code> <code>duration</code> A higher bound estimate of the expected duration of client \u201csessions\u201d working with data in this  bucket.  A session, for example, is the lifetime of an Iceberg REST catalog object on the client  side. This value is used for validating expiration times of credentials associated with the  warehouse. Must be &gt;= 1 second. <code>nessie.catalog.service.s3.default-options.client-iam.statements</code> <code>list of string</code> Additional IAM policy statements to be inserted after the automatically generated S3  location dependent <code>Allow</code> policy statement.  Example:   <code>...client-iam.statements[0]={\"Effect\":\"Allow\", \"Action\":\"s3:*\", \"Resource\":\"arn:aws:s3:::* /alwaysAllowed/*\"}  ...client-iam.statements[1]={\"Effect\":\"Deny\", \"Action\":\"s3:*\", \"Resource\":\"arn:aws:s3:::* /blocked/*\"}</code> Related docs: S3  with IAM  and about  actions, resources, conditions  and policy  reference . <code>nessie.catalog.service.s3.default-options.client-iam.enabled</code> <code>boolean</code> Optional parameter to enable assume role (vended credentials). Default is to disable assume  role. <code>nessie.catalog.service.s3.default-options.client-iam.policy</code> <code>string</code> IAM policy in JSON format to be used as an inline session  policy  (optional).  If specified, this policy will be used for all clients for all locations.   Related docs: S3  with IAM  and about  actions, resources, conditions  and policy  reference . <code>nessie.catalog.service.s3.default-options.client-iam.assume-role</code> <code>string</code> The ARN of  the role to assume for accessing S3 data.  This parameter is required for Amazon S3, but may not  be required for other storage providers (e.g. Minio does not use it at all).   If this option is defined, the server will attempt to assume the role at startup and cache  the returned session credentials. <code>nessie.catalog.service.s3.default-options.client-iam.role-session-name</code> <code>string</code> An identifier for the assumed role session. This parameter is most important in cases when the  same role is assumed by different principals in different use cases. <code>nessie.catalog.service.s3.default-options.client-iam.external-id</code> <code>string</code> An identifier for the party assuming the role. This parameter must match the external ID  configured in IAM rules that govern the assume role process for the specified <code>role-arn</code>.   This parameter is essential in preventing the Confused  Deputy  problem. <code>nessie.catalog.service.s3.default-options.client-iam.session-duration</code> <code>duration</code> A higher bound estimate of the expected duration of client \u201csessions\u201d working with data in this  bucket.  A session, for example, is the lifetime of an Iceberg REST catalog object on the client  side. This value is used for validating expiration times of credentials associated with the  warehouse. Must be &gt;= 1 second. <code>nessie.catalog.service.s3.default-options.table-config-overrides.</code><code>&lt;name&gt;</code> <code>string</code> Iceberg table configuration overrides for all tables stored in this bucket."},{"location":"nessie-latest/configuration/#s3-per-bucket-settings","title":"S3 per bucket settings","text":"<p>Per-bucket configurations. The effective value for a bucket is taken from the per-bucket  setting. If no per-bucket setting is present, uses the defaults from the top-level S3 settings  in <code>default-options</code>.</p> Property Default Value Type Description <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.endpoint</code> <code>uri</code> Endpoint URI, required for private (non-AWS) clouds, specified either per bucket or in the  top-level S3 settings.  If the endpoint URIs for the Nessie server and clients differ, this one defines the endpoint  used for the Nessie server. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.external-endpoint</code> <code>uri</code> When using a specific endpoint (<code>endpoint</code>) and the endpoint URIs for the Nessie server  differ, you can specify the URI passed down to clients using this setting.  Otherwise, clients  will receive the value from the <code>endpoint</code> setting. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.path-style-access</code> <code>boolean</code> Whether to use path-style access. If true, path-style access will be used, as in: <code>https://&lt;domain&gt;/&lt;bucket&gt;</code> . If false, a virtual-hosted style will be used instead, as in:  <code>https://&lt;bucket&gt;.&lt;domain&gt;</code>. If unspecified, the default will depend on the cloud  provider. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.access-point</code> <code>string</code> AWS Access point for this bucket. Access points can be used to perform S3 operations by  specifying a mapping of bucket to access points. This is useful for multi-region access,  cross-region access, disaster recovery, etc. See: Access      Points  <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.allow-cross-region-access-point</code> <code>boolean</code> Authorize cross-region calls when contacting an <code>access-point</code>. By default, attempting to use an access point in a different region will throw an exception.  When enabled, this property allows using access points in other regions. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.region</code> <code>string</code> DNS name of the region, required for AWS. The region must be specified for AWS, either per  bucket or in the top-level S3 settings. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.auth-type</code> <code>APPLICATION_GLOBAL, STATIC</code> The authentication mode to use by the Catalog server. If not set, the default is <code>STATIC</code> . Depending on the authentication mode, other properties may be required.   Valid values are:    * <code>APPLICATION_GLOBAL</code>: Use the AWSSDK default        credentials provider .     * <code>STATIC</code>: Static credentials provided through the <code>access-key</code> option.   <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.access-key</code> <code>uri</code> Name of the basic-credentials secret containing the access-key-id and secret-access-key, either  per bucket or in the top-level S3 settings.  Required when <code>auth-type</code> is <code>STATIC</code>.   For STS, this defines the Access Key ID and Secret Key ID to be used as a basic credential  for obtaining temporary session credentials. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.request-signing-enabled</code> <code>boolean</code> Optional parameter to disable S3 request signing. Default is to enable S3 request signing.   See <code>url-signing-expire</code>. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.url-signing-expire</code> <code>duration</code> Defines the validity of the signing endpoint returned to clients, defaults to 3 hours. See <code>request-signing-enabled</code>. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.chunked-encoding-enabled</code> <code>boolean</code> Controls whether the AWS SDK uses chunked transfer encoding for payload uploads. Disable chunked encoding for S3-compatible services such as Oracle Cloud Infrastructure  (OCI) that reject chunked payload signatures ( <code>x-amz-content-sha256</code>). Defaults to <code>true</code> to maintain AWS parity. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.sts-endpoint</code> <code>uri</code> The Security Token  Service  endpoint.  This parameter must be set when running in a private (non-AWS) cloud and the catalog is  configured to use S3 sessions (e.g. to use the \u201cassume role\u201d functionality). <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.server-iam.enabled</code> <code>boolean</code> Optional parameter to enable assume role (vended credentials). Default is to disable assume  role. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.server-iam.policy</code> <code>string</code> IAM policy in JSON format to be used as an inline session  policy  (optional).  If specified, this policy will be used for all clients for all locations.   Related docs: S3  with IAM  and about  actions, resources, conditions  and policy  reference . <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.server-iam.assume-role</code> <code>string</code> The ARN of  the role to assume for accessing S3 data.  This parameter is required for Amazon S3, but may not  be required for other storage providers (e.g. Minio does not use it at all).   If this option is defined, the server will attempt to assume the role at startup and cache  the returned session credentials. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.server-iam.role-session-name</code> <code>string</code> An identifier for the assumed role session. This parameter is most important in cases when the  same role is assumed by different principals in different use cases. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.server-iam.external-id</code> <code>string</code> An identifier for the party assuming the role. This parameter must match the external ID  configured in IAM rules that govern the assume role process for the specified <code>role-arn</code>.   This parameter is essential in preventing the Confused  Deputy  problem. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.server-iam.session-duration</code> <code>duration</code> A higher bound estimate of the expected duration of client \u201csessions\u201d working with data in this  bucket.  A session, for example, is the lifetime of an Iceberg REST catalog object on the client  side. This value is used for validating expiration times of credentials associated with the  warehouse. Must be &gt;= 1 second. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.client-iam.statements</code> <code>list of string</code> Additional IAM policy statements to be inserted after the automatically generated S3  location dependent <code>Allow</code> policy statement.  Example:   <code>...client-iam.statements[0]={\"Effect\":\"Allow\", \"Action\":\"s3:*\", \"Resource\":\"arn:aws:s3:::* /alwaysAllowed/*\"}  ...client-iam.statements[1]={\"Effect\":\"Deny\", \"Action\":\"s3:*\", \"Resource\":\"arn:aws:s3:::* /blocked/*\"}</code> Related docs: S3  with IAM  and about  actions, resources, conditions  and policy  reference . <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.client-iam.enabled</code> <code>boolean</code> Optional parameter to enable assume role (vended credentials). Default is to disable assume  role. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.client-iam.policy</code> <code>string</code> IAM policy in JSON format to be used as an inline session  policy  (optional).  If specified, this policy will be used for all clients for all locations.   Related docs: S3  with IAM  and about  actions, resources, conditions  and policy  reference . <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.client-iam.assume-role</code> <code>string</code> The ARN of  the role to assume for accessing S3 data.  This parameter is required for Amazon S3, but may not  be required for other storage providers (e.g. Minio does not use it at all).   If this option is defined, the server will attempt to assume the role at startup and cache  the returned session credentials. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.client-iam.role-session-name</code> <code>string</code> An identifier for the assumed role session. This parameter is most important in cases when the  same role is assumed by different principals in different use cases. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.client-iam.external-id</code> <code>string</code> An identifier for the party assuming the role. This parameter must match the external ID  configured in IAM rules that govern the assume role process for the specified <code>role-arn</code>.   This parameter is essential in preventing the Confused  Deputy  problem. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.client-iam.session-duration</code> <code>duration</code> A higher bound estimate of the expected duration of client \u201csessions\u201d working with data in this  bucket.  A session, for example, is the lifetime of an Iceberg REST catalog object on the client  side. This value is used for validating expiration times of credentials associated with the  warehouse. Must be &gt;= 1 second. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.name</code> <code>string</code> The human consumable name of the bucket. If unset, the name of the bucket will be extracted  from the configuration option name, e.g. if <code>nessie.catalog.service.s3.bucket1.name=my-bucket</code> is set, the bucket name will be <code>my-bucket</code> ; otherwise, it will be <code>bucket1</code>.   This can be used; if the bucket name contains non-alphanumeric characters, such as dots or  dashes. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.authority</code> <code>string</code> The authority part in a storage location URI. This is the bucket name for S3 and GCS, for ADLS  this is the storage account name (optionally prefixed with the container/file-system name).  Defaults to (<code>#name()</code>).   For S3 and GCS this option should mention the name of the bucket.   For ADLS: The value of this option is using the <code>container@storageAccount</code> syntax. It  is mentioned as <code>&lt;file_system&gt;@&lt;account_name&gt;</code> in the Azure  Docs . Note that the <code>&lt;file_system&gt;@</code> part is optional, <code>&lt;account_name&gt;</code> is the  fully qualified name, usually ending in <code>.dfs.core.windows.net</code>. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.path-prefix</code> <code>string</code> The path prefix for this storage location. <code>nessie.catalog.service.s3.buckets.</code><code>&lt;key&gt;</code><code>.table-config-overrides.</code><code>&lt;name&gt;</code> <code>string</code> Iceberg table configuration overrides for all tables stored in this bucket."},{"location":"nessie-latest/configuration/#s3-transport","title":"S3 transport","text":"Property Default Value Type Description <code>nessie.catalog.service.s3.http.max-http-connections</code> <code>int</code> Override the default maximum number of pooled connections. <code>nessie.catalog.service.s3.http.read-timeout</code> <code>duration</code> Override the default connection read timeout. <code>nessie.catalog.service.s3.http.connect-timeout</code> <code>duration</code> Override the default TCP connect timeout. <code>nessie.catalog.service.s3.http.connection-acquisition-timeout</code> <code>duration</code> Override default connection acquisition timeout. This is the time a request will wait for a  connection from the pool. <code>nessie.catalog.service.s3.http.connection-max-idle-time</code> <code>duration</code> Override default max idle time of a pooled connection. <code>nessie.catalog.service.s3.http.connection-time-to-live</code> <code>duration</code> Override default time-time of a pooled connection. <code>nessie.catalog.service.s3.http.expect-continue-enabled</code> <code>boolean</code> Override default behavior whether to expect an HTTP/100-Continue. <code>nessie.catalog.service.s3.trust-all-certificates</code> <code>boolean</code> Instruct the S3 HTTP client to accept all SSL certificates, if set to <code>true</code>. Enabling  this option is dangerous, it is strongly recommended to leave this option unset or <code>false</code> . <code>nessie.catalog.service.s3.trust-store.path</code> <code>path</code> Override to set the file path to a custom SSL key or trust store. <code>nessie.catalog.service.s3.trust-store.type</code> and <code>nessie.catalog.service.s3.trust-store.password</code> must be supplied as well when providing a  custom trust store.   When running in k8s or Docker, the path is local within the pod/container and must be  explicitly mounted. <code>nessie.catalog.service.s3.trust-store.type</code> <code>string</code> Override to set the type of the custom SSL key or trust store specified in <code>nessie.catalog.service.s3.trust-store.path</code> . Supported types include <code>JKS</code>, <code>PKCS12</code>, and all key store types supported by  Java 17. <code>nessie.catalog.service.s3.trust-store.password</code> <code>uri</code> Name of the key-secret containing the password for the custom SSL key or trust store specified  in <code>nessie.catalog.service.s3.trust-store.path</code>. <code>nessie.catalog.service.s3.key-store.path</code> <code>path</code> Override to set the file path to a custom SSL key or trust store. <code>nessie.catalog.service.s3.trust-store.type</code> and <code>nessie.catalog.service.s3.trust-store.password</code> must be supplied as well when providing a  custom trust store.   When running in k8s or Docker, the path is local within the pod/container and must be  explicitly mounted. <code>nessie.catalog.service.s3.key-store.type</code> <code>string</code> Override to set the type of the custom SSL key or trust store specified in <code>nessie.catalog.service.s3.trust-store.path</code> . Supported types include <code>JKS</code>, <code>PKCS12</code>, and all key store types supported by  Java 17. <code>nessie.catalog.service.s3.key-store.password</code> <code>uri</code> Name of the key-secret containing the password for the custom SSL key or trust store specified  in <code>nessie.catalog.service.s3.trust-store.path</code>."},{"location":"nessie-latest/configuration/#s3-sts-assume-role-global-settings","title":"S3 STS, assume-role global settings","text":"Property Default Value Type Description <code>nessie.catalog.service.s3.sts.session-grace-period</code> <code>duration</code> The time period to subtract from the S3 session credentials (assumed role credentials) expiry  time to define the time when those credentials become eligible for refreshing. <code>nessie.catalog.service.s3.sts.session-cache-max-size</code> <code>int</code> Maximum number of entries to keep in the session credentials cache (assumed role credentials). <code>nessie.catalog.service.s3.sts.clients-cache-max-size</code> <code>int</code> Maximum number of entries to keep in the STS clients cache."},{"location":"nessie-latest/configuration/#google-cloud-storage-settings","title":"Google Cloud Storage settings","text":"<p>Note</p> <p>Support for GCS is experimental.</p>"},{"location":"nessie-latest/configuration/#gcs-buckets","title":"GCS buckets","text":"<p>Configuration for Google Cloud Storage (GCS) object stores. </p> <p>Default settings to be applied to all buckets can be set in the <code>default-options</code> group.  Specific settings for each bucket can be specified via the <code>buckets</code> map.   </p> <p>All settings are optional. The defaults of these settings are defined by the Google Java SDK  client.</p>"},{"location":"nessie-latest/configuration/#gcs-default-bucket-settings","title":"GCS default bucket settings","text":"<p>Default bucket configuration, default/fallback values for all buckets are taken from this one.</p> Property Default Value Type Description <code>nessie.catalog.service.gcs.default-options.host</code> <code>uri</code> The default endpoint override to use. The endpoint is almost always used for testing purposes.   If the endpoint URIs for the Nessie server and clients differ, this one defines the endpoint  used for the Nessie server. <code>nessie.catalog.service.gcs.default-options.external-host</code> <code>uri</code> When using a specific endpoint, see <code>host</code>, and the endpoint URIs for the Nessie server  differ, you can specify the URI passed down to clients using this setting.  Otherwise, clients  will receive the value from the <code>host</code> setting. <code>nessie.catalog.service.gcs.default-options.user-project</code> <code>string</code> Optionally specify the user project (Google term). <code>nessie.catalog.service.gcs.default-options.project-id</code> <code>string</code> The Google project ID. <code>nessie.catalog.service.gcs.default-options.quota-project-id</code> <code>string</code> The Google quota project ID. <code>nessie.catalog.service.gcs.default-options.client-lib-token</code> <code>string</code> The Google client lib token. <code>nessie.catalog.service.gcs.default-options.auth-type</code> <code>NONE, USER, SERVICE_ACCOUNT, ACCESS_TOKEN, APPLICATION_DEFAULT</code> The authentication type to use. If not set, the default is <code>NONE</code>. <code>nessie.catalog.service.gcs.default-options.auth-credentials-json</code> <code>uri</code> Name of the key-secret containing the auth-credentials-JSON, this value is the name of the  credential to use, the actual credential is defined via secrets. <code>nessie.catalog.service.gcs.default-options.oauth2-token</code> <code>uri</code> Name of the token-secret containing the OAuth2 token, this value is the name of the credential  to use, the actual credential is defined via secrets. <code>nessie.catalog.service.gcs.default-options.downscoped-credentials.enable</code> <code>boolean</code> Flag to enable the currently experimental option to send short-lived and scoped-down  credentials to clients.  The current default is to not enable short-lived and scoped-down credentials, but the  default may change to enable in the future. <code>nessie.catalog.service.gcs.default-options.downscoped-credentials.expiration-margin</code> <code>duration</code> The expiration margin for the scoped down OAuth2 token. Defaults to the Google defaults. <code>nessie.catalog.service.gcs.default-options.downscoped-credentials.refresh-margin</code> <code>duration</code> The refresh margin for the scoped down OAuth2 token. Defaults to the Google defaults. <code>nessie.catalog.service.gcs.default-options.read-chunk-size</code> <code>int</code> The read chunk size in bytes. <code>nessie.catalog.service.gcs.default-options.write-chunk-size</code> <code>int</code> The write chunk size in bytes. <code>nessie.catalog.service.gcs.default-options.delete-batch-size</code> <code>int</code> The delete batch size. <code>nessie.catalog.service.gcs.default-options.encryption-key</code> <code>uri</code> Name of the key-secret containing the customer-supplied AES256 key for blob encryption when  writing. <code>nessie.catalog.service.gcs.default-options.decryption-key</code> <code>uri</code> Name of the key-secret containing the customer-supplied AES256 key for blob decryption when  reading. <code>nessie.catalog.service.gcs.default-options.table-config-overrides.</code><code>&lt;name&gt;</code> <code>string</code> Iceberg table configuration overrides for all tables stored in this bucket."},{"location":"nessie-latest/configuration/#gcs-per-bucket-settings","title":"GCS per bucket settings","text":"<p>Per-bucket configurations. The effective value for a bucket is taken from the per-bucket  setting. If no per-bucket setting is present, uses the defaults from the top-level GCS settings  in <code>default-options</code>.</p> Property Default Value Type Description <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.host</code> <code>uri</code> The default endpoint override to use. The endpoint is almost always used for testing purposes.   If the endpoint URIs for the Nessie server and clients differ, this one defines the endpoint  used for the Nessie server. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.external-host</code> <code>uri</code> When using a specific endpoint, see <code>host</code>, and the endpoint URIs for the Nessie server  differ, you can specify the URI passed down to clients using this setting.  Otherwise, clients  will receive the value from the <code>host</code> setting. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.user-project</code> <code>string</code> Optionally specify the user project (Google term). <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.project-id</code> <code>string</code> The Google project ID. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.quota-project-id</code> <code>string</code> The Google quota project ID. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.client-lib-token</code> <code>string</code> The Google client lib token. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.auth-type</code> <code>NONE, USER, SERVICE_ACCOUNT, ACCESS_TOKEN, APPLICATION_DEFAULT</code> The authentication type to use. If not set, the default is <code>NONE</code>. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.auth-credentials-json</code> <code>uri</code> Name of the key-secret containing the auth-credentials-JSON, this value is the name of the  credential to use, the actual credential is defined via secrets. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.oauth2-token</code> <code>uri</code> Name of the token-secret containing the OAuth2 token, this value is the name of the credential  to use, the actual credential is defined via secrets. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.downscoped-credentials.enable</code> <code>boolean</code> Flag to enable the currently experimental option to send short-lived and scoped-down  credentials to clients.  The current default is to not enable short-lived and scoped-down credentials, but the  default may change to enable in the future. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.downscoped-credentials.expiration-margin</code> <code>duration</code> The expiration margin for the scoped down OAuth2 token. Defaults to the Google defaults. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.downscoped-credentials.refresh-margin</code> <code>duration</code> The refresh margin for the scoped down OAuth2 token. Defaults to the Google defaults. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.read-chunk-size</code> <code>int</code> The read chunk size in bytes. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.write-chunk-size</code> <code>int</code> The write chunk size in bytes. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.delete-batch-size</code> <code>int</code> The delete batch size. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.encryption-key</code> <code>uri</code> Name of the key-secret containing the customer-supplied AES256 key for blob encryption when  writing. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.decryption-key</code> <code>uri</code> Name of the key-secret containing the customer-supplied AES256 key for blob decryption when  reading. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.name</code> <code>string</code> The human consumable name of the bucket. If unset, the name of the bucket will be extracted  from the configuration option name, e.g. if <code>nessie.catalog.service.s3.bucket1.name=my-bucket</code> is set, the bucket name will be <code>my-bucket</code> ; otherwise, it will be <code>bucket1</code>.   This can be used; if the bucket name contains non-alphanumeric characters, such as dots or  dashes. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.authority</code> <code>string</code> The authority part in a storage location URI. This is the bucket name for S3 and GCS, for ADLS  this is the storage account name (optionally prefixed with the container/file-system name).  Defaults to (<code>#name()</code>).   For S3 and GCS this option should mention the name of the bucket.   For ADLS: The value of this option is using the <code>container@storageAccount</code> syntax. It  is mentioned as <code>&lt;file_system&gt;@&lt;account_name&gt;</code> in the Azure  Docs . Note that the <code>&lt;file_system&gt;@</code> part is optional, <code>&lt;account_name&gt;</code> is the  fully qualified name, usually ending in <code>.dfs.core.windows.net</code>. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.path-prefix</code> <code>string</code> The path prefix for this storage location. <code>nessie.catalog.service.gcs.buckets.</code><code>&lt;key&gt;</code><code>.table-config-overrides.</code><code>&lt;name&gt;</code> <code>string</code> Iceberg table configuration overrides for all tables stored in this bucket."},{"location":"nessie-latest/configuration/#gcs-transport","title":"GCS transport","text":"Property Default Value Type Description <code>nessie.catalog.service.gcs.read-timeout</code> <code>duration</code> Override the default read timeout. <code>nessie.catalog.service.gcs.connect-timeout</code> <code>duration</code> Override the default connection timeout. <code>nessie.catalog.service.gcs.max-attempts</code> <code>int</code> Override the default maximum number of attempts. <code>nessie.catalog.service.gcs.logical-timeout</code> <code>duration</code> Override the default logical request timeout. <code>nessie.catalog.service.gcs.total-timeout</code> <code>duration</code> Override the default total timeout. <code>nessie.catalog.service.gcs.initial-retry-delay</code> <code>duration</code> Override the default initial retry delay. <code>nessie.catalog.service.gcs.max-retry-delay</code> <code>duration</code> Override the default maximum retry delay. <code>nessie.catalog.service.gcs.retry-delay-multiplier</code> <code>double</code> Override the default retry delay multiplier. <code>nessie.catalog.service.gcs.initial-rpc-timeout</code> <code>duration</code> Override the default initial RPC timeout. <code>nessie.catalog.service.gcs.max-rpc-timeout</code> <code>duration</code> Override the default maximum RPC timeout. <code>nessie.catalog.service.gcs.rpc-timeout-multiplier</code> <code>double</code> Override the default RPC timeout multiplier."},{"location":"nessie-latest/configuration/#adls-settings","title":"ADLS settings","text":"<p>Note</p> <p>Support for ADLS is experimental.</p> Property Default Value Type Description <code>nessie.catalog.service.adls.read-block-size</code> <code>int</code> Override the default read block size used when writing to ADLS. <code>nessie.catalog.service.adls.write-block-size</code> <code>long</code> Override the default write block size used when writing to ADLS."},{"location":"nessie-latest/configuration/#adls-default-file-system-settings","title":"ADLS default file-system settings","text":"<p>Default file-system configuration, default/fallback values for all file-systems are taken from  this one.</p> Property Default Value Type Description <code>nessie.catalog.service.adls.default-options.auth-type</code> <code>NONE, STORAGE_SHARED_KEY, SAS_TOKEN, APPLICATION_DEFAULT</code> The authentication type to use. <code>nessie.catalog.service.adls.default-options.account</code> <code>uri</code> Name of the basic-credentials secret containing the fully-qualified account name, e.g. <code>\"myaccount.dfs.core.windows.net\"</code> and account key, configured using the <code>name</code> and <code>secret</code> fields. If not specified, it will be queried via the configured credentials provider. <code>nessie.catalog.service.adls.default-options.sas-token</code> <code>uri</code> Name of the key-secret containing the SAS token to access the ADLS file system. <code>nessie.catalog.service.adls.default-options.user-delegation.enable</code> <code>boolean</code> Enable short-lived user-delegation SAS tokens per file-system. The current default is to not enable short-lived and scoped-down credentials, but the  default may change to enable in the future. <code>nessie.catalog.service.adls.default-options.user-delegation.key-expiry</code> <code>duration</code> Expiration time / validity duration of the user-delegation key, this key is  not passed to the client.  Defaults to 7 days minus 1 minute (the maximum), must be &gt;= 1 second. <code>nessie.catalog.service.adls.default-options.user-delegation.sas-expiry</code> <code>duration</code> Expiration time / validity duration of the user-delegation SAS token, which  is sent to the client.  Defaults to 3 hours, must be &gt;= 1 second. <code>nessie.catalog.service.adls.default-options.endpoint</code> <code>string</code> Define a custom HTTP endpoint. In case clients need to use a different URI, use the <code>.external-endpoint</code> setting. <code>nessie.catalog.service.adls.default-options.external-endpoint</code> <code>string</code> Define a custom HTTP endpoint, this value is used by clients. <code>nessie.catalog.service.adls.default-options.retry-policy</code> <code>NONE, EXPONENTIAL_BACKOFF, FIXED_DELAY</code> Configure the retry strategy. <code>nessie.catalog.service.adls.default-options.max-retries</code> <code>int</code> Mandatory, if any <code>retry-policy</code> is configured. <code>nessie.catalog.service.adls.default-options.try-timeout</code> <code>duration</code> Mandatory, if any <code>retry-policy</code> is configured. <code>nessie.catalog.service.adls.default-options.retry-delay</code> <code>duration</code> Mandatory, if any <code>retry-policy</code> is configured. <code>nessie.catalog.service.adls.default-options.max-retry-delay</code> <code>duration</code> Mandatory, if <code>EXPONENTIAL_BACKOFF</code> is configured. <code>nessie.catalog.service.adls.default-options.table-config-overrides.</code><code>&lt;name&gt;</code> <code>string</code> Iceberg table configuration overrides for all tables stored in this bucket."},{"location":"nessie-latest/configuration/#adls-per-file-system-settings","title":"ADLS per file-system  settings","text":"<p>Per-bucket configurations. The effective value for a bucket is taken from the per-bucket  setting. If no per-bucket setting is present, uses the defaults from the top-level ADLS  settings in <code>default-options</code>.</p> Property Default Value Type Description <code>nessie.catalog.service.adls.file-systems.</code><code>&lt;key&gt;</code><code>.auth-type</code> <code>NONE, STORAGE_SHARED_KEY, SAS_TOKEN, APPLICATION_DEFAULT</code> The authentication type to use. <code>nessie.catalog.service.adls.file-systems.</code><code>&lt;key&gt;</code><code>.account</code> <code>uri</code> Name of the basic-credentials secret containing the fully-qualified account name, e.g. <code>\"myaccount.dfs.core.windows.net\"</code> and account key, configured using the <code>name</code> and <code>secret</code> fields. If not specified, it will be queried via the configured credentials provider. <code>nessie.catalog.service.adls.file-systems.</code><code>&lt;key&gt;</code><code>.sas-token</code> <code>uri</code> Name of the key-secret containing the SAS token to access the ADLS file system. <code>nessie.catalog.service.adls.file-systems.</code><code>&lt;key&gt;</code><code>.user-delegation.enable</code> <code>boolean</code> Enable short-lived user-delegation SAS tokens per file-system. The current default is to not enable short-lived and scoped-down credentials, but the  default may change to enable in the future. <code>nessie.catalog.service.adls.file-systems.</code><code>&lt;key&gt;</code><code>.user-delegation.key-expiry</code> <code>duration</code> Expiration time / validity duration of the user-delegation key, this key is  not passed to the client.  Defaults to 7 days minus 1 minute (the maximum), must be &gt;= 1 second. <code>nessie.catalog.service.adls.file-systems.</code><code>&lt;key&gt;</code><code>.user-delegation.sas-expiry</code> <code>duration</code> Expiration time / validity duration of the user-delegation SAS token, which  is sent to the client.  Defaults to 3 hours, must be &gt;= 1 second. <code>nessie.catalog.service.adls.file-systems.</code><code>&lt;key&gt;</code><code>.endpoint</code> <code>string</code> Define a custom HTTP endpoint. In case clients need to use a different URI, use the <code>.external-endpoint</code> setting. <code>nessie.catalog.service.adls.file-systems.</code><code>&lt;key&gt;</code><code>.external-endpoint</code> <code>string</code> Define a custom HTTP endpoint, this value is used by clients. <code>nessie.catalog.service.adls.file-systems.</code><code>&lt;key&gt;</code><code>.retry-policy</code> <code>NONE, EXPONENTIAL_BACKOFF, FIXED_DELAY</code> Configure the retry strategy. <code>nessie.catalog.service.adls.file-systems.</code><code>&lt;key&gt;</code><code>.max-retries</code> <code>int</code> Mandatory, if any <code>retry-policy</code> is configured. <code>nessie.catalog.service.adls.file-systems.</code><code>&lt;key&gt;</code><code>.try-timeout</code> <code>duration</code> Mandatory, if any <code>retry-policy</code> is configured. <code>nessie.catalog.service.adls.file-systems.</code><code>&lt;key&gt;</code><code>.retry-delay</code> <code>duration</code> Mandatory, if any <code>retry-policy</code> is configured. <code>nessie.catalog.service.adls.file-systems.</code><code>&lt;key&gt;</code><code>.max-retry-delay</code> <code>duration</code> Mandatory, if <code>EXPONENTIAL_BACKOFF</code> is configured. <code>nessie.catalog.service.adls.file-systems.</code><code>&lt;key&gt;</code><code>.name</code> <code>string</code> The human consumable name of the bucket. If unset, the name of the bucket will be extracted  from the configuration option name, e.g. if <code>nessie.catalog.service.s3.bucket1.name=my-bucket</code> is set, the bucket name will be <code>my-bucket</code> ; otherwise, it will be <code>bucket1</code>.   This can be used; if the bucket name contains non-alphanumeric characters, such as dots or  dashes. <code>nessie.catalog.service.adls.file-systems.</code><code>&lt;key&gt;</code><code>.authority</code> <code>string</code> The authority part in a storage location URI. This is the bucket name for S3 and GCS, for ADLS  this is the storage account name (optionally prefixed with the container/file-system name).  Defaults to (<code>#name()</code>).   For S3 and GCS this option should mention the name of the bucket.   For ADLS: The value of this option is using the <code>container@storageAccount</code> syntax. It  is mentioned as <code>&lt;file_system&gt;@&lt;account_name&gt;</code> in the Azure  Docs . Note that the <code>&lt;file_system&gt;@</code> part is optional, <code>&lt;account_name&gt;</code> is the  fully qualified name, usually ending in <code>.dfs.core.windows.net</code>. <code>nessie.catalog.service.adls.file-systems.</code><code>&lt;key&gt;</code><code>.path-prefix</code> <code>string</code> The path prefix for this storage location. <code>nessie.catalog.service.adls.file-systems.</code><code>&lt;key&gt;</code><code>.table-config-overrides.</code><code>&lt;name&gt;</code> <code>string</code> Iceberg table configuration overrides for all tables stored in this bucket."},{"location":"nessie-latest/configuration/#adls-transport","title":"ADLS transport","text":"Property Default Value Type Description <code>nessie.catalog.service.adls.max-http-connections</code> <code>int</code> Override the default maximum number of HTTP connections that Nessie can use against all ADLS  Gen2 object stores. <code>nessie.catalog.service.adls.connect-timeout</code> <code>duration</code> Override the default TCP connect timeout for HTTP connections against ADLS Gen2 object stores. <code>nessie.catalog.service.adls.connection-idle-timeout</code> <code>duration</code> Override the default idle timeout for HTTP connections. <code>nessie.catalog.service.adls.write-timeout</code> <code>duration</code> Override the default write timeout for HTTP connections. <code>nessie.catalog.service.adls.read-timeout</code> <code>duration</code> Override the default read timeout for HTTP connections."},{"location":"nessie-latest/configuration/#advanced-catalog-settings","title":"Advanced catalog settings","text":""},{"location":"nessie-latest/configuration/#error-handling","title":"Error Handling","text":"Property Default Value Type Description <code>nessie.catalog.object-stores.health-check.enabled</code> <code>true</code> <code>boolean</code> Nessie tries to verify the connectivity to the object stores configured for each warehouse and  exposes this information as a readiness check.  It is recommended to leave this setting enabled. <code>nessie.catalog.error-handling.throttled-retry-after</code> <code>PT10S</code> <code>duration</code> Advanced property. The time interval after which a request is retried when storage I/O responds  with some \u201cretry later\u201d response."},{"location":"nessie-latest/configuration/#performance-tuning","title":"Performance Tuning","text":"Property Default Value Type Description <code>nessie.catalog.service.imports.max-concurrent</code> <code>32</code> <code>int</code> Advanced property, defines the maximum number of concurrent imports from object stores. <code>nessie.catalog.service.tasks.threads.max</code> <code>-1</code> <code>int</code> Advanced property, defines the maximum number of threads for async tasks like imports. <code>nessie.catalog.service.tasks.threads.keep-alive</code> <code>PT2S</code> <code>duration</code> Advanced thread pool setting for async tasks like imports. <code>nessie.catalog.service.tasks.minimum-delay</code> <code>PT0.001S</code> <code>duration</code> Advanced thread pool setting for async tasks like imports. <code>nessie.catalog.service.race.wait.min</code> <code>PT0.005S</code> <code>duration</code> Advanced thread pool setting for async tasks like imports. <code>nessie.catalog.service.race.wait.max</code> <code>PT0.250S</code> <code>duration</code> Advanced thread pool setting for async tasks like imports."},{"location":"nessie-latest/configuration/#secrets-manager-settings","title":"Secrets manager settings","text":"<p>Secrets for object stores are strictly separated from the actual configuration entries. This enables the use of external secrets managers. Secrets are referenced using a URN notation.</p> <p>The URN notation for Nessie secrets is <code>urn:nessie-secret:&lt;provider&gt;:&lt;secret-name&gt;</code>. <code>&lt;provider&gt;</code> references the name of the provider, for example <code>quarkus</code> to resolve secrets via the Quarkus configuration. <code>&lt;secret-name&gt;</code> is the secrets manager specific name for the secret to resolve.</p> <p>Retrieving secrets from external secrets managers like Hashicorp Vault and the Amazon, Google and Azure secrets managers can take some time. Nessie mitigates this cost by caching retrieved secrets for some time, by default 15 minutes (see config reference below). The default allows you to regularly rotate the object store secrets by updating those in the external secrets manager, Nessie will pick those up within the configured cache TTL. If you do not intent to rotate your secrets, you can bump the TTL to a very high value to prevent cached secrets from being expired and hence perform unneeded requests to secrets managers.</p> <p>Secrets manager and mapping configuration. </p> <p>Currently the following secrets managers are supported:   </p> <p>Secrets can always be provided using Quarkus\u2019 built-in  mechanisms . Additionally, the following external secrets managers can be enabled:   </p> <ul> <li><code>VAULT</code> Hashicorp Vault. See the Quarkus        docs for Hashicorp Vault  for specific information.    </li> <li><code>AMAZON</code> AWS Secrets Manager. See the Quarkus        docs for Amazon Services / Secrets Manager  for specific information.    </li> <li><code>AZURE</code> AWS Secrets Manager. NOT SUPPORTED YET! See the Quarkus        docs for Azure Key Vault  for specific information.    </li> <li><code>GOOGLE</code> Google Cloud Secrets Manager. NOT SUPPORTED YET! </li> </ul> <p>For details how secrets are stored, see below</p> Property Default Value Type Description <code>nessie.secrets.type</code> <code>ExternalSecretsManagerType</code> Choose the secrets manager to use, defaults to no secrets manager. <code>nessie.secrets.path</code> <code>string</code> The path/prefix used when accessing secrets from the secrets manager. This setting can be useful, if all Nessie related secrets have the same prefix in your  external secrets manager. <code>nessie.secrets.cache.enabled</code> <code>true</code> <code>boolean</code> Flag whether the secrets cache is enabled. <code>nessie.secrets.cache.max-elements</code> <code>1000</code> <code>long</code> Maximum number of cached secrets. <code>nessie.secrets.cache.ttl</code> <code>PT15M</code> <code>duration</code> Time until cached secrets expire. <code>nessie.secrets.get-secret-timeout</code> <code>PT2S</code> <code>duration</code> Timeout when retrieving a secret from the external secret manager, not supported for AWS."},{"location":"nessie-latest/configuration/#types-of-secrets","title":"Types of Secrets","text":"<ul> <li>Basic credentials are composites of a <code>name</code> attribute and a <code>secret</code> attribute.   AWS credentials are managed as basic credentials, where the <code>name</code> represents the access key ID and   the <code>secret</code> represents the secret access key.</li> <li>Tokens are composites of a <code>token</code> attribute and an optional <code>expiresAt</code> attribute, latter   represented as an instant.</li> <li>Keys consist of a single <code>key</code> attribute.</li> </ul>"},{"location":"nessie-latest/configuration/#quarkus-configuration-incl-environment-variables","title":"Quarkus configuration (incl environment variables)","text":"<p>Object store secrets managed via Quarkus\u2019 configuration mechanism (SmallRye Config) resolve components of the secret types (basic credentials, tokens, keys) via individual configuration keys.</p> <p>The Quarkus configuration key prefix (or environment variable name) is specified for the secret using the URN notation <code>urn:nessie-secret:quarkus:&lt;quarkus-configuration-key-prefix&gt;.&lt;secret-part&gt;</code>.</p> <p>The following example illustrates the Quarkus configuration entries to define the default S3 access-key and secret-access-key: <pre><code># \n#                   Prefix of the Quarkus configuration keys for this secret ---+\n#                                                                               |\n#                     The URN for Quarkus secrets ---+                          |\n#                                                    |                          |\n#                                                    |------------------------- |--------------------\nnessie.catalog.service.s3.default-options.access-key=urn:nessie-secret:quarkus:my-secrets.s3-default\n# The AWS access-key and secret-access-key are referenced via the \"secret-part\" name,\n# see 'Types of Secrets' above.\n# `my-secrets.s3-default` is the `secret-part` as in the last part of the above property\nmy-secrets.s3-default.name=awsAccessKeyId\nmy-secrets.s3-default.secret=awsSecretAccessKey\n</code></pre></p>"},{"location":"nessie-latest/configuration/#storing-secrets-in-hashicorp-vault","title":"Storing Secrets in Hashicorp Vault","text":"<p>Secrets in Hashicorp Vault are referenced using the URN prefix <code>urn:nessie-secret:vault:</code> followed by the name/path of the secrets in Hashicorp Vault.</p> <p>When using Hashicorp Vault make sure to configure the connection settings described in the Quarkus docs.</p> <p>In Hashicorp Vault, secrets are stored as a map of strings to strings, where the map keys are defined by the type of the secret as mentioned above.</p> <p>For example, using the <code>vault</code> tool, a basic credential is stored like this: <pre><code>vault kv put secret/nessie-secrets/... name=the_username secret=the_secret_password\n</code></pre> and similarly for AWS S3 access keys <pre><code>vault kv put secret/nessie-secrets/... name=access_key secret=secret_access_key\n</code></pre></p> <p>A token is stored like this: <pre><code>vault kv put secret/nessie-secrets/... token=value_of_the_secret_token\n</code></pre> and if it is an expiring token with the expiration timestamp as an ISO instant. <pre><code>vault kv put secret/nessie-secrets/... token=value_of_the_token expiresAt=2024-12-24T18:00:00Z\n</code></pre></p> <p>A key is stored like this: <pre><code>vault kv put secret/nessie-secrets/... key=value_of_the_secret_key\n</code></pre></p> <p>The paths mentioned above (<code>secret/nessie-secrets/...</code>) contain the path within Hashicorp Vault. Those need to be specified in the Nessie secrets URN notation starting with <code>urn:nessie-secret:vault:</code>.</p>"},{"location":"nessie-latest/configuration/#storing-secrets-in-google-cloud-and-amazon-services-secrets-managers-and-azure-key-vault","title":"Storing Secrets in Google Cloud and Amazon Services Secrets Managers and Azure Key Vault","text":"<p>Warn</p> <p>Google Secrets Manager and Azure Key Vault are both not yet supported and considered experimental! The reason is that there is no good way to test those locally and in CI.</p> <p>Secrets Store specifics:</p> Secrets Manager Nessie URN prefix Configuration Details Google Cloud Secrets Manager <code>urn:nessie-secret:google:</code> Quarkus Reference Docs Amazon Services Secrets Manager <code>urn:nessie-secret:amazon:</code> Quarkus Reference Docs Azure Key Vault <code>urn:nessie-secret:azure:</code> Quarkus Reference Docs <p>In Google Cloud and Amazon Services Secrets Managers and Azure Key Vault all secrets are stored as a single string.</p> <p>Since credentials consist of multiple values, Nessie expects the stored secret to be a JSON encoded object.</p> <p>Secrets are generally stored as JSON objects representing a map of strings to strings, where the map keys are defined by the type of the secret as mentioned above.</p> <p>For example, a basic credential has to be stored as JSON like this, without any leading or trailing whitespaces or newlines:</p> <pre><code>{\"name\": \"mysecret\", \"secret\": \"mypassword\"}\n</code></pre> <p>A token with an expiration date has to be stored as JSON like this, without any leading or trailing whitespaces or newlines, where the <code>expiresAt</code> attribute is only needed for tokens that expire:</p> <pre><code>{\"token\": \"rkljmnfgoi4jfgoiujh23o4irj\", \"expiresAt\": \"2024-06-05T20:38:16Z\"}\n</code></pre> <p>A key however is always stored as is and not encoded in JSON.</p>"},{"location":"nessie-latest/configuration/#version-store-settings","title":"Version Store Settings","text":"<p>Version store configuration.</p> Property Default Value Type Description <code>nessie.version.store.type</code> <code>IN_MEMORY</code> <code>IN_MEMORY, ROCKSDB, DYNAMODB, DYNAMODB2, MONGODB, MONGODB2, CASSANDRA, CASSANDRA2, JDBC, JDBC2, BIGTABLE</code> Sets which type of version store to use by Nessie. <code>nessie.version.store.events.enable</code> <code>true</code> <code>boolean</code> Sets whether events for the version-store are enabled. In order for events to be published,  it\u2019s not enough to enable them in the configuration; you also need to provide at least one  implementation of Nessie\u2019s EventListener SPI."},{"location":"nessie-latest/configuration/#support-for-the-database-specific-implementations","title":"Support for the database specific implementations","text":"Database Status Configuration value for <code>nessie.version.store.type</code> Notes \u201cin memory\u201d only for development and local testing <code>IN_MEMORY</code> Do not use for any serious use case. RocksDB production, single node only <code>ROCKSDB</code> Google BigTable production <code>BIGTABLE</code> MongoDB production <code>MONGODB2</code> &amp; <code>MONGODB</code> (deprecated) Amazon DynamoDB beta, only tested against the simulator <code>DYNAMODB</code> Not recommended for use with Nessie Catalog (Iceberg REST) due to its restrictive row-size limit. PostgreSQL production <code>JDBC2</code> &amp; <code>JDBC</code> (deprecated) H2 only for development and local testing <code>JDBC2</code> &amp; <code>JDBC</code> (deprecated) Do not use for any serious use case. MariaDB experimental, feedback welcome <code>JDBC2</code> &amp; <code>JDBC</code> (deprecated) MySQL experimental, feedback welcome <code>JDBC2</code> &amp; <code>JDBC</code> (deprecated) Works by connecting the MariaDB driver to a MySQL server. CockroachDB experimental, known issues <code>JDBC2</code> &amp; <code>JDBC</code> (deprecated) Known to raise user-facing \u201cwrite too old\u201d errors under contention. Apache Cassandra experimental, known issues <code>CASSANDRA2</code> &amp; <code>CASSANDRA</code> (deprecated) Known to raise user-facing errors due to Cassandra\u2019s concept of letting the driver timeout too early, or database timeouts. <p>Warn</p> <p>Warn</p> <p>Prefer the <code>CASSANDRA2</code> version store type over the <code>CASSANDRA</code> version store type, because it has way less storage overhead. The <code>CASSANDRA</code> version store type is deprecated for removal, please use the Nessie Server Admin Tool to migrate from the <code>CASSANDRA</code> version store type to <code>CASSANDRA2</code>.</p> <p>Warn</p> <p>Prefer the <code>MONGODB2</code> version store type over the <code>MONGODB</code> version store type, because it has way less storage overhead. The <code>MONGODB</code> version store type is deprecated for removal, please use the Nessie Server Admin Tool to migrate from the <code>MONGODB</code> version store type to <code>MONGODB2</code>.</p> <p>Warn</p> <p>Prefer the <code>JDBC2</code> version store type over the <code>JDBC</code> version store type, because it has way less storage overhead. The <code>JDBC</code> version store type is deprecated for removal, please use the Nessie Server Admin Tool to migrate from the <code>JDBC</code> version store type to <code>JDBC2</code>.</p> <p>Note</p> <p>Relational databases are generally slower and tend to become a bottleneck when concurrent Nessie commits against the same branch happen. This is a general limitation of relational databases and the actual unpleasant performance penalty depends on the relational database itself, its configuration and whether and how replication is enabled.</p>"},{"location":"nessie-latest/configuration/#bigtable-version-store-settings","title":"BigTable Version Store Settings","text":"<p>When setting <code>nessie.version.store.type=BIGTABLE</code> which enables Google BigTable as the  version store used by the Nessie server, the following configurations are applicable.</p> Property Default Value Type Description <code>nessie.version.store.persist.bigtable.instance-id</code> <code>nessie</code> <code>string</code> Sets the instance-id to be used with Google BigTable. <code>nessie.version.store.persist.bigtable.emulator-port</code> <code>8086</code> <code>int</code> When using the BigTable emulator, used to configure the port. <code>nessie.version.store.persist.bigtable.enable-telemetry</code> <code>true</code> <code>boolean</code> Enables telemetry with OpenCensus. <code>nessie.version.store.persist.bigtable.table-prefix</code> <code>string</code> Prefix for tables, default is no prefix. <code>nessie.version.store.persist.bigtable.no-table-admin-client</code> <code>false</code> <code>boolean</code> <code>nessie.version.store.persist.bigtable.app-profile-id</code> <code>string</code> Sets the profile-id to be used with Google BigTable. <code>nessie.version.store.persist.bigtable.quota-project-id</code> <code>string</code> Google BigTable quote project ID (optional). <code>nessie.version.store.persist.bigtable.endpoint</code> <code>string</code> Google BigTable endpoint (if not default). <code>nessie.version.store.persist.bigtable.mtls-endpoint</code> <code>string</code> Google BigTable MTLS endpoint (if not default). <code>nessie.version.store.persist.bigtable.emulator-host</code> <code>string</code> When using the BigTable emulator, used to configure the host. <code>nessie.version.store.persist.bigtable.jwt-audience-mapping.</code><code>&lt;mapping&gt;</code> <code>string</code> Was a setting for Google BigTable JWT audience mappings, but it is actually a no-op. Deprecated Audience is always set to bigtable service name. <code>nessie.version.store.persist.bigtable.initial-retry-delay</code> <code>duration</code> Initial retry delay. <code>nessie.version.store.persist.bigtable.max-retry-delay</code> <code>duration</code> Max retry-delay. <code>nessie.version.store.persist.bigtable.retry-delay-multiplier</code> <code>double</code> <code>nessie.version.store.persist.bigtable.max-attempts</code> <code>int</code> Maximum number of attempts for each Bigtable API call (including retries). <code>nessie.version.store.persist.bigtable.initial-rpc-timeout</code> <code>duration</code> Initial RPC timeout. <code>nessie.version.store.persist.bigtable.max-rpc-timeout</code> <code>duration</code> <code>nessie.version.store.persist.bigtable.rpc-timeout-multiplier</code> <code>double</code> <code>nessie.version.store.persist.bigtable.total-timeout</code> <code>duration</code> Total timeout (including retries) for Bigtable API calls. <code>nessie.version.store.persist.bigtable.min-channel-count</code> <code>int</code> Minimum number of gRPC channels. Refer to Google docs for details. <code>nessie.version.store.persist.bigtable.max-channel-count</code> <code>int</code> Maximum number of gRPC channels. Refer to Google docs for details. <code>nessie.version.store.persist.bigtable.initial-channel-count</code> <code>int</code> Initial number of gRPC channels. Refer to Google docs for details <code>nessie.version.store.persist.bigtable.min-rpcs-per-channel</code> <code>int</code> Minimum number of RPCs per channel. Refer to Google docs for details. <code>nessie.version.store.persist.bigtable.max-rpcs-per-channel</code> <code>int</code> Maximum number of RPCs per channel. Refer to Google docs for details. <p>Related Quarkus settings:</p> Property Default values Type Description <code>quarkus.google.cloud.project-id</code> <code>String</code> The Google project ID, mandatory. (Google authentication) See Quarkiverse for documentation. <p>Info</p> <p>A complete set of Google Cloud &amp; BigTable configuration options for Quarkus can be found on Quarkiverse.</p>"},{"location":"nessie-latest/configuration/#jdbc-version-store-settings","title":"JDBC Version Store Settings","text":"<p>Setting <code>nessie.version.store.type=JDBC2</code> enables transactional/RDBMS as the version store  used by the Nessie server.  </p> <p>Configuration of the datastore will be done by Quarkus and depends on many factors, such as  the actual database to use. The property <code>nessie.version.store.persist.jdbc.datasource</code> will be used to select one of the built-in datasources; currently supported values are: <code>postgresql</code> (which activates the PostgresQL driver), <code>mariadb</code> (which activates the MariaDB  driver), and <code>mysql</code> (which targets MySQL backends, but using the MariaDB driver).   </p> <p>For example, to configure a PostgresQL connection, the following configuration should be used:   </p> <ul> <li><code>nessie.version.store.type=JDBC2</code> </li> <li><code>nessie.version.store.persist.jdbc.datasource=postgresql</code> </li> <li><code>quarkus.datasource.postgresql.jdbc.url=jdbc:postgresql://localhost:5432/my_database</code> </li> <li><code>quarkus.datasource.postgresql.username=&lt;your username&gt;</code> </li> <li><code>quarkus.datasource.postgresql.password=&lt;your password&gt;</code> </li> <li>Other PostgresQL-specific properties can be set using <code>quarkus.datasource.postgresql.*</code> </li> </ul> <p>To connect to a MariaDB database instead, the following configuration should be used:   </p> <ul> <li><code>nessie.version.store.type=JDBC2</code> </li> <li><code>nessie.version.store.persist.jdbc.datasource=mariadb</code> </li> <li><code>quarkus.datasource.mariadb.jdbc.url=jdbc:mariadb://localhost:3306/my_database</code> </li> <li><code>quarkus.datasource.mariadb.username=&lt;your username&gt;</code> </li> <li><code>quarkus.datasource.mariadb.password=&lt;your password&gt;</code> </li> <li>Other MariaDB-specific properties can be set using <code>quarkus.datasource.mariadb.*</code> </li> </ul> <p>To connect to a MySQL database instead, the following configuration should be used:   </p> <ul> <li><code>nessie.version.store.type=JDBC2</code> </li> <li><code>nessie.version.store.persist.jdbc.datasource=mysql</code> </li> <li><code>quarkus.datasource.mysql.jdbc.url=jdbc:mysql://localhost:3306/my_database</code> </li> <li><code>quarkus.datasource.mysql.username=&lt;your username&gt;</code> </li> <li><code>quarkus.datasource.mysql.password=&lt;your password&gt;</code> </li> <li>Other MySQL-specific properties can be set using <code>quarkus.datasource.mysql.*</code> </li> </ul> <p>To connect to an H2 in-memory database, the following configuration should be used (note that  H2 is not recommended for production):   </p> <ul> <li><code>nessie.version.store.type=JDBC2</code> </li> <li><code>nessie.version.store.persist.jdbc.datasource=h2</code> </li> </ul> <p>Note: for MySQL, the MariaDB driver is used, as it is compatible with MySQL. You can use either  <code>jdbc:mysql</code> or <code>jdbc:mariadb</code> as the URL prefix.   </p> <p>A complete set of JDBC configuration options can be found on quarkus.io.</p> Property Default Value Type Description <code>nessie.version.store.persist.jdbc.datasource</code> <code>string</code> The name of the datasource to use. Must correspond to a configured datasource under <code>quarkus.datasource.&lt;name&gt;</code> . Supported values are: <code>postgresql</code> <code>mariadb</code>, <code>mysql</code> and <code>h2</code>. If not provided, the default Quarkus datasource, defined using the  <code>quarkus.datasource.*</code> configuration keys, will be used (the corresponding driver is  PostgresQL). Note that it is recommended to define \u201cnamed\u201d JDBC datasources, see Quarkus JDBC config  reference . <code>nessie.version.store.persist.jdbc.fetch-size</code> <code>100</code> <code>int</code> The JDBC fetch size, defaults to <code>100</code>. Must be a value <code>&gt;=0</code>, where <code>0</code> means fetching all rows in advance."},{"location":"nessie-latest/configuration/#rocksdb-version-store-settings","title":"RocksDB Version Store Settings","text":"<p>When setting <code>nessie.version.store.type=ROCKSDB</code> which enables RocksDB as the version store  used by the Nessie server, the following configurations are applicable.</p> Property Default Value Type Description <code>nessie.version.store.persist.rocks.database-path</code> <code>/tmp/nessie-rocksdb-store</code> <code>path</code> Sets RocksDB storage path."},{"location":"nessie-latest/configuration/#cassandra-version-store-settings","title":"Cassandra Version Store Settings","text":"<p>When setting <code>nessie.version.store.type=CASSANDRA</code> which enables Apache Cassandra as the  version store used by the Nessie server, the following configurations are applicable.</p> Property Default Value Type Description <code>nessie.version.store.cassandra.dml-timeout</code> <code>PT3S</code> <code>duration</code> Timeout used for queries and updates. <code>nessie.version.store.cassandra.ddl-timeout</code> <code>PT5S</code> <code>duration</code> Timeout used when creating tables. <p>Related Quarkus settings:</p> Property Default values Type Description <code>quarkus.cassandra.keyspace</code> <code>String</code> The Cassandra keyspace to use. <code>quarkus.cassandra.contact-points</code> <code>String</code> The Cassandra contact points, see Quarkus docs. <code>quarkus.cassandra.local-datacenter</code> <code>String</code> The Cassandra local datacenter to use, see Quarkus docs. <code>quarkus.cassandra.auth.username</code> <code>String</code> Cassandra authentication username, see Quarkus docs. <code>quarkus.cassandra.auth.password</code> <code>String</code> Cassandra authentication password, see Quarkus docs. <code>quarkus.cassandra.health.enabled</code> <code>false</code> <code>boolean</code> See Quarkus docs. <p>Info</p> <p>A complete set of the Quarkus Cassandra extension configuration options can be found on quarkus.io</p>"},{"location":"nessie-latest/configuration/#dynamodb-version-store-settings","title":"DynamoDB Version Store Settings","text":"<p>When setting <code>nessie.version.store.type=DYNAMODB</code> which enables DynamoDB as the version  store used by the Nessie server, the following configurations are applicable.</p> Property Default Value Type Description <code>nessie.version.store.persist.dynamodb.table-prefix</code> <code>string</code> Prefix for tables, default is no prefix. <p>Related Quarkus settings:</p> Property Default values Type Description <code>quarkus.dynamodb.aws.region</code> <code>String</code> Sets DynamoDB AWS region. <code>quarkus.dynamodb.aws.credentials.type</code> <code>default</code> <code>String</code> See Quarkiverse docs for possible values. Sets the credentials provider that should be used to authenticate with AWS. <code>quarkus.dynamodb.endpoint-override</code> <code>URI</code> Sets the endpoint URI with which the SDK should communicate. If not specified, an appropriate endpoint to be used for the given service and region. <code>quarkus.dynamodb.sync-client.type</code> <code>url</code> <code>String</code> Possible values are: <code>url</code>, <code>apache</code>. Sets the type of the sync HTTP client implementation <p>Info</p> <p>A complete set of DynamoDB configuration options for Quarkus can be found on Quarkiverse.</p>"},{"location":"nessie-latest/configuration/#mongodb-version-store-settings","title":"MongoDB Version Store Settings","text":"<p>When setting <code>nessie.version.store.type=MONGODB2</code> which enables MongoDB as the version store used by the Nessie server, the following configurations are applicable in combination with <code>nessie.version.store.type</code>.</p> <p>Related Quarkus settings:</p> Property Default values Type Description <code>quarkus.mongodb.database</code> <code>String</code> Sets MongoDB database name. <code>quarkus.mongodb.connection-string</code> <code>String</code> Sets MongoDB connection string. <p>Info</p> <p>A complete set of MongoDB configuration options for Quarkus can be found on quarkus.io.</p>"},{"location":"nessie-latest/configuration/#in-memory-version-store-settings","title":"In-Memory Version Store Settings","text":"<p>No special configuration options for this store type.</p>"},{"location":"nessie-latest/configuration/#version-store-advanced-settings","title":"Version Store Advanced Settings","text":"<p>The following configurations are advanced configurations for version stores to configure how Nessie will store the data into the configured data store:</p> <p>Usually, only the cache-capacity should be adjusted to the amount of the Java heap \u201cavailable\u201d for the cache. The default is conservative, bumping the cache size is recommended.</p> Property Default Value Type Description <code>nessie.version.store.persist.repository-id</code> (empty) <code>string</code> Nessie repository ID (optional) that identifies a particular Nessie storage repository. When remote (shared) database is used, multiple Nessie repositories may co-exist in the same  database (and in the same schema). In that case this configuration parameter can be used to  distinguish those repositories. <code>nessie.version.store.persist.commit-retries</code> <code>2147483647</code> <code>int</code> maximum retries for CAS-like operations. Used when committing to Nessie, when the HEAD (or tip)  of a branch changed during the commit, this value defines the maximum number of retries.  Default means unlimited. See: #retryMaxSleepMillis() <code>nessie.version.store.persist.commit-timeout-millis</code> <code>5000</code> <code>long</code> Timeout for CAS-like operations in milliseconds. See: #retryMaxSleepMillis() <code>nessie.version.store.persist.retry-initial-sleep-millis-lower</code> <code>5</code> <code>long</code> When the commit logic has to retry an operation due to a concurrent, conflicting update to the  database state, usually a concurrent change to a branch HEAD, this parameter defines the  initial lower bound of the exponential backoff.  See: #retryMaxSleepMillis() <code>nessie.version.store.persist.retry-initial-sleep-millis-upper</code> <code>25</code> <code>long</code> When the commit logic has to retry an operation due to a concurrent, conflicting update to the  database state, usually a concurrent change to a branch HEAD, this parameter defines the  initial upper bound of the exponential backoff.  See: #retryMaxSleepMillis() <code>nessie.version.store.persist.retry-max-sleep-millis</code> <code>250</code> <code>long</code> When the commit logic has to retry an operation due to a concurrent, conflicting update to the  database state, usually a concurrent change to a branch HEAD, this parameter defines the  maximum sleep time.  Each retry doubles the lower and upper bounds of the random sleep  time, unless the doubled upper bound would exceed the value of this configuration property. See: #retryInitialSleepMillisUpper() <code>nessie.version.store.persist.parents-per-commit</code> <code>20</code> <code>int</code> Number of parent-commit-hashes stored in each commit. This is used to allow bulk-fetches when  accessing the commit log. <code>nessie.version.store.persist.max-serialized-index-size</code> <code>204800</code> <code>int</code> The maximum allowed serialized size of the content index structure in a reference  index segment.  This value is used to determine, when elements in a reference index segment  need to be split.   Note: this value must be smaller than a database\u2019s hard item/row size  limit. <code>nessie.version.store.persist.max-incremental-index-size</code> <code>51200</code> <code>int</code> The maximum allowed serialized size of the content index structure in a Nessie commit,  called incremental index. This value is used to determine, when elements in an  incremental index, which were kept from previous commits, need to be pushed to a new or updated  reference index.   Note: this value must be smaller than a database\u2019s hard item/row size  limit. <code>nessie.version.store.persist.max-reference-stripes-per-commit</code> <code>50</code> <code>int</code> Maximum number of referenced index objects stored inside commit objects. If the external reference index for this commit consists of up to this amount of stripes,  the references to the stripes will be stored inside the commit object. If there are more than  this amount of stripes, an external index segment will be created instead. <code>nessie.version.store.persist.assumed-wall-clock-drift-micros</code> <code>5000000</code> <code>long</code> Assumed wall-clock drift between multiple Nessie instances in microseconds. <code>nessie.version.store.persist.ref-previous-head-count</code> <code>20</code> <code>int</code> Named references keep a history of up to this amount of previous HEAD pointers, and up to the  configured age. <code>nessie.version.store.persist.ref-previous-head-time-span-seconds</code> <code>300</code> <code>long</code> Named references keep a history of previous HEAD pointers with this age in seconds, and up to  the configured amount. <code>nessie.version.store.persist.cache-capacity-mb</code> <code>int</code> Fixed amount of heap used to cache objects, set to 0 to disable the cache entirely. Must not be  used with fractional cache sizing. See description for <code>cache-capacity-fraction-of-heap</code> for the default value. <code>nessie.version.store.persist.cache-enable-soft-references</code> <code>false</code> <code>boolean</code> Nessie keeps so called soft-references of the cached Java objects in addition to the serialized  representation around.  This toggle optionally enables this behavior. <code>nessie.version.store.persist.cache-capacity-fraction-min-size-mb</code> <code>64</code> <code>int</code> When using fractional cache sizing, this amount in MB is the minimum cache size. <code>nessie.version.store.persist.cache-capacity-fraction-of-heap</code> <code>0.6</code> <code>double</code> Fraction of Java\u2019s max heap size to use for cache objects, set to 0 to disable. Must not be  used with fixed cache sizing. If neither this value nor a fixed size is configured, a default  of <code>.6</code> (60%) is assumed. <code>nessie.version.store.persist.cache-capacity-fraction-adjust-mb</code> <code>256</code> <code>int</code> When using fractional cache sizing, this amount in MB of the heap will always be \u201ckept free\u201d  when calculating the cache size. <code>nessie.version.store.persist.cache-capacity-overshoot</code> <code>0.1</code> <code>double</code> Admitted cache-capacity-overshoot fraction, defaults to <code>0.1</code> (10 %).  New elements are admitted to be added to the cache, if the cache\u2019s size is less than <code>cache-capacity * (1 + cache-capacity-overshoot</code> .   Cache eviction happens asynchronously. Situations when eviction cannot keep up with the  amount of data added could lead to out-of-memory situations.   Value must be greater than 0, if present. <code>nessie.version.store.persist.reference-cache-ttl</code> <code>duration</code> Defines the duration how long references shall be kept in the cache. Defaults to not cache  references. If reference caching is enabled, it is highly recommended to also enable negative  reference caching.   It is safe to enable this for single node Nessie deployments.   Recommended value is currently <code>PT5M</code> for distributed and high values like <code>PT1H</code> for single node Nessie deployments.   This feature is experimental except for single Nessie node deployments! If in doubt,  leave this un-configured! <code>nessie.version.store.persist.reference-cache-negative-ttl</code> <code>duration</code> Defines the duration how long sentinels for non-existing references shall be kept in the cache  (negative reference caching).  Defaults to <code>reference-cache-ttl</code>. Has no effect, if <code>reference-cache-ttl</code> is  not configured. Default is not enabled. If reference caching is enabled, it is highly  recommended to also enable negative reference caching.   It is safe to enable this for single node Nessie deployments.   Recommended value is currently <code>PT5M</code> for distributed and high values like <code>PT1H</code> for single node Nessie deployments.   This feature is experimental except for single Nessie node deployments! If in doubt,  leave this un-configured! <code>nessie.version.store.persist.cache-invalidations.service-names</code> <code>list of string</code> Host names or IP addresses or kubernetes headless-service name of all Nessie server instances  accessing the same repository.  This value is automatically configured via the Nessie  Helm chart  or the Kubernetes operator (not released yet), you don\u2019t need any additional  configuration for distributed cache invalidations - it\u2019s setup and configured automatically. If  you have your own Helm chart or custom deployment, make sure to configure the IPs of all Nessie  instances here.   Names that start with an equal sign are not resolved but used \u201cas is\u201d. <code>nessie.version.store.persist.cache-invalidations.valid-tokens</code> <code>list of string</code> List of cache-invalidation tokens to authenticate incoming cache-invalidation messages. <code>nessie.version.store.persist.cache-invalidations.uri</code> <code>/nessie-management/cache-coherency</code> <code>string</code> URI of the cache-invalidation endpoint, only available on the Quarkus management port, defaults  to 9000. <code>nessie.version.store.persist.cache-invalidations.service-name-lookup-interval</code> <code>PT10S</code> <code>duration</code> Interval of service-name lookups to resolve the service names (#cacheInvalidationServiceNames()) into IP addresses. <code>nessie.version.store.persist.cache-invalidations.batch-size</code> <code>20</code> <code>int</code> <code>nessie.version.store.persist.cache-invalidations.request-timeout</code> <code>duration</code>"},{"location":"nessie-latest/configuration/#authentication-settings","title":"Authentication settings","text":"<p>Configuration for Nessie authentication settings.</p> Property Default Value Type Description <code>nessie.server.authentication.enabled</code> <code>false</code> <code>boolean</code> Enable Nessie authentication. <p>Related Quarkus settings:</p> Property Default values Type Description <code>quarkus.oidc.auth-server-url</code> <code>String</code> Sets the base URL of the OpenID Connect (OIDC) server if <code>nessie.server.authentication.enabled=true</code> <code>quarkus.oidc.client-id</code> <code>String</code> Sets client-id of the application if <code>nessie.server.authentication.enabled=true</code>. Each application has a client-id that is used to identify the application."},{"location":"nessie-latest/configuration/#authorization-settings","title":"Authorization settings","text":"<p>Configuration for Nessie authorization settings.</p> Property Default Value Type Description <code>nessie.server.authorization.enabled</code> <code>false</code> <code>boolean</code> Enable Nessie authorization. <code>nessie.server.authorization.type</code> <code>CEL</code> <code>string</code> Sets the authorizer type to use. <code>nessie.server.authorization.rules.</code><code>&lt;name&gt;</code> <code>string</code> CEL authorization rules where the key represents the rule id and the value the CEL expression."},{"location":"nessie-latest/configuration/#metrics","title":"Metrics","text":"<p>Metrics are published using Micrometer; they are available from Nessie\u2019s management interface (port 9000 by default) under the path <code>/q/metrics</code>. For example, if the server is running on localhost, the metrics can be accessed via http://localhost:9000/q/metrics.</p> <p>Metrics can be scraped by Prometheus or any compatible metrics scraping server. See: Prometheus for more information.</p> <p>Additional tags can be added to the metrics by setting the <code>nessie.metrics.tags.*</code> property. Each tag is a key-value pair, where the key is the tag name and the value is the tag value. For example, to add a tag <code>environment=prod</code> to all metrics, set <code>nessie.metrics.tags.environment=prod</code>. Many tags can be added, such as below:</p> <pre><code>nessie.metrics.tags.service=nessie\nnessie.metrics.tags.environment=prod\nnessie.metrics.tags.region=us-west-2\n</code></pre> <p>Note that by default Nessie adds one tag: <code>application=Nessie</code>. You can override this tag by setting the <code>nessie.metrics.tags.application=&lt;new-value&gt;</code> property.</p> <p>A standard Grafana dashboard is available in the <code>grafana</code> directory of the Nessie repository [here] (https://github.com/projectnessie/nessie/blob/main/grafana/nessie.json). You can use this dashboard to visualize the metrics scraped by Prometheus. Note that this dashboard is a starting point and may need to be customized to fit your specific needs. </p> <p>This Grafana dashboard expects the metrics to have a few tags defined: <code>service</code> and <code>instance</code>. The <code>instance</code> tag is generally added by Prometheus automatically, but the <code>service</code> tag needs to be added manually. You can configure Nessie to add this tag to all metrics by setting the below property:</p> <pre><code>nessie.metrics.tags.service=&lt;service-name&gt;\n</code></pre> <p>Alternatively, you can modify the dashboard to remove unnecessary tags, or configure Prometheus to add the missing ones. Here is an example configuration showing how to have the <code>service</code> tag added by Prometheus:</p> <pre><code>scrape_configs:\n  - job_name: 'nessie'\n    metrics_path: /q/metrics\n    static_configs:\n      - targets: ['nessie:9000']\n        labels:\n          service: nessie\n</code></pre>"},{"location":"nessie-latest/configuration/#traces","title":"Traces","text":"<p>Since Nessie 0.46.0, traces are published using OpenTelemetry. See Using OpenTelemetry in the Quarkus documentation.</p> <p>In order for the server to enable OpenTelemetry and publish its traces, the <code>quarkus.otel.exporter.otlp.traces.endpoint</code> property must be defined. Its value must be a valid collector endpoint URL, with either <code>http://</code> or <code>https://</code> scheme. The collector must talk the OpenTelemetry protocol (OTLP) and the port must be its gRPC port (by default 4317), e.g. \u201chttp://otlp-collector:4317\u201d. If this property is not set, the server will not publish traces.</p> <p>Alternatively, it\u2019s possible to forcibly disable OpenTelemetry at runtime by setting the following  property: <code>quarkus.otel.sdk.disabled=true</code>.</p>"},{"location":"nessie-latest/configuration/#troubleshooting-traces","title":"Troubleshooting traces","text":"<p>If the server is unable to publish traces, check first for a log warning message like the following:</p> <pre><code>SEVERE [io.ope.exp.int.grp.OkHttpGrpcExporter] (OkHttp http://localhost:4317/...) Failed to export spans. \nThe request could not be executed. Full error message: Failed to connect to localhost/0:0:0:0:0:0:0:1:4317\n</code></pre> <p>This means that the server is unable to connect to the collector. Check that the collector is running and that the URL is correct.</p>"},{"location":"nessie-latest/configuration/#docker-image-options","title":"Docker image options","text":"<p>By default, Nessie listens on port 19120. To expose that port on the host, use <code>-p 19120:19120</code>.  To expose that port on a different port on the host system, use the <code>-p</code> option and map the internal port to some port on the host. For example, to expose Nessie on port 8080 of the host  system, use the following command:</p> <pre><code>docker run -p 8080:19120 ghcr.io/projectnessie/nessie\n</code></pre> <p>Then you can browse Nessie\u2019s UI on the host by pointing your browser to http://localhost:8080.</p> <p>Note: this doesn\u2019t change the port Nessie listens on, it only changes the port on the host system that is mapped to the port Nessie listens on. Nessie still listens on port 19120 inside the container. If you want to change the port Nessie listens on, you can use the <code>QUARKUS_HTTP_PORT</code> environment variable. For example, to make Nessie listen on port 8080 inside the container,  and expose it to the host system also on 8080, use the following command:</p> <pre><code>docker run -p 8080:8080 -e QUARKUS_HTTP_PORT=8080 ghcr.io/projectnessie/nessie\n</code></pre>"},{"location":"nessie-latest/configuration/#nessie-docker-image-types","title":"Nessie Docker image types","text":"<p>Nessie publishes a Java based multiplatform (for amd64, arm64, ppc64le, s390x) image running on OpenJDK 17.</p>"},{"location":"nessie-latest/configuration/#advanced-docker-image-tuning-java-images-only","title":"Advanced Docker image tuning (Java images only)","text":"<p>There are many environment variables available to configure the Docker image. If in doubt, leave everything at its default. You can configure the behavior using the environment variables listed below, which come from the base image used by Nessie, ubi9/openjdk-21-runtime.</p>"},{"location":"nessie-latest/configuration/#examples","title":"Examples","text":"Example <code>docker run</code> option Using another GC <code>-e GC_CONTAINER_OPTIONS=\"-XX:+UseShenandoahGC\"</code> lets Nessie use Shenandoah GC instead of the default parallel GC. Set the Java heap size to a fixed amount <code>-e JAVA_OPTS_APPEND=\"-Xms8g -Xmx8g\"</code> lets Nessie use a Java heap of 8g."},{"location":"nessie-latest/configuration/#reference","title":"Reference","text":"Environment variable Description <code>JAVA_OPTS</code> or <code>JAVA_OPTIONS</code> NOT RECOMMENDED. JVM options passed to the <code>java</code> command (example: \u201c-verbose:class\u201d). Setting this variable will override all options set by any of the other variables in this table. To pass extra settings, use <code>JAVA_OPTS_APPEND</code> instead. <code>JAVA_OPTS_APPEND</code> User specified Java options to be appended to generated options in <code>JAVA_OPTS</code> (example: \u201c-Dsome.property=foo\u201d). <code>JAVA_TOOL_OPTIONS</code> This variable is defined and honored by all OpenJDK distros, see here. Options defined here take precedence over all else; using this variable is generally not necessary, but can be useful e.g. to enforce JVM startup parameters, to set up remote debug, or to define JVM agents. <code>JAVA_MAX_MEM_RATIO</code> Is used to calculate a default maximal heap memory based on a containers restriction. If used in a container without any memory constraints for the container then this option has no effect. If there is a memory constraint then <code>-XX:MaxRAMPercentage</code> is set to a ratio of the container available memory as set here. The default is <code>80</code> which means 80% of the available memory is used as an upper boundary. You can skip this mechanism by setting this value to <code>0</code> in which case no <code>-XX:MaxRAMPercentage</code> option is added. <code>JAVA_DEBUG</code> If set remote debugging will be switched on. Disabled by default (example: true\u201d). <code>JAVA_DEBUG_PORT</code> Port used for remote debugging. Defaults to \u201c5005\u201d (tip: use \u201c*:5005\u201d to enable debugging on all network interfaces). <code>GC_MIN_HEAP_FREE_RATIO</code> Minimum percentage of heap free after GC to avoid expansion. Default is 10. <code>GC_MAX_HEAP_FREE_RATIO</code> Maximum percentage of heap free after GC to avoid shrinking. Default is 20. <code>GC_TIME_RATIO</code> Specifies the ratio of the time spent outside the garbage collection. Default is 4. <code>GC_ADAPTIVE_SIZE_POLICY_WEIGHT</code> The weighting given to the current GC time versus previous GC times. Default is 90. <code>GC_METASPACE_SIZE</code> The initial metaspace size. There is no default (example: \u201c20\u201d). <code>GC_MAX_METASPACE_SIZE</code> The maximum metaspace size. There is no default (example: \u201c100\u201d). <code>GC_CONTAINER_OPTIONS</code> Specify Java GC to use. The value of this variable should contain the necessary JRE command-line options to specify the required GC, which will override the default of <code>-XX:+UseParallelGC</code> (example: <code>-XX:+UseG1GC</code>)."},{"location":"nessie-latest/configuration/#troubleshooting-configuration-issues","title":"Troubleshooting configuration issues","text":"<p>If you encounter issues with the configuration, you can ask Nessie to print out the configuration it is using. To do this, set the log level for the <code>io.smallrye.config</code> category to <code>DEBUG</code>, and also set the console appender level to <code>DEBUG</code>:</p> <pre><code>quarkus.log.console.level=DEBUG\nquarkus.log.category.\"io.smallrye.config\".level=DEBUG\n</code></pre> <p>Warn</p> <p>This will print out all configuration values, including sensitive ones like passwords. Don\u2019t do this in production, and don\u2019t share this output with anyone you don\u2019t trust!</p>"},{"location":"nessie-latest/export_import/","title":"Nessie Server Admin Tool","text":"<p>This page explains how to use the Nessie Server Admin Tool to perform repository maintenance tasks such as:</p> <ul> <li>Obtaining information about a Nessie repository;</li> <li>Exporting a Nessie repository to a ZIP file, e.g. to create a backup;</li> <li>Importing a Nessie repository from a ZIP file, e.g. to restore a backup;</li> <li>Migrating from a legacy version store type.</li> </ul>"},{"location":"nessie-latest/export_import/#usage","title":"Usage","text":"<p>The Nessie Server Admin Tool requires direct access to the database used by Nessie. The executable is named <code>nessie-server-admin-tool-x.y.z-runner.jar</code> and can be downloaded from the release page on GitHub.</p> <p>Note</p> <p>The Nessie Server Admin Tool is an executable jar that can be used to interact with a Nessie database directly. It should not be confused with the Nessie CLI tool, which interacts with Nessie servers via the REST API.</p> <p>The Nessie Server Admin Tool <code>nessie-server-admin-tool-x.y.z-runner.jar</code> should use the same configuration settings as the Nessie Quarkus server. These settings should be passed to the tool using system properties, environment variables or a configuration file. The most relevant settings are those related to the database connection.</p> <p>A help command is available to list all available commands and options:</p> <pre><code>java -jar nessie-server-admin-tool-x.y.z-runner.jar help\n</code></pre>"},{"location":"nessie-latest/export_import/#repository-information","title":"Repository information","text":"<p>The simplest command is <code>info</code>, which prints information about the Nessie repository.</p> <p>For example, here is how to print information about a Nessie repository hosted in a MongoDB database called <code>nessie</code> running on <code>nessie.example.com:27017</code>:</p> <pre><code>java \\\n  -Dnessie.version.store.type=MONGODB \\\n  -Dquarkus.mongodb.database=nessie \\\n  -Dquarkus.mongodb.connection-string=mongodb://&lt;user&gt;:&lt;password&gt;@nessie.example.com:27017 \\\n  -jar nessie-server-admin-tool-runner.jar \\\n  info\n</code></pre> <p>The output should look similar to this:</p> <pre><code>No-ancestor hash:                  2e1cfa82b035c26cbbbdae632cea070514eb8b773f616aaeaf668e2f0be8f10d\nDefault branch head commit ID:     11b5d0f393ad84da4ae9724654d35b96863eda02101f3ff1e633e0b25e0513db\nDefault branch commit count:       100\nRepository description version:    0\nRepository description properties:\n\nFrom configuration:\n-------------------\nVersion-store type:                MONGODB\nDefault branch:                    main\n</code></pre>"},{"location":"nessie-latest/export_import/#exporting","title":"Exporting","text":"<p>The following command (replace <code>x.y.z</code> with the version you\u2019re using) exports your Nessie repository to a single ZIP file called <code>my-export-file.zip</code>,</p> <pre><code>java -jar nessie-server-admin-tool-x.y.z-runner.jar export --path my-export-file.zip\n</code></pre> <p>A ZIP file export contains all necessary repository information in a single, compressed file. Note that the export will only automatically generate a ZIP file, if the output path ends with <code>.zip</code>, otherwise it will export to a directory. You can force either option using the <code>--output-format</code> option.</p> <p>Note</p> <p>Please use the following command for advanced options. <pre><code>java -jar nessie-server-admin-tool-x.y.z-runner.jar help export\n</code></pre></p>"},{"location":"nessie-latest/export_import/#importing","title":"Importing","text":"<p>The following command (replace <code>x.y.z</code> with the version you\u2019re using) imports your Nessie repository from a single ZIP file called <code>my-export-file.zip</code>,</p> <pre><code>java -jar nessie-server-admin-tool-x.y.z-runner.jar import --path my-export-file.zip\n</code></pre> <p>The import will fail, if the target Nessie repository exists and is not empty. If you intentionally want to overwrite an existing Nessie repository, then use the <code>--erase-before-import</code> option.</p> <p>Note</p> <p>Please use the following command for advanced options. <pre><code>java -jar nessie-server-admin-tool-x.y.z-runner.jar help import\n</code></pre></p>"},{"location":"nessie-latest/export_import/#migrating-from-a-legacy-version-store-type","title":"Migrating from a legacy version store type","text":"<p>The admin tool can be used to fully migrate a Nessie repository from one version store type to  another, see Migration for a detailed example.</p>"},{"location":"nessie-latest/export_import/#building-blocks","title":"Building blocks","text":"<ul> <li>Export functionality, based   on AbstractNessieExporter   to dump commits, named references, heads+fork points.</li> <li>Import functionality, based   on AbstractNessieImporter   to load the exported data.</li> <li>Commit log optimization to:</li> <li>populate the list of parent-commits in all commits, according to the target Nessie repository\u2019s     configuration</li> <li>populate the key-lists in the commits, according to the target Nessie repository\u2019s configuration</li> </ul>"},{"location":"nessie-latest/export_import/#code-examples","title":"Code examples","text":"<pre><code>class CodeExamples {\n\n  void exportExample(Persist persist, Path exportZipFile) {\n\n    ZipArchiveExporter.builder()\n      .outputFile(exportZipFile)\n      .persist(persist)\n      .build()\n      .exportNessieRepository();\n  }\n\n  void importExample(Persist persist, Path importZipFile) {\n\n    ImportResult importResult =\n      ZipArchiveImporter.builder()\n        .sourceZipFile(importZipFile)\n        .persist(persist)\n        .build()\n        .importNessieRepository();\n  }\n}\n</code></pre> <p><code>ZipArchiveImporter</code> can be replaced with <code>FileImporter</code>.</p>"},{"location":"nessie-latest/export_import/#export-contents","title":"Export contents","text":"<p>Each export contains this information:</p> <ul> <li>All commits (no specific order)</li> <li>All named references including their heads</li> <li>Heads + fork-points (used to feed commit-log optimization ran after a repository import)</li> <li>Summary and inventory</li> </ul>"},{"location":"nessie-latest/export_import/#content-commitmeta-global-state-et-al","title":"<code>Content</code>, <code>CommitMeta</code>, global state, et al","text":"<p>A Nessie export contains all <code>Content</code> information without any database internal information. This means that there is no information contained whether the source repository stored <code>Content</code> using e.g. global state. All <code>Content</code>s and <code>CommitMeta</code> are exported in their public JSON representation.</p> <p>As a side effect, an export from a Nessie repository with commits that were persisted using global state will be imported using on-reference-state. However, for content that was persisted using global state, there will multiple on-reference-states referring to the same Iceberg table-metadata.</p>"},{"location":"nessie-latest/export_import/#technical-commit-information","title":"Technical commit information","text":"<p>Exported commits do not contain key-lists or commit-parents or the like, because that is rather internal, implementation specific information and, additionally, the configuration of the target repository that controls the aggregated key-lists and commit-parent-lists might be different from the source repository.</p> <p>However, exported commits do contain information about the commit-sequence-number and the technical created-at-timestamp.</p> <p>Note</p> <p>The <code>nessie-server-admin-tool</code>\u2019s <code>import</code> command performs a commit-log optimization after all commits and named references have been created. This optimization populates missing aggregated key-lists and commit-parents. Running commit-log optimization is necessary for good performance to access contents and commit logs, but not strictly necessary. Commit-log optimization can be disabled.</p>"},{"location":"nessie-latest/export_import/#export-contents-consistency","title":"Export contents consistency","text":"<p>Any Nessie export guarantees that the commits referenced by the named references and all their parent commits are contained in the exported data.</p> <p>A Nessie export may contain unreferenced commits, for example commits that have been created while the export is running or commits that are otherwise unreferenced.</p> <p>The <code>HEAD</code>s of the named references and the heads in the <code>HeadsAndForks</code> structure may not be consistent, for example when commits have been created while the export is running.</p>"},{"location":"nessie-latest/export_import/#export-formats","title":"Export formats","text":"<p>Exported data can be written either into an empty directory or as a compressed zip file.</p> <p>Users can optionally zip the contents of an export to a directory and pass that to the zip-file based importer.</p>"},{"location":"nessie-latest/export_import/#command-reference","title":"Command Reference","text":"<pre><code>2025-12-05 15:26:33,820 WARN  [org.hibernate.validator.internal.metadata.aggregated.CascadingMetaDataBuilder] (main) HV000271: Using `@Valid` on a container (java.util.List) is deprecated. You should apply the annotation on the type argument(s). Affected element: ContentService#getMultipleContents(String, String, List, boolean, RequestMeta)\n2025-12-05 15:26:34,173 WARN  [org.hibernate.validator.internal.metadata.aggregated.CascadingMetaDataBuilder] (main) HV000271: Using `@Valid` on a container (java.util.List) is deprecated. You should apply the annotation on the type argument(s). Affected element: ContentService#getMultipleContents(String, String, List, boolean, RequestMeta)\nUsage: nessie-server-admin-tool-runner.jar [-hV] [COMMAND]\nNessie Server Admin Tool\n  -h, --help      Show this help message and exit.\n  -V, --version   Print version information and exit.\nCommands:\n  info                  Nessie repository information\n  help                  Display help information about the specified command.\n  cleanup-repository    Cleanup unreferenced data from Nessie's repository.\n  cut-history           Advanced commit log manipulation command that removes\n                          parents from the specified commit. Read the full help\n                          message before using!\n  check-content         Check content readability of active keys.\n  delete-catalog-tasks  Delete persisted state of Iceberg snapshot loading\n                          tasks previously executed by the Nessie Catalog.\n  erase-repository      Erase current Nessie repository (all data will be lost)\n                          and optionally re-initialize it.\n  export                Exports a Nessie repository to the local file system.\n  import                Imports a Nessie repository from the local file system.\n  show-licenses         Show 3rd party license information.\n</code></pre> <p>Below is the output of the Nessie Server Admin tool help for all commands.</p>"},{"location":"nessie-latest/export_import/#info","title":"<code>info</code>","text":"<pre><code>2025-12-05 15:26:52,442 WARN  [org.hibernate.validator.internal.metadata.aggregated.CascadingMetaDataBuilder] (main) HV000271: Using `@Valid` on a container (java.util.List) is deprecated. You should apply the annotation on the type argument(s). Affected element: ContentService#getMultipleContents(String, String, List, boolean, RequestMeta)\n2025-12-05 15:26:52,882 WARN  [org.hibernate.validator.internal.metadata.aggregated.CascadingMetaDataBuilder] (main) HV000271: Using `@Valid` on a container (java.util.List) is deprecated. You should apply the annotation on the type argument(s). Affected element: ContentService#getMultipleContents(String, String, List, boolean, RequestMeta)\nUsage: nessie-server-admin-tool-runner.jar info [-hV]\nNessie repository information\n  -h, --help      Show this help message and exit.\n  -V, --version   Print version information and exit.\n</code></pre>"},{"location":"nessie-latest/export_import/#check-content","title":"<code>check-content</code>","text":"<pre><code>2025-12-05 15:26:36,263 WARN  [org.hibernate.validator.internal.metadata.aggregated.CascadingMetaDataBuilder] (main) HV000271: Using `@Valid` on a container (java.util.List) is deprecated. You should apply the annotation on the type argument(s). Affected element: ContentService#getMultipleContents(String, String, List, boolean, RequestMeta)\n2025-12-05 15:26:36,617 WARN  [org.hibernate.validator.internal.metadata.aggregated.CascadingMetaDataBuilder] (main) HV000271: Using `@Valid` on a container (java.util.List) is deprecated. You should apply the annotation on the type argument(s). Affected element: ContentService#getMultipleContents(String, String, List, boolean, RequestMeta)\nUsage: nessie-server-admin-tool-runner.jar check-content [-cEhsV]\n       [-B=&lt;batchSize&gt;] [-H=&lt;hash&gt;] [-o=&lt;outputSpec&gt;] [-r=&lt;ref&gt;]\n       [-k=&lt;keyElements&gt;]...\nCheck content readability of active keys.\n  -B, --batch=&lt;batchSize&gt;   The max number of keys to load at the same time.\n                            If an error occurs while loading or parsing the\n                              values for a single key, the error will be\n                              propagated to all keys processed in the same\n                              batch. In such a case, rerun the check for the\n                              affected keys with a batch size of 1.\n  -c, --show-content        Include content for each valid key in the output.\n  -E, --error-only          Produce JSON only for keys with errors.\n  -h, --help                Show this help message and exit.\n  -H, --hash=&lt;hash&gt;         Commit hash to use (defaults to the HEAD of the\n                              specified reference).\n  -k, --key-element=&lt;keyElements&gt;\n                            Elements or a specific content key to check (zero\n                              or more). If not set, all current keys will be\n                              checked.\n  -o, --output=&lt;outputSpec&gt; JSON output file name or '-' for STDOUT. If not\n                              set, per-key status is not reported.\n  -r, --ref=&lt;ref&gt;           Reference name to use (default branch, if not set).\n  -s, --summary             Print a summary of results to STDOUT (irrespective\n                              of the --output option).\n  -V, --version             Print version information and exit.\n</code></pre>"},{"location":"nessie-latest/export_import/#delete-catalog-tasks","title":"<code>delete-catalog-tasks</code>","text":"<pre><code>2025-12-05 15:26:41,484 WARN  [org.hibernate.validator.internal.metadata.aggregated.CascadingMetaDataBuilder] (main) HV000271: Using `@Valid` on a container (java.util.List) is deprecated. You should apply the annotation on the type argument(s). Affected element: ContentService#getMultipleContents(String, String, List, boolean, RequestMeta)\n2025-12-05 15:26:41,947 WARN  [org.hibernate.validator.internal.metadata.aggregated.CascadingMetaDataBuilder] (main) HV000271: Using `@Valid` on a container (java.util.List) is deprecated. You should apply the annotation on the type argument(s). Affected element: ContentService#getMultipleContents(String, String, List, boolean, RequestMeta)\nUsage: nessie-server-admin-tool-runner.jar delete-catalog-tasks [-hV]\n       [-B=&lt;batchSize&gt;] [-H=&lt;hash&gt;] [-r=&lt;ref&gt;] [-k=&lt;keyElements&gt;]...\n       [-s=&lt;statuses&gt;]...\nDelete persisted state of Iceberg snapshot loading tasks previously executed by\nthe Nessie Catalog.\n  -B, --batch=&lt;batchSize&gt;   The max number of task IDs to process at the same\n                              time.\n  -h, --help                Show this help message and exit.\n  -H, --hash=&lt;hash&gt;         Commit hash to use (defaults to the HEAD of the\n                              specified reference).\n  -k, --key-element=&lt;keyElements&gt;\n                            Elements or a specific content key to process (zero\n                              or more). If not set, all current keys will get\n                              their snapshot tasks expired.\n  -r, --ref=&lt;ref&gt;           Reference name to use (default branch, if not set).\n  -s, --task-status=&lt;statuses&gt;\n                            Delete tasks having these statuses (zero or more).\n                              If not set, only failed tasks for matching\n                              content objects are deleted.\n  -V, --version             Print version information and exit.\n</code></pre>"},{"location":"nessie-latest/export_import/#cleanup-repository","title":"<code>cleanup-repository</code>","text":"<pre><code>2025-12-05 15:26:38,763 WARN  [org.hibernate.validator.internal.metadata.aggregated.CascadingMetaDataBuilder] (main) HV000271: Using `@Valid` on a container (java.util.List) is deprecated. You should apply the annotation on the type argument(s). Affected element: ContentService#getMultipleContents(String, String, List, boolean, RequestMeta)\n2025-12-05 15:26:39,210 WARN  [org.hibernate.validator.internal.metadata.aggregated.CascadingMetaDataBuilder] (main) HV000271: Using `@Valid` on a container (java.util.List) is deprecated. You should apply the annotation on the type argument(s). Affected element: ContentService#getMultipleContents(String, String, List, boolean, RequestMeta)\nUsage: nessie-server-admin-tool-runner.jar cleanup-repository [-hV]\n       [--allow-duplicate-commit-traversal] [--dry-run]\n       [--allowed-fpp=&lt;allowedFalsePositiveProbability&gt;]\n       [--commit-rate=&lt;resolveCommitRatePerSecond&gt;]\n       [--fpp=&lt;falsePositiveProbability&gt;] [--obj-count=&lt;expectedObjCount&gt;]\n       [--obj-rate=&lt;resolveObjRatePerSecond&gt;]\n       [--pending-objs-batch-size=&lt;pendingObjsBatchSize&gt;]\n       [--purge-obj-rate=&lt;purgeDeleteObjRatePerSecond&gt;]\n       [--recent-objs-ids-filter-size=&lt;recentObjIdsFilterSize&gt;]\n       [--referenced-grace=&lt;objReferencedGrace&gt;]\n       [--scan-obj-rate=&lt;purgeScanObjRatePerSecond&gt;]\nCleanup unreferenced data from Nessie's repository.\nThis is a two-phase implementation that first identifies the objects that are\nreferenced and the second phase scans the whole repository and deletes objects\nthat are unreferenced.\nIt is recommended to run this command regularly, but with appropriate rate\nlimits using the --commit-rate, --obj-rate, --scan-obj-rate, --purge-obj-rate\nwhich does not overload your backend database system.\nThe implementation uses a bloom-filter to identify the IDs of referenced\nobjects. The default setting is to allow for 1000000 objects in the backend\ndatabase with an FPP of 1.0E-5. These values should serve most repositories.\nHowever, if your repository is quite big, you should supply a higher expected\nobject count using the --obj-count option. If the implementation detected that\nthe bloom-filter would exceed the maximum allowed FPP, it would restart with a\nhigher number of expected objects.\nIn rare situations with an extremely huge amount of objects, the data\nstructures may require a lot of memory. The estimated heap pressure for the\ncontextual data structures is printed to the console.\nIf you are unsure whether this command works fine, specify the --dry-run option\nto perform all operations except deleting objects.\n      --allow-duplicate-commit-traversal\n                  Allow traversal of the same commit more than once. This is\n                    disabled by default.\n      --allowed-fpp=&lt;allowedFalsePositiveProbability&gt;\n                  Maximum allowed false-positive-probability to detect\n                    referenced objects, defaults to 1.0E-4.\n      --commit-rate=&lt;resolveCommitRatePerSecond&gt;\n                  Allowed number of commits to process during the 'resolve'\n                    phase per second. Default is unlimited.\n      --dry-run   Perform all operations, but do not delete any object .\n      --fpp=&lt;falsePositiveProbability&gt;\n                  Default false-positive-probability to detect referenced\n                    objects, defaults to 1.0E-5.\n  -h, --help      Show this help message and exit.\n      --obj-count=&lt;expectedObjCount&gt;\n                  Number of expected objects, defaults to 1000000.\n      --obj-rate=&lt;resolveObjRatePerSecond&gt;\n                  Allowed number of objects to process during the 'resolve'\n                    phase per second. Default is unlimited.\n      --pending-objs-batch-size=&lt;pendingObjsBatchSize&gt;\n\n      --purge-obj-rate=&lt;purgeDeleteObjRatePerSecond&gt;\n                  Allowed number of objects to delete during the 'purge' phase\n                    per second. Default is unlimited.\n      --recent-objs-ids-filter-size=&lt;recentObjIdsFilterSize&gt;\n                  Size of the filter to recognize recently processed objects.\n                    This helps to reduce effort, but should be kept to a\n                    reasonable number. Defaults to 100000.\n      --referenced-grace=&lt;objReferencedGrace&gt;\n                  Grace-time for newly created objects to not be deleted.\n                    Default is just \"now\". Specified using the ISO-8601 format,\n                    for example P1D (24 hours) or PT2H (2 hours) or P10D12H (10\n                    * 24 + 10 hours).\n      --scan-obj-rate=&lt;purgeScanObjRatePerSecond&gt;\n                  Allowed number of objects to scan during the 'purge' phase\n                    per second. Default is unlimited.\n  -V, --version   Print version information and exit.\n</code></pre>"},{"location":"nessie-latest/export_import/#erase-repository","title":"<code>erase-repository</code>","text":"<pre><code>2025-12-05 15:26:44,239 WARN  [org.hibernate.validator.internal.metadata.aggregated.CascadingMetaDataBuilder] (main) HV000271: Using `@Valid` on a container (java.util.List) is deprecated. You should apply the annotation on the type argument(s). Affected element: ContentService#getMultipleContents(String, String, List, boolean, RequestMeta)\n2025-12-05 15:26:44,677 WARN  [org.hibernate.validator.internal.metadata.aggregated.CascadingMetaDataBuilder] (main) HV000271: Using `@Valid` on a container (java.util.List) is deprecated. You should apply the annotation on the type argument(s). Affected element: ContentService#getMultipleContents(String, String, List, boolean, RequestMeta)\nUsage: nessie-server-admin-tool-runner.jar erase-repository [-hV]\n       [--confirmation-code=&lt;confirmationCode&gt;] [-r=&lt;newDefaultBranch&gt;]\nErase current Nessie repository (all data will be lost) and optionally\nre-initialize it.\n      --confirmation-code=&lt;confirmationCode&gt;\n                  Confirmation code for erasing the repository (will be emitted\n                    by this command if not set).\n  -h, --help      Show this help message and exit.\n  -r, --re-initialize=&lt;newDefaultBranch&gt;\n                  Re-initialize the repository after erasure. If set, provides\n                    the default branch name for the new repository.\n  -V, --version   Print version information and exit.\n</code></pre>"},{"location":"nessie-latest/export_import/#export","title":"<code>export</code>","text":"<pre><code>2025-12-05 15:26:47,003 WARN  [org.hibernate.validator.internal.metadata.aggregated.CascadingMetaDataBuilder] (main) HV000271: Using `@Valid` on a container (java.util.List) is deprecated. You should apply the annotation on the type argument(s). Affected element: ContentService#getMultipleContents(String, String, List, boolean, RequestMeta)\n2025-12-05 15:26:47,395 WARN  [org.hibernate.validator.internal.metadata.aggregated.CascadingMetaDataBuilder] (main) HV000271: Using `@Valid` on a container (java.util.List) is deprecated. You should apply the annotation on the type argument(s). Affected element: ContentService#getMultipleContents(String, String, List, boolean, RequestMeta)\nUsage: nessie-server-admin-tool-runner.jar export [-hV] [--full-scan]\n       [-C=&lt;expectedCommitCount&gt;] [--commit-batch-size=&lt;commitBatchSize&gt;]\n       [--content-batch-size=&lt;number&gt;] [--export-version=&lt;exportVersion&gt;]\n       [-F=&lt;output-format&gt;] [--max-file-size=&lt;maxFileSize&gt;]\n       [--output-buffer-size=&lt;outputBufferSize&gt;] -p=&lt;export-to&gt;\n       [--single-branch-current-content=&lt;branch-name&gt;]\n       [--object-resolvers=&lt;genericObjectResolvers&gt;]...\nExports a Nessie repository to the local file system.\n  -C, --expected-commit-count=&lt;expectedCommitCount&gt;\n                           Expected number of commits in the repository,\n                             defaults to 1000000.\n      --commit-batch-size=&lt;commitBatchSize&gt;\n                           Batch size when reading commits and their associated\n                             contents, defaults to 20.\n      --content-batch-size=&lt;number&gt;\n                           Group the specified number of content objects into\n                             each commit at export time. This option is ignored\n                             unless --single-branch-current-content is set. The\n                             default value is 100.\n      --export-version=&lt;exportVersion&gt;\n                           The export version, defaults to 3.\n  -F, --output-format=&lt;output-format&gt;\n                           Explicitly define the output format to use to the\n                             export.\n                           If not specified, the implementation chooses the ZIP\n                             export, if --path ends in .zip, otherwise will use\n                             the directory output format.\n                           Possible values: ZIP, DIRECTORY\n      --full-scan          Export all commits, including those that are no\n                             longer reachable any named reference.Using this\n                             option is _not_ recommended.\n  -h, --help               Show this help message and exit.\n      --max-file-size=&lt;maxFileSize&gt;\n                           Maximum size of a file in bytes inside the export.\n      --object-resolvers=&lt;genericObjectResolvers&gt;\n                           Additional jars that provide\n                             `TransferRelatedObjects` implementations.\n                           Jars can be provided as file paths or as URLs.\n      --output-buffer-size=&lt;outputBufferSize&gt;\n                           Output buffer size, defaults to 32768.\n  -p, --path=&lt;export-to&gt;   The ZIP file or directory to create with the export\n                             contents.\n      --single-branch-current-content=&lt;branch-name&gt;\n                           Export only the most recent contents from the\n                             specified branch.\n  -V, --version            Print version information and exit.\n</code></pre>"},{"location":"nessie-latest/export_import/#import","title":"<code>import</code>","text":"<pre><code>2025-12-05 15:26:49,600 WARN  [org.hibernate.validator.internal.metadata.aggregated.CascadingMetaDataBuilder] (main) HV000271: Using `@Valid` on a container (java.util.List) is deprecated. You should apply the annotation on the type argument(s). Affected element: ContentService#getMultipleContents(String, String, List, boolean, RequestMeta)\n2025-12-05 15:26:50,045 WARN  [org.hibernate.validator.internal.metadata.aggregated.CascadingMetaDataBuilder] (main) HV000271: Using `@Valid` on a container (java.util.List) is deprecated. You should apply the annotation on the type argument(s). Affected element: ContentService#getMultipleContents(String, String, List, boolean, RequestMeta)\nUsage: nessie-server-admin-tool-runner.jar import [-ehV]\n       [--commit-batch-size=&lt;commitBatchSize&gt;]\n       [--input-buffer-size=&lt;inputBufferSize&gt;] -p=&lt;import-from&gt;\nImports a Nessie repository from the local file system.\n      --commit-batch-size=&lt;commitBatchSize&gt;\n                             Batch size when writing commits, defaults to 20.\n  -e, --erase-before-import  Erase an existing repository before the import is\n                               started.\n                             This will delete all previously existing Nessie\n                               data.\n                             Using this option has no effect, if the Nessie\n                               repository does not already exist.\n  -h, --help                 Show this help message and exit.\n      --input-buffer-size=&lt;inputBufferSize&gt;\n                             Input buffer size, defaults to 32768.\n  -p, --path=&lt;import-from&gt;   The ZIP file or directory to read the export from.\n                             If this parameter refers to a file, the import\n                               will assume that it is a ZIP file, otherwise a\n                               directory.\n  -V, --version              Print version information and exit.\n</code></pre>"},{"location":"nessie-latest/export_import/#show-licenses","title":"<code>show-licenses</code>","text":"<pre><code>2025-12-05 15:26:55,207 WARN  [org.hibernate.validator.internal.metadata.aggregated.CascadingMetaDataBuilder] (main) HV000271: Using `@Valid` on a container (java.util.List) is deprecated. You should apply the annotation on the type argument(s). Affected element: ContentService#getMultipleContents(String, String, List, boolean, RequestMeta)\n2025-12-05 15:26:55,677 WARN  [org.hibernate.validator.internal.metadata.aggregated.CascadingMetaDataBuilder] (main) HV000271: Using `@Valid` on a container (java.util.List) is deprecated. You should apply the annotation on the type argument(s). Affected element: ContentService#getMultipleContents(String, String, List, boolean, RequestMeta)\nUsage: nessie-server-admin-tool-runner.jar show-licenses [-hV]\nShow 3rd party license information.\n  -h, --help      Show this help message and exit.\n  -V, --version   Print version information and exit.\n</code></pre>"},{"location":"nessie-latest/gc/","title":"Nessie GC","text":"<p>Nessie GC is a tool to clean up orphaned files in a Nessie repository. It is designed to be run periodically to keep the repository clean and to avoid unnecessary storage costs.</p>"},{"location":"nessie-latest/gc/#requirements","title":"Requirements","text":"<p>The Nessie GC tool is distributed as an uber-jar and requires Java 11 or later to be available on the host where it is running.</p> <p>It is also available as a Docker image, see below for more information.</p> <p>The Nessie GC tool requires a running Nessie server and a JDBC-compliant database. The Nessie server must be reachable from the host where the GC tool is running. The JDBC-compliant database must also be reachable from the host where the GC tool is running. The database is used to store the live content sets and the deferred deletes.</p> <p>Nessie GC has built-in support for PostgreSQL, MariaDB, MySQL (using the MariaDB driver), and H2  databases.</p> <p>Note</p> <p>Although the GC tool can run in in-memory mode, it is recommended to use a persistent database for production use. Any JDBC compliant database can be used, but it must be created and the schema initialized before running the Nessie GC tool.</p>"},{"location":"nessie-latest/gc/#running-the-standalone-uber-jar","title":"Running the standalone uber jar","text":"<p>Check download options in the Nessie download page.</p> <p>To see the available commands and options, run:</p> <pre><code>java -jar nessie-gc.jar --help\n</code></pre> <p>You should see the following output:</p> <pre><code>Usage: nessie-gc.jar [-hV] [COMMAND]\n  -h, --help      Show this help message and exit.\n  -V, --version   Print version information and exit.\nCommands:\n  help                           Display help information about the specified command.\n  mark-live, identify, mark      Run identify-live-content phase of Nessie GC, must not be used\n                                   with the in-memory contents-storage.\n  sweep, expire                  Run expire-files + delete-orphan-files phase of Nessie GC using a\n                                   live-contents-set stored by a previous run of the mark-live\n                                   command, must not be used with the in-memory contents-storage.\n  gc                             Run identify-live-content and expire-files + delete-orphan-files.\n  list                           List existing live-sets, must not be used with the in-memory\n                                   contents-storage.\n  delete                         Delete a live-set, must not be used with the in-memory\n                                   contents-storage.\n  list-deferred                  List files collected as deferred deletes, must not be used with\n                                   the in-memory contents-storage.\n  deferred-deletes               Delete files collected as deferred deletes, must not be used with\n                                   the in-memory contents-storage.\n  show                           Show information of a live-content-set, must not be used with the\n                                   in-memory contents-storage.\n  show-sql-create-schema-script  Print DDL statements to create the schema.\n  create-sql-schema              JDBC schema creation.\n  completion-script              Extracts the command-line completion script.\n  show-licenses                  Show 3rd party license information.\n</code></pre> <p>Info</p> <p>Help for all Nessie GC tool commands are below on this page</p> <p>The following example assumes that you have a Nessie server running at <code>http://localhost:19120</code> and a PostgreSQL instance running at <code>jdbc:postgresql://localhost:5432/nessie_gc</code> with user <code>pguser</code> and password <code>mysecretpassword</code>.</p> <p>Create the database schema if required:</p> <pre><code>java -jar nessie-gc.jar create-sql-schema \\\n  --jdbc-url jdbc:postgresql://localhost:5432/nessie_gc \\\n  --jdbc-user pguser \\\n  --jdbc-password mysecretpassword\n</code></pre> <p>Now we can run the Nessie GC tool:</p> <pre><code>java -jar nessie-gc.jar gc \\\n  --uri http://localhost:19120/api/v2 \\\n  --jdbc \\\n  --jdbc-url jdbc:postgresql://localhost:5432/nessie_gc \\\n  --jdbc-user pguser \\\n  --jdbc-password mysecretpassword\n</code></pre>"},{"location":"nessie-latest/gc/#running-with-docker","title":"Running with Docker","text":"<p>The tool is also available as a Docker image, hosted on GitHub Container Registry. Images are also mirrored to Quay.io.</p> <p>See Docker for more information.</p> <p>For testing purposes, let\u2019s create a JDBC datastore as follows:</p> <pre><code>docker run --rm -e POSTGRES_USER=pguser -e POSTGRES_PASSWORD=mysecretpassword -e POSTGRES_DB=nessie_gc -p 5432:5432 postgres:16.2\n</code></pre> <p>Create the database schema if required:</p> <pre><code>docker run --rm ghcr.io/projectnessie/nessie-gc:0.106.0 create-sql-schema \\\n  --jdbc-url jdbc:postgresql://127.0.0.1:5432/nessie_gc \\\n  --jdbc-user pguser \\\n  --jdbc-password mysecretpassword\n</code></pre> <p>Now we can run the Nessie GC tool:</p> <pre><code>docker run --rm ghcr.io/projectnessie/nessie-gc:0.106.0 gc \\\n  --jdbc-url jdbc:postgresql://127.0.0.1:5432/nessie_gc \\\n  --jdbc-user pguser \\\n  --jdbc-password mysecretpassword\n</code></pre> <p>The GC tool has a great number of options, which can be seen by running <code>docker run --rm ghcr.io/projectnessie/nessie-gc:0.106.0 --help</code>. The main command is <code>gc</code>, which is followed by subcommands and options. Check the available subcommands and options by running <code>docker run --rm ghcr.io/projectnessie/nessie-gc:0.106.0 gc --help</code>.</p>"},{"location":"nessie-latest/gc/#running-with-kubernetes","title":"Running with Kubernetes","text":"<p>The Nessie GC tool can be executed as a Job or a CronJob in a Kubernetes cluster. </p> <p>The following example assumes that you have a Nessie deployment and a PostgreSQL instance, all running in the same cluster and in the same namespace.</p> <p>Create a secret for the database credentials:</p> <pre><code>kubectl create secret generic nessie-gc-credentials \\\n  --from-literal=JDBC_URL=jdbc:postgresql://postgresql:5432/nessie_gc \\\n  --from-literal=JDBC_USER=pguser \\\n  --from-literal=JDBC_PASSWORD=mysecretpassword\n</code></pre> <p>Assuming that the Nessie service is reachable at <code>nessie:19120</code>, create the following Kubernetes job to run the GC tool:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: nessie-gc-job\nspec:\n  template:\n    spec:\n      containers:\n      - name: nessie-gc\n        image: ghcr.io/projectnessie/nessie-gc:0.106.0\n        args: \n          - gc\n          - --uri\n          - http://nessie:19120/api/v2\n          - --jdbc\n          - --jdbc-url\n          - \"\\$(JDBC_URL)\"\n          - --jdbc-user\n          - \"\\$(JDBC_USER)\"\n          - --jdbc-password\n          - \"\\$(JDBC_PASSWORD)\"\n        envFrom:\n        - secretRef:\n            name: nessie-gc-credentials\n      restartPolicy: Never\nEOF\n</code></pre>"},{"location":"nessie-latest/gc/#nessie-gc-tool-commands","title":"Nessie GC Tool commands","text":"<pre><code>Usage: nessie-gc.jar [-hV] [COMMAND]\n  -h, --help      Show this help message and exit.\n  -V, --version   Print version information and exit.\nCommands:\n  help                           Display help information about the specified command.\n  mark-live, identify, mark      Run identify-live-content phase of Nessie GC, must not be used\n                                   with the in-memory contents-storage.\n  sweep, expire                  Run expire-files + delete-orphan-files phase of Nessie GC using a\n                                   live-contents-set stored by a previous run of the mark-live\n                                   command, must not be used with the in-memory contents-storage.\n  gc                             Run identify-live-content and expire-files + delete-orphan-files.\n  list                           List existing live-sets, must not be used with the in-memory\n                                   contents-storage.\n  delete                         Delete a live-set, must not be used with the in-memory\n                                   contents-storage.\n  list-deferred                  List files collected as deferred deletes, must not be used with\n                                   the in-memory contents-storage.\n  deferred-deletes               Delete files collected as deferred deletes, must not be used with\n                                   the in-memory contents-storage.\n  show                           Show information of a live-content-set, must not be used with the\n                                   in-memory contents-storage.\n  show-sql-create-schema-script  Print DDL statements to create the schema.\n  create-sql-schema              JDBC schema creation.\n  completion-script              Extracts the command-line completion script.\n  show-licenses                  Show 3rd party license information.\n</code></pre> <p>Below is the output of the Nessie GC tool help for all commands.</p>"},{"location":"nessie-latest/gc/#mark-live-identify-mark","title":"<code>mark-live</code>, <code>identify</code>, <code>mark</code>","text":"<pre><code>Usage: nessie-gc.jar mark-live [-hV] [-c=&lt;defaultCutoffPolicy&gt;]\n                               [--identify-parallelism=&lt;parallelism&gt;] [--nessie-api=&lt;nessieApi&gt;]\n                               [--nessie-client=&lt;nessieClientName&gt;] [-R=&lt;cutoffPolicyRefTime&gt;]\n                               [--time-zone=&lt;zoneId&gt;] [-u=&lt;nessieUri&gt;]\n                               [--write-live-set-id-to=&lt;liveSetIdFile&gt;] [-C[=&lt;String=String&gt;[,\n                               &lt;String=String&gt;...]...]]... [-o[=&lt;String=String&gt;[,\n                               &lt;String=String&gt;...]...]]... ([--inmemory] | [[--jdbc]\n                               --jdbc-url=&lt;url&gt; [--jdbc-properties[=&lt;String=String&gt;[,\n                               &lt;String=String&gt;...]...]]... [--jdbc-user=&lt;user&gt;]\n                               [--jdbc-password=&lt;password&gt;] [--jdbc-fetch-size[=&lt;fetchSize&gt;]]\n                               [--jdbc-schema=&lt;schemaCreateStrategy&gt;]])\nRun identify-live-content phase of Nessie GC, must not be used with the in-memory contents-storage.\n  -c, --default-cutoff=&lt;defaultCutoffPolicy&gt;\n                             Default cutoff policy. Policies can be one of:\n                             - number of commits as an integer value\n                             - a duration (see java.time.Duration)\n                             - an ISO instant\n                             - 'NONE', means everything's considered as live\n  -C, --cutoff[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             Cutoff policies per reference names. Supplied as a\n                               ref-name-pattern=policy tuple.\n                             Reference name patterns are regular expressions.\n                             Policies can be one of:\n                             - number of commits as an integer value\n                             - a duration (see java.time.Duration)\n                             - an ISO instant\n                             - 'NONE', means everything's considered as live\n  -h, --help                 Show this help message and exit.\n      --identify-parallelism=&lt;parallelism&gt;\n                             Number of Nessie references that can be walked in parallel.\n      --inmemory             Flag whether to use the in-memory contents storage. Prefer a JDBC\n                               storage.\n      --inmemory             Flag whether to use the in-memory contents storage. Prefer a JDBC\n                               storage.\n      --jdbc                 Flag whether to use the JDBC contents storage.\n      --jdbc                 Flag whether to use the JDBC contents storage.\n      --jdbc-fetch-size[=&lt;fetchSize&gt;]\n                             JDBC fetch size, defaults to 100.\n      --jdbc-fetch-size[=&lt;fetchSize&gt;]\n                             JDBC fetch size, defaults to 100.\n      --jdbc-password=&lt;password&gt;\n                             JDBC password used to authenticate the database access.\n      --jdbc-password=&lt;password&gt;\n                             JDBC password used to authenticate the database access.\n      --jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             JDBC parameters.\n      --jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             JDBC parameters.\n      --jdbc-schema=&lt;schemaCreateStrategy&gt;\n                             How to create the database schema. Possible values: CREATE,\n                               DROP_AND_CREATE, CREATE_IF_NOT_EXISTS.\n      --jdbc-schema=&lt;schemaCreateStrategy&gt;\n                             How to create the database schema. Possible values: CREATE,\n                               DROP_AND_CREATE, CREATE_IF_NOT_EXISTS.\n      --jdbc-url=&lt;url&gt;       JDBC URL of the database to connect to.\n      --jdbc-url=&lt;url&gt;       JDBC URL of the database to connect to.\n      --jdbc-user=&lt;user&gt;     JDBC user name used to authenticate the database access.\n      --jdbc-user=&lt;user&gt;     JDBC user name used to authenticate the database access.\n      --nessie-api=&lt;nessieApi&gt;\n                             Class name of the NessieClientBuilder implementation to use, defaults\n                               to HttpClientBuilder suitable for REST. Using this parameter is not\n                               recommended. Prefer the --nessie-client parameter instead.\n      --nessie-client=&lt;nessieClientName&gt;\n                             Name of the Nessie client to use, defaults to HTTP suitable for REST.\n  -o, --nessie-option[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             Parameters to configure the NessieClientBuilder.\n  -R, --cutoff-ref-time=&lt;cutoffPolicyRefTime&gt;\n                             Reference timestamp for durations specified for --cutoff. Defaults to\n                               'now'.\n      --time-zone=&lt;zoneId&gt;   Time zone ID used to show timestamps.\n                             Defaults to system time zone.\n  -u, --uri=&lt;nessieUri&gt;      Nessie API endpoint URI, defaults to http://localhost:19120/api/v2.\n  -V, --version              Print version information and exit.\n      --write-live-set-id-to=&lt;liveSetIdFile&gt;\n                             Optional, the file name to persist the created live-set-id to.\n</code></pre>"},{"location":"nessie-latest/gc/#sweep-expire","title":"<code>sweep</code>, <code>expire</code>","text":"<pre><code>Usage: nessie-gc.jar sweep [-hV] [--[no-]defer-deletes]\n                           [--allowed-fpp=&lt;allowedFalsePositiveProbability&gt;]\n                           [--expected-file-count=&lt;expectedFileCount&gt;]\n                           [--expiry-parallelism=&lt;parallelism&gt;] [--fpp=&lt;falsePositiveProbability&gt;]\n                           [--max-file-modification=&lt;maxFileModificationTime&gt;]\n                           [--time-zone=&lt;zoneId&gt;] [-H=&lt;String=String&gt;[,&lt;String=String&gt;...]]...\n                           [-I=&lt;String=String&gt;[,&lt;String=String&gt;...]]... ([--inmemory] | [[--jdbc]\n                           --jdbc-url=&lt;url&gt; [--jdbc-properties[=&lt;String=String&gt;[,\n                           &lt;String=String&gt;...]...]]... [--jdbc-user=&lt;user&gt;]\n                           [--jdbc-password=&lt;password&gt;] [--jdbc-fetch-size[=&lt;fetchSize&gt;]]\n                           [--jdbc-schema=&lt;schemaCreateStrategy&gt;]]) (-l=&lt;liveSetId&gt; |\n                           -L=&lt;liveSetIdFile&gt;)\nRun expire-files + delete-orphan-files phase of Nessie GC using a live-contents-set stored by a\nprevious run of the mark-live command, must not be used with the in-memory contents-storage.\n      --allowed-fpp=&lt;allowedFalsePositiveProbability&gt;\n                             The worst allowed effective false-positive-probability checked after\n                               the files for a single content have been checked, defaults to 1.0E-4.\n      --[no-]defer-deletes   Identified unused/orphan files are by default immediately deleted.\n                               Using deferred deletion stores the files to be deleted, so the can\n                               be inspected and deleted later. This option is incompatible with\n                               --inmemory.\n      --expected-file-count=&lt;expectedFileCount&gt;\n                             The total number of expected live files for a single content, defaults\n                               to 1000000.\n      --expiry-parallelism=&lt;parallelism&gt;\n                             Number of contents that are checked in parallel.\n      --fpp=&lt;falsePositiveProbability&gt;\n                             The false-positive-probability used to construct the bloom-filter\n                               identifying whether a file is live, defaults to 1.0E-5.\n  -h, --help                 Show this help message and exit.\n  -H, --hadoop=&lt;String=String&gt;[,&lt;String=String&gt;...]\n                             Hadoop configuration option, required when using an Iceberg FileIO\n                               that is not S3FileIO.\n                             The following configuration settings might be required.\n\n                             For S3:\n                             - fs.s3.impl=org.apache.hadoop.fs.s3a.S3AFileSystem\n                             - fs.s3a.access.key\n                             - fs.s3a.secret.key\n                             - fs.s3a.endpoint, if you use an S3 compatible object store like MinIO\n\n                             For GCS:\n                             - fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\n                             - fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.\n                               GoogleHadoopFS\n                             - fs.gs.project.id\n                             - fs.gs.auth.type=USER_CREDENTIALS\n                             - fs.gs.auth.client.id\n                             - fs.gs.auth.client.secret\n                             - fs.gs.auth.refresh.token\n\n                             For ADLS:\n                             - fs.azure.impl=org.apache.hadoop.fs.azure.AzureNativeFileSystemStore\n                             - fs.AbstractFileSystem.azure.impl=org.apache.hadoop.fs.azurebfs.Abfs\n                             - fs.azure.storage.emulator.account.name\n                             - fs.azure.account.auth.type=SharedKey\n                             - fs.azure.account.key.&lt;account&gt;=&lt;base-64-encoded-secret&gt;\n  -I, --iceberg=&lt;String=String&gt;[,&lt;String=String&gt;...]\n                             Iceberg properties used to configure the FileIO.\n                             The following properties are almost always required.\n\n                             For S3:\n                             - s3.access-key-id\n                             - s3.secret-access-key\n                             For non-AWS S3 you need to specify the endpoint and possibly enable\n                               path-style-access:\n                             - s3.endpoint\n                             - s3.path-style-access=true\n\n                             For GCS:\n                             - io-impl=org.apache.iceberg.gcp.gcs.GCSFileIO\n                             - gcs.project-id\n                             - gcs.oauth2.token\n\n                             For ADLS:\n                             - io-impl=org.apache.iceberg.azure.adlsv2.ADLSFileIO\n                             - adls.auth.shared-key.account.name\n                             - adls.auth.shared-key.account.key\n      --inmemory             Flag whether to use the in-memory contents storage. Prefer a JDBC\n                               storage.\n      --inmemory             Flag whether to use the in-memory contents storage. Prefer a JDBC\n                               storage.\n      --jdbc                 Flag whether to use the JDBC contents storage.\n      --jdbc                 Flag whether to use the JDBC contents storage.\n      --jdbc-fetch-size[=&lt;fetchSize&gt;]\n                             JDBC fetch size, defaults to 100.\n      --jdbc-fetch-size[=&lt;fetchSize&gt;]\n                             JDBC fetch size, defaults to 100.\n      --jdbc-password=&lt;password&gt;\n                             JDBC password used to authenticate the database access.\n      --jdbc-password=&lt;password&gt;\n                             JDBC password used to authenticate the database access.\n      --jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             JDBC parameters.\n      --jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             JDBC parameters.\n      --jdbc-schema=&lt;schemaCreateStrategy&gt;\n                             How to create the database schema. Possible values: CREATE,\n                               DROP_AND_CREATE, CREATE_IF_NOT_EXISTS.\n      --jdbc-schema=&lt;schemaCreateStrategy&gt;\n                             How to create the database schema. Possible values: CREATE,\n                               DROP_AND_CREATE, CREATE_IF_NOT_EXISTS.\n      --jdbc-url=&lt;url&gt;       JDBC URL of the database to connect to.\n      --jdbc-url=&lt;url&gt;       JDBC URL of the database to connect to.\n      --jdbc-user=&lt;user&gt;     JDBC user name used to authenticate the database access.\n      --jdbc-user=&lt;user&gt;     JDBC user name used to authenticate the database access.\n  -l, --live-set-id=&lt;liveSetId&gt;\n                             ID of the live content set.\n  -L, --read-live-set-id-from=&lt;liveSetIdFile&gt;\n                             The file to read the live-set-id from.\n      --max-file-modification=&lt;maxFileModificationTime&gt;\n                             The maximum allowed file modification time. Files newer than this\n                               timestamp will not be deleted. Defaults to the created timestamp of\n                               the live-content-set.\n      --time-zone=&lt;zoneId&gt;   Time zone ID used to show timestamps.\n                             Defaults to system time zone.\n  -V, --version              Print version information and exit.\n</code></pre>"},{"location":"nessie-latest/gc/#gc","title":"<code>gc</code>","text":"<pre><code>Usage: nessie-gc.jar gc [-hV] [--[no-]defer-deletes]\n                        [--allowed-fpp=&lt;allowedFalsePositiveProbability&gt;]\n                        [-c=&lt;defaultCutoffPolicy&gt;] [--expected-file-count=&lt;expectedFileCount&gt;]\n                        [--expiry-parallelism=&lt;parallelism&gt;] [--fpp=&lt;falsePositiveProbability&gt;]\n                        [--identify-parallelism=&lt;parallelism&gt;]\n                        [--max-file-modification=&lt;maxFileModificationTime&gt;]\n                        [--nessie-api=&lt;nessieApi&gt;] [--nessie-client=&lt;nessieClientName&gt;]\n                        [-R=&lt;cutoffPolicyRefTime&gt;] [--time-zone=&lt;zoneId&gt;] [-u=&lt;nessieUri&gt;]\n                        [--write-live-set-id-to=&lt;liveSetIdFile&gt;] [-H=&lt;String=String&gt;[,\n                        &lt;String=String&gt;...]]... [-I=&lt;String=String&gt;[,&lt;String=String&gt;...]]... [-C\n                        [=&lt;String=String&gt;[,&lt;String=String&gt;...]...]]... [-o[=&lt;String=String&gt;[,\n                        &lt;String=String&gt;...]...]]... ([--inmemory] | [[--jdbc] --jdbc-url=&lt;url&gt;\n                        [--jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]]...\n                        [--jdbc-user=&lt;user&gt;] [--jdbc-password=&lt;password&gt;] [--jdbc-fetch-size\n                        [=&lt;fetchSize&gt;]] [--jdbc-schema=&lt;schemaCreateStrategy&gt;]])\nRun identify-live-content and expire-files + delete-orphan-files.\nThis is the same as running a 'mark-live' + a 'sweep' command, but this variant works with the\nin-memory contents storage.\n      --allowed-fpp=&lt;allowedFalsePositiveProbability&gt;\n                             The worst allowed effective false-positive-probability checked after\n                               the files for a single content have been checked, defaults to 1.0E-4.\n  -c, --default-cutoff=&lt;defaultCutoffPolicy&gt;\n                             Default cutoff policy. Policies can be one of:\n                             - number of commits as an integer value\n                             - a duration (see java.time.Duration)\n                             - an ISO instant\n                             - 'NONE', means everything's considered as live\n  -C, --cutoff[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             Cutoff policies per reference names. Supplied as a\n                               ref-name-pattern=policy tuple.\n                             Reference name patterns are regular expressions.\n                             Policies can be one of:\n                             - number of commits as an integer value\n                             - a duration (see java.time.Duration)\n                             - an ISO instant\n                             - 'NONE', means everything's considered as live\n      --[no-]defer-deletes   Identified unused/orphan files are by default immediately deleted.\n                               Using deferred deletion stores the files to be deleted, so the can\n                               be inspected and deleted later. This option is incompatible with\n                               --inmemory.\n      --expected-file-count=&lt;expectedFileCount&gt;\n                             The total number of expected live files for a single content, defaults\n                               to 1000000.\n      --expiry-parallelism=&lt;parallelism&gt;\n                             Number of contents that are checked in parallel.\n      --fpp=&lt;falsePositiveProbability&gt;\n                             The false-positive-probability used to construct the bloom-filter\n                               identifying whether a file is live, defaults to 1.0E-5.\n  -h, --help                 Show this help message and exit.\n  -H, --hadoop=&lt;String=String&gt;[,&lt;String=String&gt;...]\n                             Hadoop configuration option, required when using an Iceberg FileIO\n                               that is not S3FileIO.\n                             The following configuration settings might be required.\n\n                             For S3:\n                             - fs.s3.impl=org.apache.hadoop.fs.s3a.S3AFileSystem\n                             - fs.s3a.access.key\n                             - fs.s3a.secret.key\n                             - fs.s3a.endpoint, if you use an S3 compatible object store like MinIO\n\n                             For GCS:\n                             - fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\n                             - fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.\n                               GoogleHadoopFS\n                             - fs.gs.project.id\n                             - fs.gs.auth.type=USER_CREDENTIALS\n                             - fs.gs.auth.client.id\n                             - fs.gs.auth.client.secret\n                             - fs.gs.auth.refresh.token\n\n                             For ADLS:\n                             - fs.azure.impl=org.apache.hadoop.fs.azure.AzureNativeFileSystemStore\n                             - fs.AbstractFileSystem.azure.impl=org.apache.hadoop.fs.azurebfs.Abfs\n                             - fs.azure.storage.emulator.account.name\n                             - fs.azure.account.auth.type=SharedKey\n                             - fs.azure.account.key.&lt;account&gt;=&lt;base-64-encoded-secret&gt;\n  -I, --iceberg=&lt;String=String&gt;[,&lt;String=String&gt;...]\n                             Iceberg properties used to configure the FileIO.\n                             The following properties are almost always required.\n\n                             For S3:\n                             - s3.access-key-id\n                             - s3.secret-access-key\n                             For non-AWS S3 you need to specify the endpoint and possibly enable\n                               path-style-access:\n                             - s3.endpoint\n                             - s3.path-style-access=true\n\n                             For GCS:\n                             - io-impl=org.apache.iceberg.gcp.gcs.GCSFileIO\n                             - gcs.project-id\n                             - gcs.oauth2.token\n\n                             For ADLS:\n                             - io-impl=org.apache.iceberg.azure.adlsv2.ADLSFileIO\n                             - adls.auth.shared-key.account.name\n                             - adls.auth.shared-key.account.key\n      --identify-parallelism=&lt;parallelism&gt;\n                             Number of Nessie references that can be walked in parallel.\n      --inmemory             Flag whether to use the in-memory contents storage. Prefer a JDBC\n                               storage.\n      --inmemory             Flag whether to use the in-memory contents storage. Prefer a JDBC\n                               storage.\n      --jdbc                 Flag whether to use the JDBC contents storage.\n      --jdbc                 Flag whether to use the JDBC contents storage.\n      --jdbc-fetch-size[=&lt;fetchSize&gt;]\n                             JDBC fetch size, defaults to 100.\n      --jdbc-fetch-size[=&lt;fetchSize&gt;]\n                             JDBC fetch size, defaults to 100.\n      --jdbc-password=&lt;password&gt;\n                             JDBC password used to authenticate the database access.\n      --jdbc-password=&lt;password&gt;\n                             JDBC password used to authenticate the database access.\n      --jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             JDBC parameters.\n      --jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             JDBC parameters.\n      --jdbc-schema=&lt;schemaCreateStrategy&gt;\n                             How to create the database schema. Possible values: CREATE,\n                               DROP_AND_CREATE, CREATE_IF_NOT_EXISTS.\n      --jdbc-schema=&lt;schemaCreateStrategy&gt;\n                             How to create the database schema. Possible values: CREATE,\n                               DROP_AND_CREATE, CREATE_IF_NOT_EXISTS.\n      --jdbc-url=&lt;url&gt;       JDBC URL of the database to connect to.\n      --jdbc-url=&lt;url&gt;       JDBC URL of the database to connect to.\n      --jdbc-user=&lt;user&gt;     JDBC user name used to authenticate the database access.\n      --jdbc-user=&lt;user&gt;     JDBC user name used to authenticate the database access.\n      --max-file-modification=&lt;maxFileModificationTime&gt;\n                             The maximum allowed file modification time. Files newer than this\n                               timestamp will not be deleted. Defaults to the created timestamp of\n                               the live-content-set.\n      --nessie-api=&lt;nessieApi&gt;\n                             Class name of the NessieClientBuilder implementation to use, defaults\n                               to HttpClientBuilder suitable for REST. Using this parameter is not\n                               recommended. Prefer the --nessie-client parameter instead.\n      --nessie-client=&lt;nessieClientName&gt;\n                             Name of the Nessie client to use, defaults to HTTP suitable for REST.\n  -o, --nessie-option[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             Parameters to configure the NessieClientBuilder.\n  -R, --cutoff-ref-time=&lt;cutoffPolicyRefTime&gt;\n                             Reference timestamp for durations specified for --cutoff. Defaults to\n                               'now'.\n      --time-zone=&lt;zoneId&gt;   Time zone ID used to show timestamps.\n                             Defaults to system time zone.\n  -u, --uri=&lt;nessieUri&gt;      Nessie API endpoint URI, defaults to http://localhost:19120/api/v2.\n  -V, --version              Print version information and exit.\n      --write-live-set-id-to=&lt;liveSetIdFile&gt;\n                             Optional, the file name to persist the created live-set-id to.\n</code></pre>"},{"location":"nessie-latest/gc/#list","title":"<code>list</code>","text":"<pre><code>Usage: nessie-gc.jar list [-hV] [--time-zone=&lt;zoneId&gt;] ([--inmemory] | [[--jdbc] --jdbc-url=&lt;url&gt;\n                          [--jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]]...\n                          [--jdbc-user=&lt;user&gt;] [--jdbc-password=&lt;password&gt;] [--jdbc-fetch-size\n                          [=&lt;fetchSize&gt;]] [--jdbc-schema=&lt;schemaCreateStrategy&gt;]])\nList existing live-sets, must not be used with the in-memory contents-storage.\n  -h, --help                 Show this help message and exit.\n      --inmemory             Flag whether to use the in-memory contents storage. Prefer a JDBC\n                               storage.\n      --inmemory             Flag whether to use the in-memory contents storage. Prefer a JDBC\n                               storage.\n      --jdbc                 Flag whether to use the JDBC contents storage.\n      --jdbc                 Flag whether to use the JDBC contents storage.\n      --jdbc-fetch-size[=&lt;fetchSize&gt;]\n                             JDBC fetch size, defaults to 100.\n      --jdbc-fetch-size[=&lt;fetchSize&gt;]\n                             JDBC fetch size, defaults to 100.\n      --jdbc-password=&lt;password&gt;\n                             JDBC password used to authenticate the database access.\n      --jdbc-password=&lt;password&gt;\n                             JDBC password used to authenticate the database access.\n      --jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             JDBC parameters.\n      --jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             JDBC parameters.\n      --jdbc-schema=&lt;schemaCreateStrategy&gt;\n                             How to create the database schema. Possible values: CREATE,\n                               DROP_AND_CREATE, CREATE_IF_NOT_EXISTS.\n      --jdbc-schema=&lt;schemaCreateStrategy&gt;\n                             How to create the database schema. Possible values: CREATE,\n                               DROP_AND_CREATE, CREATE_IF_NOT_EXISTS.\n      --jdbc-url=&lt;url&gt;       JDBC URL of the database to connect to.\n      --jdbc-url=&lt;url&gt;       JDBC URL of the database to connect to.\n      --jdbc-user=&lt;user&gt;     JDBC user name used to authenticate the database access.\n      --jdbc-user=&lt;user&gt;     JDBC user name used to authenticate the database access.\n      --time-zone=&lt;zoneId&gt;   Time zone ID used to show timestamps.\n                             Defaults to system time zone.\n  -V, --version              Print version information and exit.\n</code></pre>"},{"location":"nessie-latest/gc/#delete","title":"<code>delete</code>","text":"<pre><code>Usage: nessie-gc.jar delete [-hV] [--time-zone=&lt;zoneId&gt;] ([--inmemory] | [[--jdbc] --jdbc-url=&lt;url&gt;\n                            [--jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]]...\n                            [--jdbc-user=&lt;user&gt;] [--jdbc-password=&lt;password&gt;] [--jdbc-fetch-size\n                            [=&lt;fetchSize&gt;]] [--jdbc-schema=&lt;schemaCreateStrategy&gt;]])\n                            (-l=&lt;liveSetId&gt; | -L=&lt;liveSetIdFile&gt;)\nDelete a live-set, must not be used with the in-memory contents-storage.\n  -h, --help                 Show this help message and exit.\n      --inmemory             Flag whether to use the in-memory contents storage. Prefer a JDBC\n                               storage.\n      --inmemory             Flag whether to use the in-memory contents storage. Prefer a JDBC\n                               storage.\n      --jdbc                 Flag whether to use the JDBC contents storage.\n      --jdbc                 Flag whether to use the JDBC contents storage.\n      --jdbc-fetch-size[=&lt;fetchSize&gt;]\n                             JDBC fetch size, defaults to 100.\n      --jdbc-fetch-size[=&lt;fetchSize&gt;]\n                             JDBC fetch size, defaults to 100.\n      --jdbc-password=&lt;password&gt;\n                             JDBC password used to authenticate the database access.\n      --jdbc-password=&lt;password&gt;\n                             JDBC password used to authenticate the database access.\n      --jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             JDBC parameters.\n      --jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             JDBC parameters.\n      --jdbc-schema=&lt;schemaCreateStrategy&gt;\n                             How to create the database schema. Possible values: CREATE,\n                               DROP_AND_CREATE, CREATE_IF_NOT_EXISTS.\n      --jdbc-schema=&lt;schemaCreateStrategy&gt;\n                             How to create the database schema. Possible values: CREATE,\n                               DROP_AND_CREATE, CREATE_IF_NOT_EXISTS.\n      --jdbc-url=&lt;url&gt;       JDBC URL of the database to connect to.\n      --jdbc-url=&lt;url&gt;       JDBC URL of the database to connect to.\n      --jdbc-user=&lt;user&gt;     JDBC user name used to authenticate the database access.\n      --jdbc-user=&lt;user&gt;     JDBC user name used to authenticate the database access.\n  -l, --live-set-id=&lt;liveSetId&gt;\n                             ID of the live content set.\n  -L, --read-live-set-id-from=&lt;liveSetIdFile&gt;\n                             The file to read the live-set-id from.\n      --time-zone=&lt;zoneId&gt;   Time zone ID used to show timestamps.\n                             Defaults to system time zone.\n  -V, --version              Print version information and exit.\n</code></pre>"},{"location":"nessie-latest/gc/#list-deferred","title":"<code>list-deferred</code>","text":"<pre><code>Usage: nessie-gc.jar list-deferred [-hV] [--time-zone=&lt;zoneId&gt;] ([--inmemory] | [[--jdbc]\n                                   --jdbc-url=&lt;url&gt; [--jdbc-properties[=&lt;String=String&gt;[,\n                                   &lt;String=String&gt;...]...]]... [--jdbc-user=&lt;user&gt;]\n                                   [--jdbc-password=&lt;password&gt;] [--jdbc-fetch-size[=&lt;fetchSize&gt;]]\n                                   [--jdbc-schema=&lt;schemaCreateStrategy&gt;]]) (-l=&lt;liveSetId&gt; |\n                                   -L=&lt;liveSetIdFile&gt;)\nList files collected as deferred deletes, must not be used with the in-memory contents-storage.\n  -h, --help                 Show this help message and exit.\n      --inmemory             Flag whether to use the in-memory contents storage. Prefer a JDBC\n                               storage.\n      --inmemory             Flag whether to use the in-memory contents storage. Prefer a JDBC\n                               storage.\n      --jdbc                 Flag whether to use the JDBC contents storage.\n      --jdbc                 Flag whether to use the JDBC contents storage.\n      --jdbc-fetch-size[=&lt;fetchSize&gt;]\n                             JDBC fetch size, defaults to 100.\n      --jdbc-fetch-size[=&lt;fetchSize&gt;]\n                             JDBC fetch size, defaults to 100.\n      --jdbc-password=&lt;password&gt;\n                             JDBC password used to authenticate the database access.\n      --jdbc-password=&lt;password&gt;\n                             JDBC password used to authenticate the database access.\n      --jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             JDBC parameters.\n      --jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             JDBC parameters.\n      --jdbc-schema=&lt;schemaCreateStrategy&gt;\n                             How to create the database schema. Possible values: CREATE,\n                               DROP_AND_CREATE, CREATE_IF_NOT_EXISTS.\n      --jdbc-schema=&lt;schemaCreateStrategy&gt;\n                             How to create the database schema. Possible values: CREATE,\n                               DROP_AND_CREATE, CREATE_IF_NOT_EXISTS.\n      --jdbc-url=&lt;url&gt;       JDBC URL of the database to connect to.\n      --jdbc-url=&lt;url&gt;       JDBC URL of the database to connect to.\n      --jdbc-user=&lt;user&gt;     JDBC user name used to authenticate the database access.\n      --jdbc-user=&lt;user&gt;     JDBC user name used to authenticate the database access.\n  -l, --live-set-id=&lt;liveSetId&gt;\n                             ID of the live content set.\n  -L, --read-live-set-id-from=&lt;liveSetIdFile&gt;\n                             The file to read the live-set-id from.\n      --time-zone=&lt;zoneId&gt;   Time zone ID used to show timestamps.\n                             Defaults to system time zone.\n  -V, --version              Print version information and exit.\n</code></pre>"},{"location":"nessie-latest/gc/#deferred-deletes","title":"<code>deferred-deletes</code>","text":"<pre><code>Usage: nessie-gc.jar deferred-deletes [-hV] [--time-zone=&lt;zoneId&gt;] [-H=&lt;String=String&gt;[,\n                                      &lt;String=String&gt;...]]... [-I=&lt;String=String&gt;[,\n                                      &lt;String=String&gt;...]]... ([--inmemory] | [[--jdbc]\n                                      --jdbc-url=&lt;url&gt; [--jdbc-properties[=&lt;String=String&gt;[,\n                                      &lt;String=String&gt;...]...]]... [--jdbc-user=&lt;user&gt;]\n                                      [--jdbc-password=&lt;password&gt;] [--jdbc-fetch-size\n                                      [=&lt;fetchSize&gt;]] [--jdbc-schema=&lt;schemaCreateStrategy&gt;]])\n                                      (-l=&lt;liveSetId&gt; | -L=&lt;liveSetIdFile&gt;)\nDelete files collected as deferred deletes, must not be used with the in-memory contents-storage.\n  -h, --help                 Show this help message and exit.\n  -H, --hadoop=&lt;String=String&gt;[,&lt;String=String&gt;...]\n                             Hadoop configuration option, required when using an Iceberg FileIO\n                               that is not S3FileIO.\n                             The following configuration settings might be required.\n\n                             For S3:\n                             - fs.s3.impl=org.apache.hadoop.fs.s3a.S3AFileSystem\n                             - fs.s3a.access.key\n                             - fs.s3a.secret.key\n                             - fs.s3a.endpoint, if you use an S3 compatible object store like MinIO\n\n                             For GCS:\n                             - fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\n                             - fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.\n                               GoogleHadoopFS\n                             - fs.gs.project.id\n                             - fs.gs.auth.type=USER_CREDENTIALS\n                             - fs.gs.auth.client.id\n                             - fs.gs.auth.client.secret\n                             - fs.gs.auth.refresh.token\n\n                             For ADLS:\n                             - fs.azure.impl=org.apache.hadoop.fs.azure.AzureNativeFileSystemStore\n                             - fs.AbstractFileSystem.azure.impl=org.apache.hadoop.fs.azurebfs.Abfs\n                             - fs.azure.storage.emulator.account.name\n                             - fs.azure.account.auth.type=SharedKey\n                             - fs.azure.account.key.&lt;account&gt;=&lt;base-64-encoded-secret&gt;\n  -I, --iceberg=&lt;String=String&gt;[,&lt;String=String&gt;...]\n                             Iceberg properties used to configure the FileIO.\n                             The following properties are almost always required.\n\n                             For S3:\n                             - s3.access-key-id\n                             - s3.secret-access-key\n                             For non-AWS S3 you need to specify the endpoint and possibly enable\n                               path-style-access:\n                             - s3.endpoint\n                             - s3.path-style-access=true\n\n                             For GCS:\n                             - io-impl=org.apache.iceberg.gcp.gcs.GCSFileIO\n                             - gcs.project-id\n                             - gcs.oauth2.token\n\n                             For ADLS:\n                             - io-impl=org.apache.iceberg.azure.adlsv2.ADLSFileIO\n                             - adls.auth.shared-key.account.name\n                             - adls.auth.shared-key.account.key\n      --inmemory             Flag whether to use the in-memory contents storage. Prefer a JDBC\n                               storage.\n      --inmemory             Flag whether to use the in-memory contents storage. Prefer a JDBC\n                               storage.\n      --jdbc                 Flag whether to use the JDBC contents storage.\n      --jdbc                 Flag whether to use the JDBC contents storage.\n      --jdbc-fetch-size[=&lt;fetchSize&gt;]\n                             JDBC fetch size, defaults to 100.\n      --jdbc-fetch-size[=&lt;fetchSize&gt;]\n                             JDBC fetch size, defaults to 100.\n      --jdbc-password=&lt;password&gt;\n                             JDBC password used to authenticate the database access.\n      --jdbc-password=&lt;password&gt;\n                             JDBC password used to authenticate the database access.\n      --jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             JDBC parameters.\n      --jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             JDBC parameters.\n      --jdbc-schema=&lt;schemaCreateStrategy&gt;\n                             How to create the database schema. Possible values: CREATE,\n                               DROP_AND_CREATE, CREATE_IF_NOT_EXISTS.\n      --jdbc-schema=&lt;schemaCreateStrategy&gt;\n                             How to create the database schema. Possible values: CREATE,\n                               DROP_AND_CREATE, CREATE_IF_NOT_EXISTS.\n      --jdbc-url=&lt;url&gt;       JDBC URL of the database to connect to.\n      --jdbc-url=&lt;url&gt;       JDBC URL of the database to connect to.\n      --jdbc-user=&lt;user&gt;     JDBC user name used to authenticate the database access.\n      --jdbc-user=&lt;user&gt;     JDBC user name used to authenticate the database access.\n  -l, --live-set-id=&lt;liveSetId&gt;\n                             ID of the live content set.\n  -L, --read-live-set-id-from=&lt;liveSetIdFile&gt;\n                             The file to read the live-set-id from.\n      --time-zone=&lt;zoneId&gt;   Time zone ID used to show timestamps.\n                             Defaults to system time zone.\n  -V, --version              Print version information and exit.\n</code></pre>"},{"location":"nessie-latest/gc/#show","title":"<code>show</code>","text":"<pre><code>Usage: nessie-gc.jar show [-BCDhV] [--time-zone=&lt;zoneId&gt;] ([--inmemory] | [[--jdbc]\n                          --jdbc-url=&lt;url&gt; [--jdbc-properties[=&lt;String=String&gt;[,\n                          &lt;String=String&gt;...]...]]... [--jdbc-user=&lt;user&gt;]\n                          [--jdbc-password=&lt;password&gt;] [--jdbc-fetch-size[=&lt;fetchSize&gt;]]\n                          [--jdbc-schema=&lt;schemaCreateStrategy&gt;]]) (-l=&lt;liveSetId&gt; |\n                          -L=&lt;liveSetIdFile&gt;)\nShow information of a live-content-set, must not be used with the in-memory contents-storage.\n  -B, --with-base-locations  Show base locations.\n  -C, --with-content-references\n                             Show content references.\n  -D, --with-deferred-deletes\n                             Show deferred deletes.\n  -h, --help                 Show this help message and exit.\n      --inmemory             Flag whether to use the in-memory contents storage. Prefer a JDBC\n                               storage.\n      --inmemory             Flag whether to use the in-memory contents storage. Prefer a JDBC\n                               storage.\n      --jdbc                 Flag whether to use the JDBC contents storage.\n      --jdbc                 Flag whether to use the JDBC contents storage.\n      --jdbc-fetch-size[=&lt;fetchSize&gt;]\n                             JDBC fetch size, defaults to 100.\n      --jdbc-fetch-size[=&lt;fetchSize&gt;]\n                             JDBC fetch size, defaults to 100.\n      --jdbc-password=&lt;password&gt;\n                             JDBC password used to authenticate the database access.\n      --jdbc-password=&lt;password&gt;\n                             JDBC password used to authenticate the database access.\n      --jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             JDBC parameters.\n      --jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                             JDBC parameters.\n      --jdbc-schema=&lt;schemaCreateStrategy&gt;\n                             How to create the database schema. Possible values: CREATE,\n                               DROP_AND_CREATE, CREATE_IF_NOT_EXISTS.\n      --jdbc-schema=&lt;schemaCreateStrategy&gt;\n                             How to create the database schema. Possible values: CREATE,\n                               DROP_AND_CREATE, CREATE_IF_NOT_EXISTS.\n      --jdbc-url=&lt;url&gt;       JDBC URL of the database to connect to.\n      --jdbc-url=&lt;url&gt;       JDBC URL of the database to connect to.\n      --jdbc-user=&lt;user&gt;     JDBC user name used to authenticate the database access.\n      --jdbc-user=&lt;user&gt;     JDBC user name used to authenticate the database access.\n  -l, --live-set-id=&lt;liveSetId&gt;\n                             ID of the live content set.\n  -L, --read-live-set-id-from=&lt;liveSetIdFile&gt;\n                             The file to read the live-set-id from.\n      --time-zone=&lt;zoneId&gt;   Time zone ID used to show timestamps.\n                             Defaults to system time zone.\n  -V, --version              Print version information and exit.\n</code></pre>"},{"location":"nessie-latest/gc/#show-sql-create-schema-script","title":"<code>show-sql-create-schema-script</code>","text":"<pre><code>Usage: nessie-gc.jar show-sql-create-schema-script [-hV] [--output-file=&lt;outputFile&gt;]\nPrint DDL statements to create the schema.\n  -h, --help      Show this help message and exit.\n      --output-file=&lt;outputFile&gt;\n\n  -V, --version   Print version information and exit.\n</code></pre>"},{"location":"nessie-latest/gc/#create-sql-schema","title":"<code>create-sql-schema</code>","text":"<pre><code>Usage: nessie-gc.jar create-sql-schema [-hV] ([--jdbc] --jdbc-url=&lt;url&gt; [--jdbc-properties\n                                       [=&lt;String=String&gt;[,&lt;String=String&gt;...]...]]...\n                                       [--jdbc-user=&lt;user&gt;] [--jdbc-password=&lt;password&gt;]\n                                       [--jdbc-fetch-size[=&lt;fetchSize&gt;]]\n                                       [--jdbc-schema=&lt;schemaCreateStrategy&gt;])\nJDBC schema creation.\n  -h, --help               Show this help message and exit.\n      --jdbc               Flag whether to use the JDBC contents storage.\n      --jdbc-fetch-size[=&lt;fetchSize&gt;]\n                           JDBC fetch size, defaults to 100.\n      --jdbc-password=&lt;password&gt;\n                           JDBC password used to authenticate the database access.\n      --jdbc-properties[=&lt;String=String&gt;[,&lt;String=String&gt;...]...]\n                           JDBC parameters.\n      --jdbc-schema=&lt;schemaCreateStrategy&gt;\n                           How to create the database schema. Possible values: CREATE,\n                             DROP_AND_CREATE, CREATE_IF_NOT_EXISTS.\n      --jdbc-url=&lt;url&gt;     JDBC URL of the database to connect to.\n      --jdbc-user=&lt;user&gt;   JDBC user name used to authenticate the database access.\n  -V, --version            Print version information and exit.\n</code></pre>"},{"location":"nessie-latest/gc/#completion-script","title":"<code>completion-script</code>","text":"<pre><code>Usage: nessie-gc.jar completion-script [-hV] -O=&lt;outputFile&gt;\nExtracts the command-line completion script.\n  -h, --help      Show this help message and exit.\n  -O, --output-file=&lt;outputFile&gt;\n                  Completion script file name.\n  -V, --version   Print version information and exit.\n</code></pre>"},{"location":"nessie-latest/gc/#show-licenses","title":"<code>show-licenses</code>","text":"<pre><code>Usage: nessie-gc.jar show-licenses [-hV]\nShow 3rd party license information.\n  -h, --help      Show this help message and exit.\n  -V, --version   Print version information and exit.\n</code></pre>"},{"location":"nessie-latest/gc/#nessie-gc-for-nessie-administrators","title":"Nessie GC for Nessie Administrators","text":"<p>Please refer to the Garbage Collection documentation for information on how to run the Nessie GC on a regular basis in production.</p>"},{"location":"nessie-latest/gc/#nessie-gc-internals","title":"Nessie GC Internals","text":"<p>The rest of this document describes the internals of the Nessie GC tool and is intended for developers who want to understand how the tool works.</p> <p>The GC tool consists of a <code>gc-base</code> module, which contains the general base functionality to access a repository to identify the live contents, to identify the live files, to list the existing files and to purge orphan files.</p> <p>Modules that supplement the <code>gc-base</code> module:</p> <ul> <li><code>gc-iceberg</code> implements the Iceberg table-format specific functionality.</li> <li><code>gc-iceberg-files</code> implements file listing + deletion using Iceberg\u2019s <code>FileIO</code>.</li> <li><code>gc-iceberg-mock</code> is a testing-only module to generate mock metadata, manifest-lists, manifests   and (empty) data files.</li> <li><code>gc-repository-jdbc</code> implements the live-content-sets-store using JDBC (PostgreSQL, MariaDB, MySQL    and any other compatible database).</li> <li><code>s3mock</code> is a testing-only module containing a S3 mock backend that allows listing objects and   getting objects programmatically.</li> <li><code>s3mino</code> is a junit 5 test extension providing a Minio based S3 backend.</li> </ul> <p>The <code>gc-tool</code> module is a command-line interface, a standalone tool provided as an executable, it is an uber jar prefixed with a shell script, and can still be executed with <code>java -jar ...</code>.</p>"},{"location":"nessie-latest/gc/#basic-nessie-gc-functionality","title":"Basic Nessie-GC functionality","text":"<p>Nessie-GC implements a mark-and-sweep approach, a two-phase process:</p> <p>The \u201cmark phase\u201d, or \u201clive content identification\u201d, walks all named references and collects references to all <code>Content</code> objects that are considered as live. Those references are stored in a repository as a \u201clive contents set\u201d. The \u201cmark phase\u201d is implemented in <code>IdentifyLiveContents</code>.</p> <p>The \u201csweep phase\u201d, or \u201cdelete orphan files\u201d, operates per content-id. For each content, all live versions of a <code>Content</code> are scanned to identify the set of live data files. After that, the base-location(s)  are scanned and all files that are not in the set of live data files are deleted. The \u201csweep phase\u201d is implemented by <code>DefaultLocalExpire</code>.</p>"},{"location":"nessie-latest/gc/#inner-workings","title":"Inner workings","text":"<p>To minimize the amount of data needed to match against the set of live data files for a <code>Content</code>, the implementation does not actually remember all individual data files, like maintaining a <code>java.util.Set</code> of all those data files, but remembers all data files in a bloom filter.</p> <p>Both the \u201cmark\u201d (identify live contents) and \u201csweep\u201d (identify and delete expired contents) phases provide a configurable parallelism: the number of concurrently scanned named references can be configured and the amount of concurrently processed tables can be configured.</p>"},{"location":"nessie-latest/gc/#mark-phase-optimization","title":"Mark phase optimization","text":"<p>The implementation that walks the commit logs can be configured with a <code>VisitedDeduplicator</code>, which is meant to reduce the work required during the \u201cmark\u201d phase, if the commit to be examined has already been processed.</p> <p>There is a <code>DefaultVisitedDeduplicator</code> implementation, but it is likely that it requires too much memory during runtime, especially when the identify-run is configured with multiple GC policies and/or has to walk many commits. This <code>DefaultVisitedDeduplicator</code> is present, but due to the mentioned concerns not available in the Nessie GC tool and the use of <code>DefaultVisitedDeduplicator</code> is not supported at all, and not recommended.</p>"},{"location":"nessie-latest/gc/#identified-live-contents-repository","title":"Identified live contents repository","text":"<p>It is recommended to use an external database for the Nessie GC repository. This is especially recommended for big Nessie repositories.</p> <p>Nessie GC runs against small-ish repositories do technically work with an in-memory repository. But, as the term \u201cin memory\u201d suggests, the identified live-contents-set, its state, duration, etc. cannot be inspected afterwards.</p>"},{"location":"nessie-latest/gc/#pluggable-code","title":"Pluggable code","text":"<p>Different parts / functionalities are quite isolated and abstracted to allow proper unit-testability and also allow reuse of similar functionality.</p> <p>Examples of abstracted/isolated functionality:</p> <ul> <li>Functionality to recursively walk a base location</li> <li>Functionality to delete files</li> <li>Nessie GC repository</li> <li>Getting all data files for a specific content reference (think: Iceberg table snapshot)</li> <li>Commit-log-scanning duplicate work elimination</li> </ul>"},{"location":"nessie-latest/gc/#file-references","title":"File references","text":"<p>All files (or objects, in case of an object store like S3) are described using a <code>FileReference</code>, using a base URI plus a URI relative to the base URI. Noteworthy: the \u201csweep phase\u201d, which \u201ccollects\u201d all live files in a bloom filter and after that lists files in all base URIs, always uses only the relative URI, never the full URI, to check whether a file is orphan or probably not (bloom filter is probabilistic data structure).</p> <p>Since object stores are the primary target, only files but not directories are supported. Object stores do not know about directories, further Iceberg\u2019s <code>FileIO</code> does not know about directories either. For file systems that do support directories this means, that empty directories will not be deleted, and prematurely deleting directories could break concurrent operations.</p>"},{"location":"nessie-latest/gc/#runtime-requirements","title":"Runtime requirements","text":"<p>Nessie GC work is dominated by network and/or disk I/O, less by CPU and heap pressure.</p> <p>Memory requirements (rough estimates):</p> <ul> <li>Number of concurrent content-scans (\u201csweep phase\u201d) times the bloom-filter on-heap size   (assume that can be a couple MB, depending on the expected number of files and allowed   false-positive ratio).</li> <li>Duplicate-commit-log-walk elimination requires some amount of memory for each distinct cut-off   time times the (possible) number of commits over the matching references.</li> <li>Additional memory is required for the currently processed chunks of metadata, for example   Iceberg\u2019s table-metadata and manifests, to identify the live data files. (The raw metadata is   only read and processed, but not memoized.)</li> <li>An in-memory live-contents-repository (not recommended for production workloads) requires   memory for all content-references.</li> </ul>"},{"location":"nessie-latest/gc/#cpu-heap-pressure-testing","title":"CPU &amp; heap pressure testing","text":"<p>Special \u201ctests\u201d (this and (this) have been used to verify that even a huge amount of objects does not let a tiny Java heap \u201cexplode\u201d and not use excessive CPU resources. This \u201ctest\u201d simulates a Nessie repository with many references, commits, contents and files per content version. Runs of that test using a profiler proved that the implementation requires little memory and little CPU - runtime is largely dominated by bloom-filter put and maybe-contains operations for the per-content-expire runs. Both tests proved the concept.</p>"},{"location":"nessie-latest/gc/#deferred-deletion","title":"Deferred deletion","text":"<p>The default behavior is to immediately deletes orphan files. But it is also possible to record the files to be deleted and delete those later. The <code>nessie-gc.jar</code> tool supports deferred deletion.</p>"},{"location":"nessie-latest/gc/#non-nessie-use-cases","title":"Non-Nessie use cases","text":"<p>Although all the above is designed for Nessie, it is possible to reuse the core implementation with \u201cplain\u201d Iceberg, effectively a complete replacement of Iceberg\u2019s expire snapshots and delete orphan files, but without Iceberg\u2019s implicit requirement of using Spark. Things needed for this:</p> <ul> <li>A \u201cpure Iceberg\u201d implementation of <code>org.projectnessie.gc.repository.RepositoryConnector</code>:</li> <li>Return one reference per Iceberg table, derived from the underlying Iceberg catalog.</li> <li>Provide a commit log with one <code>Put</code> operation for each Iceberg snapshot.</li> <li>(The <code>allContents</code> function can return an empty <code>Stream</code>for the \u201cpure Iceberg\u201d use case.)</li> <li>Existing functionality, the mark-and-sweep logic and the code in <code>nessie-gc-iceberg</code> and   <code>nessie-gc-iceberg-files</code>, can be reused without any changes.</li> </ul>"},{"location":"nessie-latest/gc/#potential-future-enhancements","title":"Potential future enhancements","text":"<p>Since Nessie GC keeps track of all ever live content-references and all ever known base content locations, it is possible to identify \u2026</p> <ul> <li>\u2026 the base content locations that are no longer used. In other words: storage of e.g. Iceberg   tables that have been deleted and are no longer referenced in any live Nessie commit.</li> <li>\u2026 the content references (aka Iceberg snapshots) are no longer used. This information can be   used to no longer expose the affected e.g. Iceberg snapshots in any table metadata.</li> </ul>"},{"location":"nessie-latest/gc/#completely-unreferenced-contents","title":"Completely unreferenced contents","text":"<p>Files of contents that are not visible from any live Nessie commit can be completely removed. Detecting this situation is not directly supported by the above approach.</p> <p>The live-contents-set generated by Nessie GC\u2019s identify phase contains all content IDs that are \u201clive\u201d. Nessie (server) could (and this approach is really just a thought) help here, by sending the \u201clive\u201d content IDs to Nessie and Nessie returning one content object for all content IDs that are not contained in the set of \u201clive\u201d content IDs. Another implementation would then be responsible to inspect the returned contents and purge the base locations in the data lake, where the data files, manifests, etc were stored.</p> <p>The above must not purge files for content IDs that have just been recently created.</p>"},{"location":"nessie-latest/gc/#potential-iceberg-specific-enhancements","title":"Potential Iceberg specific enhancements","text":"<p>Nessie GC can easily identify the Iceberg snapshots, as each Nessie commit references exactly one Iceberg table snapshot. Nessie (the runtime/server) has no knowledge of whether a particular Iceberg partition-spec, sort-order or schema is used via any live Iceberg snapshot or not, because partition-specs, sort-orders and schemas are referenced via Iceberg manifests. Although Nessie GC can identify these three kinds of structures during the identification of live contents, the expire phase, that maps content references to files, does not respect Nessie commit order.</p> <p>So it is necessary to fully think through a potential \u201cexpire specs/sort-orders/schemas\u201d, keeping in mind:</p> <ul> <li>Commit-chains are important, because IDs of partition-specs, sort-orders and schemas are assigned   sequentially (a 32 bit int).</li> <li>The logically same partition-specs, sort-orders and schemas may exist with different IDs on   different Nessie references.</li> <li>Partition-specs, sort-orders and schemas are only maintained in table-metadata, but referenced   from \u201cdeeper\u201d structures (manifest list, manifest files, data files).</li> <li>Is it really worth to have an \u201cexpire specs/sort-orders/schemas\u201d.</li> </ul>"},{"location":"nessie-latest/pynessie/","title":"Python based CLI","text":"<p>Warning</p> <p>The Python based CLI has been superseded by the new Nessie CLI. This Python CLI is currently no longer developed and does not support Nessie REST API v2.</p> <p>The Nessie CLI is an easy way to get started with Nessie. It supports multiple branch  and tag management capabilities. This is installed as <code>pynessie</code> via <code>pip install pynessie</code>. Additional information about <code>pynessie</code> and release notes can be found at the PyPI site. </p>"},{"location":"nessie-latest/pynessie/#installation","title":"Installation","text":"<pre><code># python 3 required\npip install pynessie\n</code></pre>"},{"location":"nessie-latest/pynessie/#usage","title":"Usage","text":"<p>All the REST API calls are exposed via the command line interface. To see a list of what is available run:</p> <pre><code>$ nessie --help\n</code></pre> <p>All docs of the CLI can be found here.</p>"},{"location":"nessie-latest/pynessie/#configuration","title":"Configuration","text":"<p>You can configure the Nessie CLI by creating a configuration file as described below:</p> <ul> <li>macOS: <code>~/.config/nessie</code> and <code>~/Library/Application Support/nessie</code></li> <li>Other Unix: <code>~/.config/nessie</code> and <code>/etc/nessie</code></li> <li>Windows: <code>%APPDATA%\\nessie</code> where the <code>APPDATA</code> environment variable falls   back to <code>%HOME%\\AppData\\Roaming</code> if undefined</li> <li>Via the environment variable <code>DREMIO_CLIENTDIR</code></li> </ul> <p>The default config file is as follows:</p> <pre><code>auth:\n    # Authentication type can be: none, bearer or aws\n    type: none\n\n    # OpenID token for the \"bearer\" authentication type\n    # token: &lt;OpenID token&gt;\n\n    timeout: 10\n\n# Nessie endpoint\nendpoint: http://localhost/api/v1\n\n# whether to skip SSL cert verification\nverify: true \n</code></pre> <p>Possible values for the <code>auth.type</code> property are:</p> <ul> <li><code>none</code> (default)</li> <li><code>bearer</code></li> <li><code>aws</code></li> </ul> <p>When configuring authentication type <code>bearer</code>, the <code>auth.token</code> parameter should be set to a valid OpenID token. The token can be set in the Nessie configuration file, as an environment variable (details below), or by the <code>--auth-token &lt;TOKEN&gt;</code> command line option (for each command).</p> <p>When configuring authentication type <code>aws</code>, the client delegates to the Boto  library. You can configure credentials using any of the standard Boto AWS methods. Additionally, the Nessie <code>auth.region</code> parameter should be set to the relevant AWS region.</p> <p>The command line interface can be configured with most of the above parameters via flags or by setting a config directory. The relevant configs can also be set via environment variables. These take precedence. The environment variable format is to append <code>NESSIE_</code> to a config parameter and nested configs are separated by a _. For example: <code>NESSIE_AUTH_TIMEOUT</code> maps to <code>auth.timeout</code> in the default configuration file above.</p>"},{"location":"nessie-latest/pynessie/#working-with-json","title":"Working with JSON","text":"<p>The Nessie CLI can return data in json format and can be used effectively with <code>jq</code>. For example:</p> <pre><code>$ nessie --json branch -l | jq .\n</code></pre> <p>The Nessie CLI is built on the great Python Click library. It requires Python 3.x.</p>"},{"location":"nessie-latest/security/","title":"Security","text":""},{"location":"nessie-latest/security/#authentication","title":"Authentication","text":"<p>Nessie currently supports 3 security modes:</p> <ul> <li>No Security</li> <li>Open Id Connect</li> <li>AWS IAM Roles (supported only on the Nessie client side)</li> </ul>"},{"location":"nessie-latest/security/#authorization-access-control","title":"Authorization (Access Control)","text":"<p>Nessie supports authorization. Details are described in the Authorization section.</p>"},{"location":"nessie-latest/spark-sql/","title":"Nessie Spark SQL Extension Reference","text":"<p>See also the Nessie Spark SQL Extensions main page.</p>"},{"location":"nessie-latest/spark-sql/#nessie-sql-commands-reference","title":"Nessie SQL commands reference","text":"<p>The following syntax descriptions illustrate how commands are used and the order of the clauses.</p> <p>The commands provided by the Nessie Spark SQL are actually a subset of the commands that are available in the Nessie CLI. Nessie Spark SQL commands however have the <code>IN &lt;catalog-name&gt;</code> clause, which is not needed in the Nessie CLI.</p> <p>Warn</p> <p>Use the version of the Nessie Spark SQL extensions that matches the Nessie version included in the Iceberg version you want to use! See Nessie Spark SQL Extensions page for details.</p> <p>Info</p> <p><code>CODE</code> style means the term is a keyword.</p> <p>BoldTerms mean variable input, see Descripton of Command Parts below</p> <p>Square brackets <code>[</code> <code>]</code> mean that the contents are optional (0 or 1 occurrence).</p> <p>Curly brackets <code>{</code> <code>}</code> mean that the contents can be repeated 0 or more times.</p>"},{"location":"nessie-latest/spark-sql/#create-branch-tag","title":"<code>CREATE BRANCH</code> / <code>TAG</code>","text":"<p><code>CREATE</code> ReferenceType        [ <code>IF</code> <code>NOT</code> <code>EXISTS</code> ]  ReferenceName        [ <code>IN</code> CatalogName ]        [ <code>FROM</code> ExistingReference ]        [ <code>AT</code> [ <code>TIMESTAMP</code> | <code>COMMIT</code> ] TimestampOrCommit ]</p> <p>Creates a new Nessie branch or tag using the name specified using the <code>ReferenceName</code> parameter. The reference type is specified using the <code>ReferenceType</code> parameter.</p> <p>By default, the new branch or tag is created from the latest commit on the current reference of the Nessie CLI (see <code>USE</code> statement). Another source reference name can use specified using the <code>FROM</code> clause. The optional <code>AT</code> clause allows specifying a different commit ID (hash) to create the new reference from.</p> <p>This command will fail, if a references with the name <code>ReferenceName</code> already exists, unless the optional <code>IF NOT EXISTS</code> is specified.</p>"},{"location":"nessie-latest/spark-sql/#drop-branch-tag","title":"<code>DROP BRANCH</code> / <code>TAG</code>","text":"<p><code>DROP</code> ReferenceType        [ <code>IF</code> <code>EXISTS</code> ]  ExistingReference        [ <code>IN</code> CatalogName ]</p> <p>Drops a new Nessie branch or tag using the name specified using the <code>ReferenceName</code> parameter. The reference type is specified using the <code>ReferenceType</code> parameter.</p> <p>This command will fail, if a references with the name <code>ReferenceName</code> does not exist, unless the optional <code>IF EXISTS</code> is specified.</p>"},{"location":"nessie-latest/spark-sql/#assign-branch-tag","title":"<code>ASSIGN BRANCH</code> / <code>TAG</code>","text":"<p><code>ASSIGN</code> ReferenceType        [ ExistingReference ]        [ <code>TO</code> ExistingReference [ <code>AT</code> [ <code>TIMESTAMP</code> | <code>COMMIT</code> ] TimestampOrCommit ] ]        [ <code>IN</code> CatalogName ]</p> <p>Assigns a Nessie branch or tag using the name specified using the <code>ReferenceName</code> parameter to another commit. The reference type is specified using the <code>ReferenceType</code> parameter.</p> <p>By default, the branch or tag is updated to the latest commit on the current reference of the Nessie CLI (see <code>USE</code> statement). Another target reference name can use specified using the <code>TO</code> clause. The optional <code>AT</code> clause allows specifying a different commit ID (hash) to assign the reference to.</p>"},{"location":"nessie-latest/spark-sql/#list-references","title":"<code>LIST REFERENCES</code>","text":"<p><code>LIST</code> <code>REFERENCES</code>        [ <code>FILTER</code> Value       | [ <code>STARTING</code> <code>WITH</code> Value ] [ <code>CONTAINING</code> Value ]       ]        [ <code>IN</code> CatalogName ]</p> <p>Lists all named references.</p> <p>An optional CEL-filter can be specified, which is evaluated on the server side.</p> <p>The optional <code>STARTING WITH</code> clause starts the output at the content-key with the given value.</p> <p>The optional <code>CONTAINING</code> clause only outputs entities with a content-key that contain the given value.</p>"},{"location":"nessie-latest/spark-sql/#merge-branch","title":"<code>MERGE BRANCH</code>","text":"<p><code>MERGE</code>        [ <code>DRY</code> ]        [ ReferenceType ]  ExistingReference        [ <code>AT</code> [ <code>TIMESTAMP</code> | <code>COMMIT</code> ] TimestampOrCommit ]        [ <code>INTO</code> ExistingReference ]        [ <code>BEHAVIOR</code> MergeBehaviorKind ]        [ <code>BEHAVIORS</code> ContentKey <code>=</code> MergeBehaviorKind { <code>AND</code> ContentKey <code>=</code> MergeBehaviorKind } ]        [ <code>IN</code> CatalogName ]</p> <p>Merges a branch or tag into another branch, supporting manual conflict resolution.</p> <p>The optional <code>DRY</code> keyword defines that Nessie shall simulate a merge operation. This is useful to check whether a merge operation would succeed.</p> <p>Specifying the name of the \u201cfrom\u201d reference is mandatory. By default, the latest commit of the \u201cfrom\u201d branch or tag will be merged, which can be overridden using the <code>AT</code> clause.</p> <p>By default, <code>MERGE</code> uses the CLI\u2019s current reference as the target branch. The <code>INTO</code> clause can be used to specify another target branch.</p> <p>Nessie merge operations currently support three different merge behaviors:</p> <ul> <li><code>NORMAL</code>: a merge succeeds, if the content does not have a conflicting change in the target branch.</li> <li><code>FORCE</code>: a merge always succeeds, the content from the \u201cfrom\u201d reference will be applied onto the target branch.</li> <li><code>DROP</code>: like <code>NORMAL</code>, but does not cause a conflict, so does not fail the whole merge operation.</li> </ul> <p>The merge behavior for all contents defaults to <code>NORMAL</code> and can be changed using the <code>BEHAVIOR</code> clause.</p> <p>Specific merge behaviors can be specified using the <code>BEHAVIORS</code> clause for individual content keys.</p>"},{"location":"nessie-latest/spark-sql/#show-log","title":"<code>SHOW LOG</code>","text":"<p><code>SHOW</code> <code>LOG</code>        [ [ <code>ON</code> [ ReferenceType ] ] ExistingReference ]        [ <code>AT</code> [ <code>TIMESTAMP</code> | <code>COMMIT</code> ] TimestampOrCommit ]        [ <code>LIMIT</code> PositiveInt ]        [ <code>IN</code> CatalogName ]</p> <p>Shows the Nessie commit log.</p> <p>By default, the commit log fetched for the current reference of the Nessie CLI, or in the branch or tab specified using the <code>IN</code> clause. By default entities on the latest commit of the branch or tag will be listed, which can be overridden using the <code>AT</code> clause.</p> <p>The output can be limited using the <code>LIMIT</code> clause. It is safe to omit the <code>LIMIT</code> clause for ANSI terminals, because the commit log will be safely paged with neither overloading the Nessie CLI or Nessie server.</p>"},{"location":"nessie-latest/spark-sql/#show-reference","title":"<code>SHOW REFERENCE</code>","text":"<p><code>SHOW</code> <code>REFERENCE</code>        [ ExistingReference ]        [ <code>AT</code> [ <code>TIMESTAMP</code> | <code>COMMIT</code> ] TimestampOrCommit ]        [ <code>IN</code> CatalogName ]</p> <p>Shows information about the current or given reference.</p> <p>If no reference is specified, information about the current reference of the Nessie CLI is shown, otherwise information about the given reference. By default, entities information of latest commit of the branch or tag will be shown, which can be overridden using the <code>AT</code> clause.</p>"},{"location":"nessie-latest/spark-sql/#command-parts","title":"Command parts","text":""},{"location":"nessie-latest/spark-sql/#catalogname","title":"CatalogName","text":"<p>Spark catalog name.</p>"},{"location":"nessie-latest/spark-sql/#referencetype","title":"ReferenceType","text":"<p><code>BRANCH</code> | <code>TAG</code></p>"},{"location":"nessie-latest/spark-sql/#existingreference","title":"ExistingReference","text":"<p>Name of an existing reference in Nessie.</p>"},{"location":"nessie-latest/spark-sql/#referencename","title":"ReferenceName","text":"<p>Nessie reference name.</p>"},{"location":"nessie-latest/spark-sql/#timestamporcommit","title":"TimestampOrCommit","text":"<p>Either a Nessie commit ID (hash) or a timestamp in ISO format. Examples:</p> <ul> <li><code>2024-04-26T10:31:05.277650575Z</code> is a valid ISO timestamp</li> <li><code>fa32a50d5303a53826f65649277561f5c6772eba019e7f1e01a359becb764877</code> is a valid Nessie commit ID (hash)</li> </ul>"},{"location":"nessie-latest/trino/","title":"Trino with Iceberg REST","text":"<p>Using Trino with Nessie Catalog (Iceberg REST) works and has been validated using S3 and Minio.</p>"},{"location":"nessie-latest/trino/#object-stores","title":"Object Stores","text":"<p>Currently only S3 is supported for Trino with Nessie Catalog. GCS and ADLS might work.</p> <p>Trino does not support S3 request signing, so it is mandatory to configure the assume role/credentials vending functionality, see Nessie Docs starting here. The Trino <code>fs.native-s3.enabled=true</code> setting is mandatory for this to work.</p>"},{"location":"nessie-latest/trino/#oauth2-support","title":"OAuth2 support","text":"<p>Warn</p> <p>Trino lacks functionality to configure the oauth endpoint and is therefore unable to work with any Iceberg REST catalog implementation and demands a standard OAuth2 server like Keycloak or Authelia.</p> <p>Until the missing OAuth2 endpoint support issue is fixed in Trino, users have to supply an OAuth2 token via the <code>iceberg.rest-catalog.oauth2.token</code> Trino catalog configuration.</p>"},{"location":"nessie-latest/trino/#starter-configuration","title":"Starter configuration","text":"<p>You can query Nessie Catalog to get an example starter configuration template for the Trino catalog configuration.</p> Static Catalog ConfigurationDynamic Catalog Configuration <p>To get the Trino \u2018static\u2019 catalog configuration, use <code>curl</code> or <code>wget</code>:</p> <pre><code>NESSIE_BASE_URL=\"http://127.0.0.1:19120/\"\ncurl \"${NESSIE_BASE_URL}/iceberg-ext/v1/client-template/trino?format=static\"\n</code></pre> <p><code>wget</code> can be used to download the configuration SQL to a <code>nessie.properties</code> file:</p> <pre><code>NESSIE_BASE_URL=\"http://127.0.0.1:19120/\"\nwget --content-disposition \"${NESSIE_CATALOG_URL}/iceberg-ext/v1/client-template/trino?format=static\" \n</code></pre> <p>The above will return the contents for a Trino catalog configuration properties file for \u2018static\u2019 catalog configuration.</p> <pre><code>#\n# Example Trino starter configuration properties for warehouse s3://bucket-yzeq68\n# generated by Nessie to be placed for example in\n# /etc/trino/catalogs/nessie.properties within a Trino container/pod when.\n# using Trino 'static' configurations.\n# \n# This starter configuration must be inspected and verified to validate that\n# all options and values match your specific needs and no mandatory options\n# are missing or superfluous options are present.\n# \n# When using OAuth2, you have to supply the 'iceberg.rest-catalog.oauth2.token'\n# configuration.\n# \n# WARNING! Trino lacks functionality to configure the oauth endpoint and is therefore\n# unable to work with any Iceberg REST catalog implementation and demands a standard\n# OAuth2 server like Keycloak or Authelia. If you feel you need client-ID/secret flow,\n# please report an issue against Trino.\n# \n# No guarantees that this configuration works for your specific needs.\n# Use at your own risk!\n# Do not distribute the contents as those may contain sensitive information!\n#\n\nconnector.name=iceberg\nfs.native-s3.enabled=true\niceberg.catalog.type=rest\niceberg.rest-catalog.uri=http\\://localhost\\:33405/iceberg/\niceberg.rest-catalog.vended-credentials-enabled=true\ns3.endpoint=http\\://localhost\\:37029/\ns3.path-style-access=true\ns3.region=us-east-1\n</code></pre> <p>To get the Trino \u2018dynamic\u2019 catalog configuration, use <code>curl</code> or <code>wget</code>:</p> <pre><code>NESSIE_CATALOG_URL=\"http://127.0.0.1:19120/iceberg\"\ncurl \"${NESSIE_CATALOG_URL}/v1-clients/trino?format=dynamic\"\n</code></pre> <p><code>wget</code> can be used to download the configuration SQL to a <code>create-catalog-nessie.sql</code> file:</p> <pre><code>NESSIE_CATALOG_URL=\"http://127.0.0.1:19120/iceberg\"\nwget --content-disposition \"${NESSIE_CATALOG_URL}/v1-clients/trino?format=dynamic\" \n</code></pre> <p>The above will return the SQL for a Trino catalog configuration for \u2018dynamic\u2019 catalog configuration.</p> <pre><code>/*\n * Example Trino starter configuration properties for warehouse s3://bucket-yzeq68\n * generated by Nessie to be placed for example in\n * /etc/trino/catalogs/nessie.properties within a Trino container/pod when.\n * using Trino 'static' configurations.\n * \n * This starter configuration must be inspected and verified to validate that\n * all options and values match your specific needs and no mandatory options\n * are missing or superfluous options are present.\n * \n * When using OAuth2, you have to supply the 'iceberg.rest-catalog.oauth2.token'\n * configuration.\n * \n * WARNING! Trino lacks functionality to configure the oauth endpoint and is therefore\n * unable to work with any Iceberg REST catalog implementation and demands a standard\n * OAuth2 server like Keycloak or Authelia. If you feel you need client-ID/secret flow,\n * please report an issue against Trino.\n * \n * No guarantees that this configuration works for your specific needs.\n * Use at your own risk!\n * Do not distribute the contents as those may contain sensitive information!\n */\nCREATE CATALOG nessie\n  USING iceberg\n  WITH (\n    \"connector.name\" = 'iceberg',\n    \"fs.native-s3.enabled\" = 'true',\n    \"iceberg.catalog.type\" = 'rest',\n    \"iceberg.rest-catalog.uri\" = 'http://localhost:33405/iceberg/',\n    \"iceberg.rest-catalog.vended-credentials-enabled\" = 'true',\n    \"s3.endpoint\" = 'http://localhost:37029/',\n    \"s3.path-style-access\" = 'true',\n    \"s3.region\" = 'us-east-1'\n  );\n</code></pre>"},{"location":"nessie-latest/trino/#references","title":"References","text":"<p>Refer to the Trino documentation for more information about the configuration settings:</p> <ul> <li>Iceberg specific</li> <li>Iceberg REST specific</li> <li>File System Access Configuration<ul> <li>S3 specific</li> <li>GCS specific</li> <li>ADLS specific</li> </ul> </li> </ul>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2022/","title":"2022","text":""},{"location":"blog/page/2/","title":"Nessie Blog","text":""},{"location":"blog/archive/2024/page/2/","title":"2024","text":""}]}