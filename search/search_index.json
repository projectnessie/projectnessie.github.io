{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Git-inspired data version control Cross-table transactions and visibility Open data lake approach, supporting Hive, Spark, Dremio, AWS Athena, etc. Works with Apache Iceberg, Delta Lake and Hive tables Run as a docker image, AWS Lambda or fork it on GitHub Get in touch via our Google Group and our Slack Channel and follow us on Twitter . Nessie source code, code contributions and bug reports are on GitHub .","title":"Home"},{"location":"develop/","text":"Community \u00b6 Nessie is developed as a consensus-driven open source product under the Apache 2.0 license. Development is done in the open leveraging GitHub issues, PRs and using Google Groups as a mailing list. Get In Touch \u00b6 Slack Channel The developers on Nessie frequent the nessie-public Slack channel. You can get an invite to the channel by emailing slack-subscribe@projectnessie.org . If you want your organization invited to the channel, please state that in the request. Whether you\u2019re super excited about development or just want to hear what is happening, everyone is welcome to join. Google Group If long form is more your thing, we also have created a mailing list on Google groups that you can subscribe to. GitHub Issues Nessie is developed via GitHub issues and pull requests. If you see a problem or want to enhance the product, we suggest you file a GitHub issue for developers to review. Twitter The @projectnessie account on Twitter is our official account. Follow-up to keep to date on what is happening with Project Nessie! YouTube Channel Video content for Nessie will be hosted on our YouTube channel. Docs Our website is all maintained in our source repository. If there is something you think can be improved, feel free to fork our repository and post a pull request. Reviewable We use Reviewable (in addition to GitHub reviews) to collaborate on code and docs. They are a great service and support OSS projects. Contribution \u00b6 All contributors are welcome to Project Nessie. To get started, feel free to introduce yourself on Slack or our Google Group. Nessie is open to everyone!","title":"Community"},{"location":"develop/#community","text":"Nessie is developed as a consensus-driven open source product under the Apache 2.0 license. Development is done in the open leveraging GitHub issues, PRs and using Google Groups as a mailing list.","title":"Community"},{"location":"develop/#get-in-touch","text":"Slack Channel The developers on Nessie frequent the nessie-public Slack channel. You can get an invite to the channel by emailing slack-subscribe@projectnessie.org . If you want your organization invited to the channel, please state that in the request. Whether you\u2019re super excited about development or just want to hear what is happening, everyone is welcome to join. Google Group If long form is more your thing, we also have created a mailing list on Google groups that you can subscribe to. GitHub Issues Nessie is developed via GitHub issues and pull requests. If you see a problem or want to enhance the product, we suggest you file a GitHub issue for developers to review. Twitter The @projectnessie account on Twitter is our official account. Follow-up to keep to date on what is happening with Project Nessie! YouTube Channel Video content for Nessie will be hosted on our YouTube channel. Docs Our website is all maintained in our source repository. If there is something you think can be improved, feel free to fork our repository and post a pull request. Reviewable We use Reviewable (in addition to GitHub reviews) to collaborate on code and docs. They are a great service and support OSS projects.","title":"Get In Touch"},{"location":"develop/#contribution","text":"All contributors are welcome to Project Nessie. To get started, feel free to introduce yourself on Slack or our Google Group. Nessie is open to everyone!","title":"Contribution"},{"location":"develop/architecture/","text":"Architecture \u00b6 Nessie builds on the recent ecosystem developments around table formats. The rise of very large metadata and eventually consistent cloud data lakes (S3 specifically) drove the need for an updated model around metadata managmeent. Where consistent directory listings in HDFS used to be sufficient, there were many features lacking. This includes snapshotting, consistency and fast planning. Apache Iceberg and Delta Lake were both created to help alleviate those problems. For more insight into why we created Nessie, you can read the founding blog post by one of Nessie\u2019s creators. Inspiration \u00b6 The Iceberg format (as well as the Delta Lake format) relies on a set of metadata files stored with (or near) the actual data tables. This allows Iceberg to fulfill the same role as the Hive Metastore for transactions without the need for expensive metadata scans or centralized planning (see Iceberg performance ). This includes things such as partitioning (including hidden partitions), schema migrations, appends and deletes. It does however require a pointer to the active metadata set to function. This pointer allows the Iceberg client to acquire and read the current schema, files and partitions in the dataset. Iceberg currently relies on the Hive metastore or hdfs to perform this role. The requirements for this root pointer store is it must hold (at least) information about the location of the current up to date metadata file and it must be able to update this location atomically. In Hive this is accomplished by locks and in hdfs by using atomic file swap operations. These operations don\u2019t exist in eventually consistent cloud object stores, necessitating a Hive metastore for cloud data lakes. The Nessie system is designed to store the root metadata pointer and perform atomic updates to this pointer, obviating the need for a Hive metastore. Removing the need for a Hive metastore simplifies deployment and broadens the reach of tools that can work with Iceberg tables. The above is specific to how Iceberg behaves however Delta Lake operates in a near identical way. The Nessie service is a lightweight Java based REST API server. It uses a standard optimistic locking strategy to ensure atomic transactions. This relies on every operation carrying an expected hash state for the store and allows for a very light weight and scalable implementation. The implementation uses configurable authentication (eg IAM on AWS, JWT elsewhere) and a configurable backend (currently supporting DynamoDB on AWS) and uses the optimistic locking features of cloud based key value stores to ensure scalability across servers. This architecture allows for Nessie to run in a docker container, as a Lambda function or in a number of other configurations.","title":"Architecture"},{"location":"develop/architecture/#architecture","text":"Nessie builds on the recent ecosystem developments around table formats. The rise of very large metadata and eventually consistent cloud data lakes (S3 specifically) drove the need for an updated model around metadata managmeent. Where consistent directory listings in HDFS used to be sufficient, there were many features lacking. This includes snapshotting, consistency and fast planning. Apache Iceberg and Delta Lake were both created to help alleviate those problems. For more insight into why we created Nessie, you can read the founding blog post by one of Nessie\u2019s creators.","title":"Architecture"},{"location":"develop/architecture/#inspiration","text":"The Iceberg format (as well as the Delta Lake format) relies on a set of metadata files stored with (or near) the actual data tables. This allows Iceberg to fulfill the same role as the Hive Metastore for transactions without the need for expensive metadata scans or centralized planning (see Iceberg performance ). This includes things such as partitioning (including hidden partitions), schema migrations, appends and deletes. It does however require a pointer to the active metadata set to function. This pointer allows the Iceberg client to acquire and read the current schema, files and partitions in the dataset. Iceberg currently relies on the Hive metastore or hdfs to perform this role. The requirements for this root pointer store is it must hold (at least) information about the location of the current up to date metadata file and it must be able to update this location atomically. In Hive this is accomplished by locks and in hdfs by using atomic file swap operations. These operations don\u2019t exist in eventually consistent cloud object stores, necessitating a Hive metastore for cloud data lakes. The Nessie system is designed to store the root metadata pointer and perform atomic updates to this pointer, obviating the need for a Hive metastore. Removing the need for a Hive metastore simplifies deployment and broadens the reach of tools that can work with Iceberg tables. The above is specific to how Iceberg behaves however Delta Lake operates in a near identical way. The Nessie service is a lightweight Java based REST API server. It uses a standard optimistic locking strategy to ensure atomic transactions. This relies on every operation carrying an expected hash state for the store and allows for a very light weight and scalable implementation. The implementation uses configurable authentication (eg IAM on AWS, JWT elsewhere) and a configurable backend (currently supporting DynamoDB on AWS) and uses the optimistic locking features of cloud based key value stores to ensure scalability across servers. This architecture allows for Nessie to run in a docker container, as a Lambda function or in a number of other configurations.","title":"Inspiration"},{"location":"develop/java/","text":"Java \u00b6 Java Client \u00b6 Nessie has a thin client designed to be incorporated into existing projects with minimum difficulty. The client is a thin layer over Nessie\u2019s openapi Rest APIs . To use the Nessie client, you can add it as a dependency to your Java project using Maven. The coordinates are: <dependency> <groupId>org.projectnessie</groupId> <artifactId>nessie-client</artifactId> <version>0.5.1</version> </dependency> For ease of integration with tools that carry many dependencies, the Nessie client\u2019s dependencies are declared optionaly. It is designed to work with any recent version JAX-RS client (Jersey and Resteasy are both tested inside Nessie\u2019s tests) + Jackson\u2019s DataBinding and JAX-RS modules (any version from the last ~3+ years). API \u00b6 The NessieClient object wraps a Jersey Client and exposes interactions with the Nessie Rest API. To use it simply client = NessieClient . basic ( path , username , password ); List < Reference > references = client . getTreeApi (). getAllReferences (); references . stream () . map ( Reference :: getName ) . forEach ( System . out :: println ); The client API has the full set of methods required to interact with Nessie at this level. The above example authenticates with basic auth using a username and password. If using the AWS client and authenticating with IAM roles use NessieClient.aws(path) to instantiate a client. The path argument is the full url for the nessie endpoint (eg http://localhost:19120/api/v1 ). Server \u00b6 The Nessie server is based on the Quarkus microkernel appserver and can run in a traditional JVM or be precompiled into a native image via GraalVM. We develop all code using JDK11 and then compile for release on JDK8. Rest API \u00b6 The Rest API is composed primarily of the Contents , Tree and Config APIs. Versioning Kernel \u00b6 Deeper in the server, the core commit kernel is built on top of the VersionStore SPI New Backing Store \u00b6 Nessie comes pre-packaged with three storage engines: JGit, In-Memory and DynamoDB. For production deployments, DynamoDB is the recommended storage engine. The Nessie team is always evaluating new potential backing stores. If you\u2019re looking to implement something new, shout out to us via one of our communication channels to discuss what you\u2019re thinking so we can figure out the best way to move forward.","title":"Java"},{"location":"develop/java/#java","text":"","title":"Java"},{"location":"develop/java/#java-client","text":"Nessie has a thin client designed to be incorporated into existing projects with minimum difficulty. The client is a thin layer over Nessie\u2019s openapi Rest APIs . To use the Nessie client, you can add it as a dependency to your Java project using Maven. The coordinates are: <dependency> <groupId>org.projectnessie</groupId> <artifactId>nessie-client</artifactId> <version>0.5.1</version> </dependency> For ease of integration with tools that carry many dependencies, the Nessie client\u2019s dependencies are declared optionaly. It is designed to work with any recent version JAX-RS client (Jersey and Resteasy are both tested inside Nessie\u2019s tests) + Jackson\u2019s DataBinding and JAX-RS modules (any version from the last ~3+ years).","title":"Java Client"},{"location":"develop/java/#api","text":"The NessieClient object wraps a Jersey Client and exposes interactions with the Nessie Rest API. To use it simply client = NessieClient . basic ( path , username , password ); List < Reference > references = client . getTreeApi (). getAllReferences (); references . stream () . map ( Reference :: getName ) . forEach ( System . out :: println ); The client API has the full set of methods required to interact with Nessie at this level. The above example authenticates with basic auth using a username and password. If using the AWS client and authenticating with IAM roles use NessieClient.aws(path) to instantiate a client. The path argument is the full url for the nessie endpoint (eg http://localhost:19120/api/v1 ).","title":"API"},{"location":"develop/java/#server","text":"The Nessie server is based on the Quarkus microkernel appserver and can run in a traditional JVM or be precompiled into a native image via GraalVM. We develop all code using JDK11 and then compile for release on JDK8.","title":"Server"},{"location":"develop/java/#rest-api","text":"The Rest API is composed primarily of the Contents , Tree and Config APIs.","title":"Rest API"},{"location":"develop/java/#versioning-kernel","text":"Deeper in the server, the core commit kernel is built on top of the VersionStore SPI","title":"Versioning Kernel"},{"location":"develop/java/#new-backing-store","text":"Nessie comes pre-packaged with three storage engines: JGit, In-Memory and DynamoDB. For production deployments, DynamoDB is the recommended storage engine. The Nessie team is always evaluating new potential backing stores. If you\u2019re looking to implement something new, shout out to us via one of our communication channels to discuss what you\u2019re thinking so we can figure out the best way to move forward.","title":"New Backing Store"},{"location":"develop/kernel/","text":"Commit Kernel \u00b6 Nessie\u2019s production commit kernel is optimized to provide high commit throughput against a distributed key value store that provides record-level ACID guarantees. Today, this kernel is built on top of DynamoDB. The commit kernel is the heart of Nessie\u2019s oeprations and enables it to provide lightweight creation of new tags and branches, merges, rebases all with a very high concurrent commit rate. Data Structures \u00b6 Nessie\u2019s commit kernel works with two tables: the refs table and the objects table. Refs Table The refs table will have objects equal to the current number of active tags and branches. This will generally be small (10s-1000s). All commits run through this table and thus the writes and reads of this table should be provisioned based on the amount of read and write operations expected per second. Since the dataset is small, sharding will be unlikely to happen on this table. Scans are regularly done on this table. Objects Table The objects table stores data structures associated with commits and will be a small multiple of the number of objects and version tracked in Nessie. Assume approximately number objects x active versions x 4 (write multiplier) to get a rough sense of the number of 4kb objects that Nessie will be storing in the objects table. Scans are not done on this table except in the case of rare Garbage Collection operations. Space Consumption: The data stored in DynamoDB is designed to largely fit within DynamoDBs 4kb read size unit (and 4x the write size unit). This true for objects in both the refs table and the objects table. At a high level, Nessie\u2019s commit kernel breaks keyspace into a 3 level tree and each operation restates some portion of that tree. Each level of the tree is identified by sha256 20 byte hash value, which is used to as a storage key in DynamoDB. Commit operations are atomic and interact with branch objects using a data structure akin to a 151-way striped locked. This data structure is updated using patch conditional operations against DynamoDB. To maintain linear history, the branch object also maintains a commit log that works with the striped lock to make history clear and consistent. Performance \u00b6 The design target for the Nessie commit kernel was to support 100,000 tables with each table mutating every 5 minutes. This equates to ~300 operations/second. Tests have shown the Nessie commit kernel on DynamoDB to be able to exceed that by a large margin. We will publish a more comprehensive benchmark soon.","title":"Commit Kernel"},{"location":"develop/kernel/#commit-kernel","text":"Nessie\u2019s production commit kernel is optimized to provide high commit throughput against a distributed key value store that provides record-level ACID guarantees. Today, this kernel is built on top of DynamoDB. The commit kernel is the heart of Nessie\u2019s oeprations and enables it to provide lightweight creation of new tags and branches, merges, rebases all with a very high concurrent commit rate.","title":"Commit Kernel"},{"location":"develop/kernel/#data-structures","text":"Nessie\u2019s commit kernel works with two tables: the refs table and the objects table. Refs Table The refs table will have objects equal to the current number of active tags and branches. This will generally be small (10s-1000s). All commits run through this table and thus the writes and reads of this table should be provisioned based on the amount of read and write operations expected per second. Since the dataset is small, sharding will be unlikely to happen on this table. Scans are regularly done on this table. Objects Table The objects table stores data structures associated with commits and will be a small multiple of the number of objects and version tracked in Nessie. Assume approximately number objects x active versions x 4 (write multiplier) to get a rough sense of the number of 4kb objects that Nessie will be storing in the objects table. Scans are not done on this table except in the case of rare Garbage Collection operations. Space Consumption: The data stored in DynamoDB is designed to largely fit within DynamoDBs 4kb read size unit (and 4x the write size unit). This true for objects in both the refs table and the objects table. At a high level, Nessie\u2019s commit kernel breaks keyspace into a 3 level tree and each operation restates some portion of that tree. Each level of the tree is identified by sha256 20 byte hash value, which is used to as a storage key in DynamoDB. Commit operations are atomic and interact with branch objects using a data structure akin to a 151-way striped locked. This data structure is updated using patch conditional operations against DynamoDB. To maintain linear history, the branch object also maintains a commit log that works with the striped lock to make history clear and consistent.","title":"Data Structures"},{"location":"develop/kernel/#performance","text":"The design target for the Nessie commit kernel was to support 100,000 tables with each table mutating every 5 minutes. This equates to ~300 operations/second. Tests have shown the Nessie commit kernel on DynamoDB to be able to exceed that by a large margin. We will publish a more comprehensive benchmark soon.","title":"Performance"},{"location":"develop/nessie_vs_git/","text":"Nessie vs Git \u00b6 Git is awesome. Nessie was inspired by Git but makes very different tradeoffs. Nessie focuses on the specific use case of data version control. By narrowing our focus, we were able to better serve the needs of the data ops experience while continuing to support a general git metaphore. The key difference between the two is that Nessie does not support disconnected copies. This allows several other dimensions to be substantially more powerful. Key differences \u00b6 Dimension Git Nessie Rationale Clones Allowed Not Allowed This is the biggest difference between Nessie and Git. Git is a distributed version control system, Nessie is not. This is appropriate in the context of Nessie\u2019s role as an RPS . When talking about Cloud Data ops, everyone doesn\u2019t get their own copy of data\u2013the datasets are typically large and centralized. Because Nessie is layered on top of those shared datasets, it clones make less sense. In the Nessie world, using personal branches provides a similar mechanism while keeping a shared world view of what can be managed for GC policies, etc. Speed (commits/second)) <1 >2000 When we started working on Nessie, we actually tried to use Git. We evaluated Git directly, implemented a version that used JGit (used by tools like Gerrit and Eclipse) as well as explored the capabilities of GitHub, Azure Git and AWS Git. What we saw was a fairly expensive operation. Typically, a single commit operation took on the order of a few seconds. Scale 100s MB Unconstrained While there are multiple examples of larger or higher performance Git implementations [ 1 , 2 ] , in general Git repositories are fairly small in size. Things like Git LFS were created to help accommodate this but given the nature of clones, large repositories are frowned upon. Because Nessie provides a centralized repository, typical repository constraints do not apply. History Thousands Billions Nessie supports optional garbage collection of historical commits based on user-defined rules to reduce storage consumption. Committer Human Human & Machine Git was designed for human time: commits happen 100-1000s of times a day, not 100x per second. Data in many systems is changing very frequently (especially when you consider a large number of tables). These systems are frequently driven by automated scripts Objects Files Tables & Views Nessie is focused on tracking versions of tables using a well-known set of open table formats. Nessie\u2019s capabilities are integrated into the particular properties of these formats. Git is focused on the generalized problem of tracking arbitrary files (buckets of bytes). Nessie on Git? \u00b6 While we describe the reasoning and differences above, we actually support running Nessie on top of Git. In fact, the first version of Nessie was built on top of Git. Once implemented, we then evaluated it against one of our key design criterion. This design criterion was to support commits in the situation where there are 100,000 tables and each table is changing every 5 minutes. (For reference, the 5 minutes comes from community guidance on commit speed per table for Iceberg. The 100,000 tables comes from various users we\u2019ve worked with before.) The math for this comes out to ~333 commits/second. 333 Commits/second? \u00b6 Using the design goal above, we looked at the major Git service providers to evaluate their performance. We saw an average commit turn-around speed of 1-5/s for most services (GitHub, Azure Git, AWS Git, etc). Worse case commit latency were >20s for a single commit. Given this initial result, things were not looking good. We took one more attempt to try to achieve the performance requirements using Git. We built a custom storage mechanism for the awesome JGit library . This showed better promise, providing up to 20/commits second when run against DynamoDB. However, it was still insufficient. As such, we ultimately built our own commit kernel to power Nessie. In Nessie, we continue to include an experimental backing store built on top of JGit. This serves as both a good trial tool and homage to git. So Which is Better \u00b6 Like all engineering solutions, this isn\u2019t about what is better, only what is better for a certain use case. Git is good at generalized version control. Nessie is good at data version control. Nessie vs DVC \u00b6 DVC is a popular package within ML community that is described as \u201cVersion Control System for Machine Learning Projects\u201d it presents. While both Nessie and DVC are focused on data, DVC is focused on smaller datasets and maintaining the distributed capabilities of Git. This works great for individual projects that are typically run on single workstations where datasets can be replicated. Nessie works at a table and metadata level specifically focused on data management problems.","title":"Nessie vs Git"},{"location":"develop/nessie_vs_git/#nessie-vs-git","text":"Git is awesome. Nessie was inspired by Git but makes very different tradeoffs. Nessie focuses on the specific use case of data version control. By narrowing our focus, we were able to better serve the needs of the data ops experience while continuing to support a general git metaphore. The key difference between the two is that Nessie does not support disconnected copies. This allows several other dimensions to be substantially more powerful.","title":"Nessie vs Git"},{"location":"develop/nessie_vs_git/#key-differences","text":"Dimension Git Nessie Rationale Clones Allowed Not Allowed This is the biggest difference between Nessie and Git. Git is a distributed version control system, Nessie is not. This is appropriate in the context of Nessie\u2019s role as an RPS . When talking about Cloud Data ops, everyone doesn\u2019t get their own copy of data\u2013the datasets are typically large and centralized. Because Nessie is layered on top of those shared datasets, it clones make less sense. In the Nessie world, using personal branches provides a similar mechanism while keeping a shared world view of what can be managed for GC policies, etc. Speed (commits/second)) <1 >2000 When we started working on Nessie, we actually tried to use Git. We evaluated Git directly, implemented a version that used JGit (used by tools like Gerrit and Eclipse) as well as explored the capabilities of GitHub, Azure Git and AWS Git. What we saw was a fairly expensive operation. Typically, a single commit operation took on the order of a few seconds. Scale 100s MB Unconstrained While there are multiple examples of larger or higher performance Git implementations [ 1 , 2 ] , in general Git repositories are fairly small in size. Things like Git LFS were created to help accommodate this but given the nature of clones, large repositories are frowned upon. Because Nessie provides a centralized repository, typical repository constraints do not apply. History Thousands Billions Nessie supports optional garbage collection of historical commits based on user-defined rules to reduce storage consumption. Committer Human Human & Machine Git was designed for human time: commits happen 100-1000s of times a day, not 100x per second. Data in many systems is changing very frequently (especially when you consider a large number of tables). These systems are frequently driven by automated scripts Objects Files Tables & Views Nessie is focused on tracking versions of tables using a well-known set of open table formats. Nessie\u2019s capabilities are integrated into the particular properties of these formats. Git is focused on the generalized problem of tracking arbitrary files (buckets of bytes).","title":"Key differences"},{"location":"develop/nessie_vs_git/#nessie-on-git","text":"While we describe the reasoning and differences above, we actually support running Nessie on top of Git. In fact, the first version of Nessie was built on top of Git. Once implemented, we then evaluated it against one of our key design criterion. This design criterion was to support commits in the situation where there are 100,000 tables and each table is changing every 5 minutes. (For reference, the 5 minutes comes from community guidance on commit speed per table for Iceberg. The 100,000 tables comes from various users we\u2019ve worked with before.) The math for this comes out to ~333 commits/second.","title":"Nessie on Git?"},{"location":"develop/nessie_vs_git/#333-commitssecond","text":"Using the design goal above, we looked at the major Git service providers to evaluate their performance. We saw an average commit turn-around speed of 1-5/s for most services (GitHub, Azure Git, AWS Git, etc). Worse case commit latency were >20s for a single commit. Given this initial result, things were not looking good. We took one more attempt to try to achieve the performance requirements using Git. We built a custom storage mechanism for the awesome JGit library . This showed better promise, providing up to 20/commits second when run against DynamoDB. However, it was still insufficient. As such, we ultimately built our own commit kernel to power Nessie. In Nessie, we continue to include an experimental backing store built on top of JGit. This serves as both a good trial tool and homage to git.","title":"333 Commits/second?"},{"location":"develop/nessie_vs_git/#so-which-is-better","text":"Like all engineering solutions, this isn\u2019t about what is better, only what is better for a certain use case. Git is good at generalized version control. Nessie is good at data version control.","title":"So Which is Better"},{"location":"develop/nessie_vs_git/#nessie-vs-dvc","text":"DVC is a popular package within ML community that is described as \u201cVersion Control System for Machine Learning Projects\u201d it presents. While both Nessie and DVC are focused on data, DVC is focused on smaller datasets and maintaining the distributed capabilities of Git. This works great for individual projects that are typically run on single workstations where datasets can be replicated. Nessie works at a table and metadata level specifically focused on data management problems.","title":"Nessie vs DVC"},{"location":"develop/python/","text":"Python \u00b6 # using python 3 pip install pynessie Configuration \u00b6 When you install pynessie, you get the Python client along with a Python CLI. Configuration for both is covered in our reference for the command line interface . Usage \u00b6 To instantiate a client simply run from pynessie import init client = init () # this will look for the client config as per above branches = client . list_branches () print ( branches ) All endpoint options are available from this client. Spark usage from Python \u00b6 A common way to interact with Nessie is via Spark. You can read more about working with Nessie and Spark together on our Spark docs page. API Documentation \u00b6 API docs are hosted on readthedocs","title":"Python"},{"location":"develop/python/#python","text":"# using python 3 pip install pynessie","title":"Python"},{"location":"develop/python/#configuration","text":"When you install pynessie, you get the Python client along with a Python CLI. Configuration for both is covered in our reference for the command line interface .","title":"Configuration"},{"location":"develop/python/#usage","text":"To instantiate a client simply run from pynessie import init client = init () # this will look for the client config as per above branches = client . list_branches () print ( branches ) All endpoint options are available from this client.","title":"Usage"},{"location":"develop/python/#spark-usage-from-python","text":"A common way to interact with Nessie is via Spark. You can read more about working with Nessie and Spark together on our Spark docs page.","title":"Spark usage from Python"},{"location":"develop/python/#api-documentation","text":"API docs are hosted on readthedocs","title":"API Documentation"},{"location":"develop/rest/","text":"Rest API \u00b6 Nessie\u2019s REST APIs are how all applications interact with Nessie. The APIs are specified according to the openapi v3 standard and are available when running the server by going to localhost:19120/openapi . You can also peruse the set of operations our APIs support by going to SwaggerHub . If you are working in development, our Quarkus server will automatically start with the swagger-ui for experimentation. You can find that at localhost:19120/swagger-ui","title":"Rest API"},{"location":"develop/rest/#rest-api","text":"Nessie\u2019s REST APIs are how all applications interact with Nessie. The APIs are specified according to the openapi v3 standard and are available when running the server by going to localhost:19120/openapi . You can also peruse the set of operations our APIs support by going to SwaggerHub . If you are working in development, our Quarkus server will automatically start with the swagger-ui for experimentation. You can find that at localhost:19120/swagger-ui","title":"Rest API"},{"location":"develop/spec/","text":"Nessie Specification \u00b6 This page documents the complete nessie specification. This includes: API and its constraints Contract for value objects requirements for backend storage implementations API contract \u00b6 Warning todo Contract for Value Objects \u00b6 General Contract \u00b6 All contents object must have an id field. This field is unique to the object and immutable once created. By convention, it is a UUID though this is not enforced by the Specification. There are several expectations on this field: They are immutable. Once created the object will keep the same id for its entire lifetime If the object is moved (eg stored under a different Key ) it will keep the id Two objects with the same key (eg on different branches) will only have the same id if the object is the same (eg the same iceberg table or view) There is no API to look up an object by id and the intention of an id is not to serve in that capacity. An example usage of the id field might be storing auxiliary data on an object in a local cache and using id to look up that auxiliary data. Iceberg Table \u00b6 An Iceberg table object is exceedingly simple. The Iceberg table object only stores the path to the metadata JSON document that should be read to describe the Iceberg table at this version. This model puts a strong restriction on the Iceberg table. All metadata JSON documents must be stored and none of the built-in iceberg maintenance procedures can be used. There are potentially serious issues regarding schema migrations in this model as well. Therefore, the Iceberg table spec should be considered subject to change in the near future. View \u00b6 Delta Lake Table \u00b6 Hive Table & Database \u00b6 Contract for backing database \u00b6 Warning todo Version Store \u00b6 Warning todo Tiered Version Store \u00b6 Warning todo","title":"Nessie Specification"},{"location":"develop/spec/#nessie-specification","text":"This page documents the complete nessie specification. This includes: API and its constraints Contract for value objects requirements for backend storage implementations","title":"Nessie Specification"},{"location":"develop/spec/#api-contract","text":"Warning todo","title":"API contract"},{"location":"develop/spec/#contract-for-value-objects","text":"","title":"Contract for Value Objects"},{"location":"develop/spec/#general-contract","text":"All contents object must have an id field. This field is unique to the object and immutable once created. By convention, it is a UUID though this is not enforced by the Specification. There are several expectations on this field: They are immutable. Once created the object will keep the same id for its entire lifetime If the object is moved (eg stored under a different Key ) it will keep the id Two objects with the same key (eg on different branches) will only have the same id if the object is the same (eg the same iceberg table or view) There is no API to look up an object by id and the intention of an id is not to serve in that capacity. An example usage of the id field might be storing auxiliary data on an object in a local cache and using id to look up that auxiliary data.","title":"General Contract"},{"location":"develop/spec/#iceberg-table","text":"An Iceberg table object is exceedingly simple. The Iceberg table object only stores the path to the metadata JSON document that should be read to describe the Iceberg table at this version. This model puts a strong restriction on the Iceberg table. All metadata JSON documents must be stored and none of the built-in iceberg maintenance procedures can be used. There are potentially serious issues regarding schema migrations in this model as well. Therefore, the Iceberg table spec should be considered subject to change in the near future.","title":"Iceberg Table"},{"location":"develop/spec/#view","text":"","title":"View"},{"location":"develop/spec/#delta-lake-table","text":"","title":"Delta Lake Table"},{"location":"develop/spec/#hive-table-database","text":"","title":"Hive Table &amp; Database"},{"location":"develop/spec/#contract-for-backing-database","text":"Warning todo","title":"Contract for backing database"},{"location":"develop/spec/#version-store","text":"Warning todo","title":"Version Store"},{"location":"develop/spec/#tiered-version-store","text":"Warning todo","title":"Tiered Version Store"},{"location":"features/","text":"About Nessie \u00b6 Nessie is to Data Lakes what Git is to source code repositories. Therefore, Nessie uses many terms from both Git and data lakes. This page explains how Nessie makes working with data in data lakes much easier without requiring much prior knowledge of either Git or data lakes. Nessie is designed to give users an always-consistent view of their data across all involved data sets (tables). Changes to your data, for example from batch jobs, happen independently and are completely isolated. Users will not see any incomplete changes. Once all the changes are done, all the changes can be atomically and consistently applied and become visible to your users. Nessie completely eliminates the hard and often manual work required to keep track of the individual data files. Nessie knows which data files are being used and which data files can safely be deleted. Production, staging and development environments can use the same data lake without risking the consistent state of production data. Nessie does not copy your data, instead it references the existing data, which works fine, because data files 1 are immutable. Nessie 101 \u00b6 Changes to the contents of the data lake are recorded in Nessie as commits without copying the actual data. Add meaning to the changes to your data lake. Always-consistent view to all the data. Sets of changes, like the whole work of a distributed Spark job . or experiments of data engineers are isolated in Nessie via branches . Failed jobs do not add additional harm to the data. Known, fixed versions of all data can be tagged . Automatic removal of unused data files ( garbage collection ). Data Lake 101 \u00b6 \u201cA data lake is a system or repository of data stored in its natural/raw format, usually object blobs or files.\u201d (cite from Wikipedia ) Data is stored in immutable data files 1 . Each data file defines the schema of the data (i.e. names and types of the columns) and contains the data. A single, logical table (for example a customers or a bank_account_transactions table) consists of many data files. A common (mis)understanding of Data Lakes is \u201cthrow everything in and see what happens\u201d. This might work for some time, leaving data, especially large amounts of data, unorganized is a rather bad idea. A common best-practice is still to properly organize the (immutable) data files in directories that reflect both organizational (think: units/groups in your company) and structural (think: table schema) aspects. New data files can be added to the set of files for a particular table. Data files can also contain updates to and deletions of existing data. For example: if you need to make changes to the data in data-file A , you basically have to read that data-file, apply the changes and write a new data-file A' with the changes, which makes data-file A irrelevant. The amount of data held in data lakes is rather huge (GBs, TBs, PBs), and so is the number of tables and data files (100s of thousands, millions). Managing that amount of data and data files while keeping track of schema changes, for example adding or removing a column, changing a column\u2019s type, renaming a column in a table and views, is one of the things that Nessie tackles. Data in a data lake is usually consumed and written using tools like Apache Hive 2 or Apache Spark 2 . Your existing jobs can easily integrate Nessie without any production code changes, it\u2019s a simple configuration change. Git 101 \u00b6 \u201cGit is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency\u201d (cite from git-scm.com ) Git maintains the history or all changes of a software project from the very first commit until the current state. Git is used by humans, i.e. developers. Many of the concepts of Git for source code are implemented by Nessie for all the data your data lake. It would be rather confusing to explain all Git concepts here and then outline the differences in the next chapter. If you want to learn more about Git, we recommend looking this Git book (available in many languages) or the About Git pages as a quick start. Terms summary \u00b6 Term Meaning in Nessie Commit An atomic change to a set of data files. Hash Nessie-commits are identified by a SHA-hash. 3 (Multi-table) transaction Since a Nessie commit can group data data files from many tables, you can think of a Nessie commit as a (multi-table) transaction. Branch Named reference to a commit. A new commit to a branch updates the branch to the new commit. Tag Named reference to a commit. Not automatically changed. Merge Combination of two commits. Usually applies the changes of one source-branch onto another target-branch. Working with data in Nessie \u00b6 Each individual state in Nessie is defined by a Nessie commit . Each commit in Nessie, except the very first one, has references to its predecessor, the previous versions of the data. For those who know Git and merge-commits: One important difference of Nessie-merges is that Nessie-commits have only parent (predecessor). Nessie-merge operations technically work a bit different: the changes in branch to be merged are replayed on top of the target branch. Each Nessie commit also indirectly \u201cknows\u201d about the data files (via some metadata) in your data lake, which represent the state of all data in all tables. The following example illustrates that our current commit adds a 3 rd data file. The other two data files 1+2 have been added by previous commit . +-------------------+ +-------------------------+ | previous commit | --<-- | current commit | +-------------------+ +-------------------------+ | | | | | (add) (add) | | (add) | | | | | +------+ +------+ +------+ +------+ +------+ | data | | data | | data | | data | | data | | file | | file | | file | | file | | file | | #1 | | #2 | | #1 | | #2 | | #3 | | _| | _| | _| | _| | _| | __/ | __/ | __/ | __/ | __/ |_/ |_/ |_/ |_/ |_/ In \u201crelational SQL\u201d you can think of the following sequence of SQL statements: BEGIN TRANSACTION ; -- The data for data file #1 INSERT INTO table_one (...) VALUES (...); -- The data for data file #2 INSERT INTO other_table (...) VALUES (...); -- creates our \"previous commit\" COMMIT TRANSACTION ; BEGIN TRANSACTION ; -- Data added to 'table_one' will \"land\" in a new data file #3, because -- data files are immutable. INSERT INTO table_one (...) VALUES (...); -- Creates our \"current commit\" COMMIT TRANSACTION ; Each commit is identified by a sequence of hexadecimal characters like 2898591840e992ec5a7d5c811c58c8b42a8e0d0914f86a37badbeedeadaffe 3 , which is not easy to read and remember for us humans. Transaction in Nessie \u00b6 The term \u201ctransaction\u201d has different meanings to different people coming from different backgrounds. It is probably fair to say that, in general, a transaction is a group of changes applied to some data. The term \u201ctransaction\u201d alone does not define any guarantees. Different systems provide different guarantees, for example whether (or: when) changes performed in a transaction become visible to others, whether (parts of) the data gets locked, and so on. Relational database systems (RDBMS) for example usually provide certain levels of isolation (think: others cannot see uncommitted changes) and also ensure that either a change within a transaction succeeds, the request times out or fails straight away. Relational databases have a single and central transaction-coordinator 4 and are designed to always provide a consistent data set. The smallest atomic change in Nessie is a single commit. It is fair to say, that a commit is the smallest possible transaction in Nessie. A single Nessie commit in Nessie: \u2026 can be \u201cjust\u201d the set of changes of a single worker out of many distributed workers. \u2026 can cover a quite small change or cover a huge amount of changes and/or huge amount of changed data or even group many Nessie commits into an atomic merge operation (think: a transaction over many transactions). The major difference between \u201cNessie\u2019s (distributed) transactions\u201d and transactions in a relational database is that Nessie\u2019s concept of having multiple commits plus the concept of merging one branch into another branch provides a lot of flexibility. Branches \u00b6 Nessie uses the concept of \u201cbranches\u201d to always reference the latest version in a chain of commits. Our example branch is named \u201cmain\u201d and has just a single commit: +-------------+ | commit #1 | +-------------+ ^ | | \"main\" branch When we add changes to our \u201cmain\u201d branch, a new commit #2 will be created: the new commit #2 will reference commit #1 as its predecessor and the named reference \u201cmain\u201d will be updated to point to our new commit #2 +-------------+ +-------------+ | commit #1 | --<-- | commit #2 | +-------------+ +-------------+ ^ | | \"main\" branch This behavior ensures that the named reference \u201cmain\u201d always points to the very latest version of our data. Working-branches for analytics jobs \u00b6 The above example with a single branch works well, if all changes to all tables can be grouped into a single commit. In a distributed world, computational work is distributed across many machines running many processes. All these individual tasks generate commits, but only the \u201csum\u201d of all commits from all the tasks represents a consistent state. If all the tasks of a job would directly commit onto our \u201cmain\u201d branch, the \u201cmain\u201d branch would be inconsistent at least until not all tasks have finished. Further, if the whole job fails, it would be hard to rollback the changes, especially if other jobs are running. Last but not least, the \u201cmain\u201d branch would contain a lot of commits (for example job#213, task#47346, add 1234 rows to table x ), which do not make a lot of sense on their own, but a single commit (for example aggregate-financial-stuff 2020/12/24 ) would. To get around that issue, jobs can create a new \u201cwork\u201d-branch when they start. The results from all tasks of a job are recorded as individual commits into that \u201cwork\u201d-branch. Once the job has finished, all changes are then merged into the \u201cmain\u201d branch at once. \"work\" branch | | v +-----------+ | commit #1 | +-----------+ ^ | | \"main\" branch Our example Spark job has two tasks, each generates a separate commit, which are only visible on our \u201cwork\u201d-branch: task#1 task#2 \"work\" result result branch | | | v v v +-----------+ +-----------+ | commit #2 | --<-- | commit #3 | +-----------+ +-----------+ | v | +-----------+ | commit #1 | +-----------+ ^ | | \"main\" branch When the job has finished, you can merge the now consistent result back into the \u201cmain\u201d-branch. task#1 task#2 \"work\" result result branch | | | v v v +-----------+ +-----------+ | commit #2 | --<-- | commit #3 | +-----------+ +-----------+ | | v ^ | | +-----------+ +-----------+ | commit #1 | --------<---------- | commit #4 | +-----------+ +-----------+ ^ | | \"main\" branch Technically, Nessie replays commit #2 and commit #3 on top of the most-recent commit of the \u201cmain\u201d branch. For those who know Git and merge-commits: One important difference of Nessie-merges is that Nessie-commits have only parent (predecessor). Nessie-merge operations technically work a bit different: the changes in branch to be merged are replayed on top of the target branch. It is recommended to give a commit a meaningful commit message and to let someone review the changes . As described above in Transactions in Nessie , the merge operation in the above example can be considered a Nessie distributed transaction . Working branches for \u201chumans\u201d \u00b6 You can also use \u201cdeveloper\u201d branches to run experiments against your data, test changes of your jobs etc. Production, staging and development environments can use the same data lake without risking the consistent state of production data. Squashing \u00b6 Nessie can not yet squash commits. Tags \u00b6 Another type of named references are tags . Nessie tags are named references to specific commits. Tags do always point to the same commit and won\u2019t be changed automatically. This means, that tags are useful to reference specific commits, for example a tag named financial-data-of-FY2021 could reference all sources of financial data relevant used for some financial year report. See Git tags for comparison and to learn how tagging works in Git. Commit messages and more \u00b6 As briefly mentioned above, every commit in Nessie has a set of attributes. Some of the more important ones are \u201csummary\u201d and \u201cdescription\u201d, which are exactly that - meaningful summaries and detailed descriptions that explain what has been changed and why it has been changed. In addition to \u201csummary\u201d and \u201cdescription\u201d, there are a bunch of additional attributes as shown in the following table. We plan to add more structure to these attributes in the future. Attribute Meaning in Nessie commit timestamp The timestamp when the commit was recorded in Nessie. committer The one (human user, system id) that actually recorded the change in Nessie. author timestamp the timestamp when a change has been implemented (can be different from the commit timestamp). author The one (human user, system id) that authored the change, can be different if someone else actually commits the change to Nessie. summary A short, one-line meaningful summary of the changes. description potentially long description of the changes. \u2026 There are potentially way more attributes, just too many to mention here. Garbage collection \u00b6 Data lakes contain a lot of data. The amount of data has a direct relation to the cost of ownership of a data lake. Keeping all data forever is probably going to be just too expensive, practically not useful and can also collide with data privacy regulations (for example GDPR or CCPA). Nessie keeps track of unused data files and collects the garbage for you. See Table Management Footnotes \u00b6 Common data file formats are Apache Iceberg Tables , Delta Lake Tables , Hive Metastore Tables \u21a9 \u21a9 Apache, Hive, Spark, Iceberg, Parquet are trademarks of The Apache Software Foundation. \u21a9 \u21a9 Nessie-commits are identified by a SHA-hash. All commits in Nessie (and in Git) are identified using such a hash. The value of each hash is generated from the relevant contents and attributes of each commit that are stored in Nessie. \u21a9 \u21a9 There are distributed relational databases that are not implemented as a single monolith. Those \u201cproper\u201d distributed relational databases use distributed consensus algorithms like RAFT to provide the same (or even better) guarantees that classic relational databases give. However, the concepts of a classic relational database still apply. \u21a9","title":"About Nessie"},{"location":"features/#about-nessie","text":"Nessie is to Data Lakes what Git is to source code repositories. Therefore, Nessie uses many terms from both Git and data lakes. This page explains how Nessie makes working with data in data lakes much easier without requiring much prior knowledge of either Git or data lakes. Nessie is designed to give users an always-consistent view of their data across all involved data sets (tables). Changes to your data, for example from batch jobs, happen independently and are completely isolated. Users will not see any incomplete changes. Once all the changes are done, all the changes can be atomically and consistently applied and become visible to your users. Nessie completely eliminates the hard and often manual work required to keep track of the individual data files. Nessie knows which data files are being used and which data files can safely be deleted. Production, staging and development environments can use the same data lake without risking the consistent state of production data. Nessie does not copy your data, instead it references the existing data, which works fine, because data files 1 are immutable.","title":"About Nessie"},{"location":"features/#nessie-101","text":"Changes to the contents of the data lake are recorded in Nessie as commits without copying the actual data. Add meaning to the changes to your data lake. Always-consistent view to all the data. Sets of changes, like the whole work of a distributed Spark job . or experiments of data engineers are isolated in Nessie via branches . Failed jobs do not add additional harm to the data. Known, fixed versions of all data can be tagged . Automatic removal of unused data files ( garbage collection ).","title":"Nessie 101"},{"location":"features/#data-lake-101","text":"\u201cA data lake is a system or repository of data stored in its natural/raw format, usually object blobs or files.\u201d (cite from Wikipedia ) Data is stored in immutable data files 1 . Each data file defines the schema of the data (i.e. names and types of the columns) and contains the data. A single, logical table (for example a customers or a bank_account_transactions table) consists of many data files. A common (mis)understanding of Data Lakes is \u201cthrow everything in and see what happens\u201d. This might work for some time, leaving data, especially large amounts of data, unorganized is a rather bad idea. A common best-practice is still to properly organize the (immutable) data files in directories that reflect both organizational (think: units/groups in your company) and structural (think: table schema) aspects. New data files can be added to the set of files for a particular table. Data files can also contain updates to and deletions of existing data. For example: if you need to make changes to the data in data-file A , you basically have to read that data-file, apply the changes and write a new data-file A' with the changes, which makes data-file A irrelevant. The amount of data held in data lakes is rather huge (GBs, TBs, PBs), and so is the number of tables and data files (100s of thousands, millions). Managing that amount of data and data files while keeping track of schema changes, for example adding or removing a column, changing a column\u2019s type, renaming a column in a table and views, is one of the things that Nessie tackles. Data in a data lake is usually consumed and written using tools like Apache Hive 2 or Apache Spark 2 . Your existing jobs can easily integrate Nessie without any production code changes, it\u2019s a simple configuration change.","title":"Data Lake 101"},{"location":"features/#git-101","text":"\u201cGit is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency\u201d (cite from git-scm.com ) Git maintains the history or all changes of a software project from the very first commit until the current state. Git is used by humans, i.e. developers. Many of the concepts of Git for source code are implemented by Nessie for all the data your data lake. It would be rather confusing to explain all Git concepts here and then outline the differences in the next chapter. If you want to learn more about Git, we recommend looking this Git book (available in many languages) or the About Git pages as a quick start.","title":"Git 101"},{"location":"features/#terms-summary","text":"Term Meaning in Nessie Commit An atomic change to a set of data files. Hash Nessie-commits are identified by a SHA-hash. 3 (Multi-table) transaction Since a Nessie commit can group data data files from many tables, you can think of a Nessie commit as a (multi-table) transaction. Branch Named reference to a commit. A new commit to a branch updates the branch to the new commit. Tag Named reference to a commit. Not automatically changed. Merge Combination of two commits. Usually applies the changes of one source-branch onto another target-branch.","title":"Terms summary"},{"location":"features/#working-with-data-in-nessie","text":"Each individual state in Nessie is defined by a Nessie commit . Each commit in Nessie, except the very first one, has references to its predecessor, the previous versions of the data. For those who know Git and merge-commits: One important difference of Nessie-merges is that Nessie-commits have only parent (predecessor). Nessie-merge operations technically work a bit different: the changes in branch to be merged are replayed on top of the target branch. Each Nessie commit also indirectly \u201cknows\u201d about the data files (via some metadata) in your data lake, which represent the state of all data in all tables. The following example illustrates that our current commit adds a 3 rd data file. The other two data files 1+2 have been added by previous commit . +-------------------+ +-------------------------+ | previous commit | --<-- | current commit | +-------------------+ +-------------------------+ | | | | | (add) (add) | | (add) | | | | | +------+ +------+ +------+ +------+ +------+ | data | | data | | data | | data | | data | | file | | file | | file | | file | | file | | #1 | | #2 | | #1 | | #2 | | #3 | | _| | _| | _| | _| | _| | __/ | __/ | __/ | __/ | __/ |_/ |_/ |_/ |_/ |_/ In \u201crelational SQL\u201d you can think of the following sequence of SQL statements: BEGIN TRANSACTION ; -- The data for data file #1 INSERT INTO table_one (...) VALUES (...); -- The data for data file #2 INSERT INTO other_table (...) VALUES (...); -- creates our \"previous commit\" COMMIT TRANSACTION ; BEGIN TRANSACTION ; -- Data added to 'table_one' will \"land\" in a new data file #3, because -- data files are immutable. INSERT INTO table_one (...) VALUES (...); -- Creates our \"current commit\" COMMIT TRANSACTION ; Each commit is identified by a sequence of hexadecimal characters like 2898591840e992ec5a7d5c811c58c8b42a8e0d0914f86a37badbeedeadaffe 3 , which is not easy to read and remember for us humans.","title":"Working with data in Nessie"},{"location":"features/#transaction-in-nessie","text":"The term \u201ctransaction\u201d has different meanings to different people coming from different backgrounds. It is probably fair to say that, in general, a transaction is a group of changes applied to some data. The term \u201ctransaction\u201d alone does not define any guarantees. Different systems provide different guarantees, for example whether (or: when) changes performed in a transaction become visible to others, whether (parts of) the data gets locked, and so on. Relational database systems (RDBMS) for example usually provide certain levels of isolation (think: others cannot see uncommitted changes) and also ensure that either a change within a transaction succeeds, the request times out or fails straight away. Relational databases have a single and central transaction-coordinator 4 and are designed to always provide a consistent data set. The smallest atomic change in Nessie is a single commit. It is fair to say, that a commit is the smallest possible transaction in Nessie. A single Nessie commit in Nessie: \u2026 can be \u201cjust\u201d the set of changes of a single worker out of many distributed workers. \u2026 can cover a quite small change or cover a huge amount of changes and/or huge amount of changed data or even group many Nessie commits into an atomic merge operation (think: a transaction over many transactions). The major difference between \u201cNessie\u2019s (distributed) transactions\u201d and transactions in a relational database is that Nessie\u2019s concept of having multiple commits plus the concept of merging one branch into another branch provides a lot of flexibility.","title":"Transaction in Nessie"},{"location":"features/#branches","text":"Nessie uses the concept of \u201cbranches\u201d to always reference the latest version in a chain of commits. Our example branch is named \u201cmain\u201d and has just a single commit: +-------------+ | commit #1 | +-------------+ ^ | | \"main\" branch When we add changes to our \u201cmain\u201d branch, a new commit #2 will be created: the new commit #2 will reference commit #1 as its predecessor and the named reference \u201cmain\u201d will be updated to point to our new commit #2 +-------------+ +-------------+ | commit #1 | --<-- | commit #2 | +-------------+ +-------------+ ^ | | \"main\" branch This behavior ensures that the named reference \u201cmain\u201d always points to the very latest version of our data.","title":"Branches"},{"location":"features/#working-branches-for-analytics-jobs","text":"The above example with a single branch works well, if all changes to all tables can be grouped into a single commit. In a distributed world, computational work is distributed across many machines running many processes. All these individual tasks generate commits, but only the \u201csum\u201d of all commits from all the tasks represents a consistent state. If all the tasks of a job would directly commit onto our \u201cmain\u201d branch, the \u201cmain\u201d branch would be inconsistent at least until not all tasks have finished. Further, if the whole job fails, it would be hard to rollback the changes, especially if other jobs are running. Last but not least, the \u201cmain\u201d branch would contain a lot of commits (for example job#213, task#47346, add 1234 rows to table x ), which do not make a lot of sense on their own, but a single commit (for example aggregate-financial-stuff 2020/12/24 ) would. To get around that issue, jobs can create a new \u201cwork\u201d-branch when they start. The results from all tasks of a job are recorded as individual commits into that \u201cwork\u201d-branch. Once the job has finished, all changes are then merged into the \u201cmain\u201d branch at once. \"work\" branch | | v +-----------+ | commit #1 | +-----------+ ^ | | \"main\" branch Our example Spark job has two tasks, each generates a separate commit, which are only visible on our \u201cwork\u201d-branch: task#1 task#2 \"work\" result result branch | | | v v v +-----------+ +-----------+ | commit #2 | --<-- | commit #3 | +-----------+ +-----------+ | v | +-----------+ | commit #1 | +-----------+ ^ | | \"main\" branch When the job has finished, you can merge the now consistent result back into the \u201cmain\u201d-branch. task#1 task#2 \"work\" result result branch | | | v v v +-----------+ +-----------+ | commit #2 | --<-- | commit #3 | +-----------+ +-----------+ | | v ^ | | +-----------+ +-----------+ | commit #1 | --------<---------- | commit #4 | +-----------+ +-----------+ ^ | | \"main\" branch Technically, Nessie replays commit #2 and commit #3 on top of the most-recent commit of the \u201cmain\u201d branch. For those who know Git and merge-commits: One important difference of Nessie-merges is that Nessie-commits have only parent (predecessor). Nessie-merge operations technically work a bit different: the changes in branch to be merged are replayed on top of the target branch. It is recommended to give a commit a meaningful commit message and to let someone review the changes . As described above in Transactions in Nessie , the merge operation in the above example can be considered a Nessie distributed transaction .","title":"Working-branches for analytics jobs"},{"location":"features/#working-branches-for-humans","text":"You can also use \u201cdeveloper\u201d branches to run experiments against your data, test changes of your jobs etc. Production, staging and development environments can use the same data lake without risking the consistent state of production data.","title":"Working branches for \"humans\""},{"location":"features/#squashing","text":"Nessie can not yet squash commits.","title":"Squashing"},{"location":"features/#tags","text":"Another type of named references are tags . Nessie tags are named references to specific commits. Tags do always point to the same commit and won\u2019t be changed automatically. This means, that tags are useful to reference specific commits, for example a tag named financial-data-of-FY2021 could reference all sources of financial data relevant used for some financial year report. See Git tags for comparison and to learn how tagging works in Git.","title":"Tags"},{"location":"features/#commit-messages-and-more","text":"As briefly mentioned above, every commit in Nessie has a set of attributes. Some of the more important ones are \u201csummary\u201d and \u201cdescription\u201d, which are exactly that - meaningful summaries and detailed descriptions that explain what has been changed and why it has been changed. In addition to \u201csummary\u201d and \u201cdescription\u201d, there are a bunch of additional attributes as shown in the following table. We plan to add more structure to these attributes in the future. Attribute Meaning in Nessie commit timestamp The timestamp when the commit was recorded in Nessie. committer The one (human user, system id) that actually recorded the change in Nessie. author timestamp the timestamp when a change has been implemented (can be different from the commit timestamp). author The one (human user, system id) that authored the change, can be different if someone else actually commits the change to Nessie. summary A short, one-line meaningful summary of the changes. description potentially long description of the changes. \u2026 There are potentially way more attributes, just too many to mention here.","title":"Commit messages and more"},{"location":"features/#garbage-collection","text":"Data lakes contain a lot of data. The amount of data has a direct relation to the cost of ownership of a data lake. Keeping all data forever is probably going to be just too expensive, practically not useful and can also collide with data privacy regulations (for example GDPR or CCPA). Nessie keeps track of unused data files and collects the garbage for you. See Table Management","title":"Garbage collection"},{"location":"features/#footnotes","text":"Common data file formats are Apache Iceberg Tables , Delta Lake Tables , Hive Metastore Tables \u21a9 \u21a9 Apache, Hive, Spark, Iceberg, Parquet are trademarks of The Apache Software Foundation. \u21a9 \u21a9 Nessie-commits are identified by a SHA-hash. All commits in Nessie (and in Git) are identified using such a hash. The value of each hash is generated from the relevant contents and attributes of each commit that are stored in Nessie. \u21a9 \u21a9 There are distributed relational databases that are not implemented as a single monolith. Those \u201cproper\u201d distributed relational databases use distributed consensus algorithms like RAFT to provide the same (or even better) guarantees that classic relational databases give. However, the concepts of a classic relational database still apply. \u21a9","title":"Footnotes"},{"location":"features/best-practices/","text":"Best Practices \u00b6 Commit Messages \u00b6 Give Nessie commits a meaningful commit summary and message, like aggregate-financial-stuff 2020/12/24 , so people that look through the history of the data can grasp what that commit changes and why it\u2019s there. Reviews \u00b6 Before merging manually performed changes back, it is really helpful to let someone else who is familiar with the topic, the changes applied in a work-branch (aka \u201cdevelopment branch\u201d), review the changes.","title":"Best Practices"},{"location":"features/best-practices/#best-practices","text":"","title":"Best Practices"},{"location":"features/best-practices/#commit-messages","text":"Give Nessie commits a meaningful commit summary and message, like aggregate-financial-stuff 2020/12/24 , so people that look through the history of the data can grasp what that commit changes and why it\u2019s there.","title":"Commit Messages"},{"location":"features/best-practices/#reviews","text":"Before merging manually performed changes back, it is really helpful to let someone else who is familiar with the topic, the changes applied in a work-branch (aka \u201cdevelopment branch\u201d), review the changes.","title":"Reviews"},{"location":"features/intro/","text":"Introduction \u00b6 Nessie is an OSS service and libraries that enable you to maintain multiple versions of your data and leverage Git-like Branches & Tags for your Data Lake. Nessie enhances the following table formats with version control techniques: Apache Iceberg Tables ( more ) Delta Lake Tables ( more ) Hive Metastore Tables ( more ) SQL Views ( more ) Basic Concepts \u00b6 Nessie is heavily inspired by Git. The main concepts Nessie exposes map directly to Git concepts . In most cases, you simply need to replace references of files and directories in Git with Tables in Nessie. The primary concepts in Nessie are: Commit: Consistent snapshot of all tables at a particular point in time. Branch: Human-friendly reference that a user can add commits to. Tag: Human-friendly reference that points to a particular commit. Hash: Hexadecimal string representation of a particular commit. Out of the box, Nessie starts with a single branch called main that points to the beginning of time. A user can immediately start adding tables to that branch. For example (in pseudo code): $ create t1 ... $ insert 2 records into t1 ... $ create t2 ... $ insert 2 records into t2 ... A user can then use the Nessie CLI to view the history of the main branch. You\u2019ll see that each operation was automatically recorded as a commit within Nessie: $ nessie log hash4 t2 data added hash3 t2 created hash2 t1 data added hash1 t1 created A user can then create a new tag referencing this point in time. After doing so, a user can continue changing the tables but that point in time snapshot will maintain that version of data. $ nessie tag mytag hash4 $ insert records into t1 $ select count(*) from t1 join t2 .. record 1 .. .. record 2 .. .. record 3 .. .. 3 records .. $ select count(*) from t1@mytag join t2@mytag .. record 1 .. .. record 2 .. .. only 2 records .. Data and Metadata \u00b6 Nessie does not make copies of your underlying data. Instead, it works to version separate lists of files associated with your dataset. Whether using Spark, Hive or some other tool, each mutation operation you do will add or delete one or more files from the definition of your table. Nessie keeps tracks of which files are related to each of your tables at every point in time and then allows you to recall those as needed. Scale & Performance \u00b6 Nessie is built for very large data warehouses. Nessie supports millions of tables and thousands of commits/second. Because Nessie builds on top of Iceberg and Delta Lake, each table can have millions of files. As such, Nessie can support data warehouses several magnitudes larger than the largest in the world today. This is possible in large part due to the separation of transaction management (Nessie) from table metadata management (Iceberg and Delta Lake). Technology \u00b6 Nessie can be deployed in multiple ways and is composed primarily of the Nessie service, which exposes a set of REST APIs and a simple browser UI. This service works with multiple libraries to expose Nessie\u2019s version control capabilities to common data management technologies. Nessie was built as a Cloud native technology and is designed to be highly scalable, performant and resilient. Built on Java and leveraging Quarkus , it is compiled to a GraalVM native image that starts in less than 20ms. This makes Nessie work very well in Docker and FaaS environments. Nessie has a pluggable storage backend and comes pre-packaged with support for DynamoDB and local storage. License and Governance \u00b6 Nessie is Apache-Licensed and built in an open source, consensus-driven GitHub community. Nessie was originally conceived and built by engineers at Dremio . Getting Started \u00b6 Read more about Nessie transactions Get started with the Nessie quickstart .","title":"Introduction"},{"location":"features/intro/#introduction","text":"Nessie is an OSS service and libraries that enable you to maintain multiple versions of your data and leverage Git-like Branches & Tags for your Data Lake. Nessie enhances the following table formats with version control techniques: Apache Iceberg Tables ( more ) Delta Lake Tables ( more ) Hive Metastore Tables ( more ) SQL Views ( more )","title":"Introduction"},{"location":"features/intro/#basic-concepts","text":"Nessie is heavily inspired by Git. The main concepts Nessie exposes map directly to Git concepts . In most cases, you simply need to replace references of files and directories in Git with Tables in Nessie. The primary concepts in Nessie are: Commit: Consistent snapshot of all tables at a particular point in time. Branch: Human-friendly reference that a user can add commits to. Tag: Human-friendly reference that points to a particular commit. Hash: Hexadecimal string representation of a particular commit. Out of the box, Nessie starts with a single branch called main that points to the beginning of time. A user can immediately start adding tables to that branch. For example (in pseudo code): $ create t1 ... $ insert 2 records into t1 ... $ create t2 ... $ insert 2 records into t2 ... A user can then use the Nessie CLI to view the history of the main branch. You\u2019ll see that each operation was automatically recorded as a commit within Nessie: $ nessie log hash4 t2 data added hash3 t2 created hash2 t1 data added hash1 t1 created A user can then create a new tag referencing this point in time. After doing so, a user can continue changing the tables but that point in time snapshot will maintain that version of data. $ nessie tag mytag hash4 $ insert records into t1 $ select count(*) from t1 join t2 .. record 1 .. .. record 2 .. .. record 3 .. .. 3 records .. $ select count(*) from t1@mytag join t2@mytag .. record 1 .. .. record 2 .. .. only 2 records ..","title":"Basic Concepts"},{"location":"features/intro/#data-and-metadata","text":"Nessie does not make copies of your underlying data. Instead, it works to version separate lists of files associated with your dataset. Whether using Spark, Hive or some other tool, each mutation operation you do will add or delete one or more files from the definition of your table. Nessie keeps tracks of which files are related to each of your tables at every point in time and then allows you to recall those as needed.","title":"Data and Metadata"},{"location":"features/intro/#scale-performance","text":"Nessie is built for very large data warehouses. Nessie supports millions of tables and thousands of commits/second. Because Nessie builds on top of Iceberg and Delta Lake, each table can have millions of files. As such, Nessie can support data warehouses several magnitudes larger than the largest in the world today. This is possible in large part due to the separation of transaction management (Nessie) from table metadata management (Iceberg and Delta Lake).","title":"Scale &amp; Performance"},{"location":"features/intro/#technology","text":"Nessie can be deployed in multiple ways and is composed primarily of the Nessie service, which exposes a set of REST APIs and a simple browser UI. This service works with multiple libraries to expose Nessie\u2019s version control capabilities to common data management technologies. Nessie was built as a Cloud native technology and is designed to be highly scalable, performant and resilient. Built on Java and leveraging Quarkus , it is compiled to a GraalVM native image that starts in less than 20ms. This makes Nessie work very well in Docker and FaaS environments. Nessie has a pluggable storage backend and comes pre-packaged with support for DynamoDB and local storage.","title":"Technology"},{"location":"features/intro/#license-and-governance","text":"Nessie is Apache-Licensed and built in an open source, consensus-driven GitHub community. Nessie was originally conceived and built by engineers at Dremio .","title":"License and Governance"},{"location":"features/intro/#getting-started","text":"Read more about Nessie transactions Get started with the Nessie quickstart .","title":"Getting Started"},{"location":"features/management/","text":"Management Services \u00b6 Nessie can and needs to manage several operations within your data lake. Each management service can be scheduled and Nessie reports the outcome of each scheduled operation. Scheduled operations require that Nessie have access to a Spark cluster to complete those operations and many of them are distributed compute operations. Garbage Collection \u00b6 Since Nessie is maintaining many versions of metadata and data-pointers simultaneously, you must rely on Nessie to clean up old data. Nessie calls this garbage collection. There are at least two steps to a garbage collection action. The first steps are instructive, and the last step is destructive. Info currently the GC algorithm only works for Iceberg tables and Dynamo as a backend Identify Unreferenced Assets \u00b6 This is a spark job which should be run periodically to identify no longer referenced assets. Assets are defined as the set of files, records, entries etc that make up a table, view or other Nessie object. For example, iceberg assets are: * manifest files * manifest lists * data files * metadata files * the entire table directory on disk (if it is empty) To be marked as unreferenced an asset must either be: 1. No longer referenced by any branch or tag. For example, an entire branch was deleted, and a table on that branch is no longer accessible. 2. Assets created in a commit which has passed the (configurable) commit age. If they are not referenced by newer commits Identifying unreferenced assets is a non-destructive action. The result of the spark job is a Spark DataFrame of all the unreferenced assets. This dataframe is stored in an iceberg table managed by nessie at a configurable key. This table is referencable in Nessie so can be examined via Spark or any other Nessie/Iceberg compatible engine. An example of the table output is shown below. This action is designed to run concurrently to normal operational workloads and can/should be run regularly. This table is used as input into the destructive GC operation described below. Configuration and running \u00b6 GcActionsConfig actionsConfig, GcOptions gcConfig, TableIdentifier table The relevant configuration items are: | parameter | default value | description | |\u2014|\u2014|\u2014| | table | null | The Iceberg TableIdentifier to which the unreferenced assets should be written | | GcOptions.getBloomFilterCapacity | 10000000 | Size (number of items) of bloom filter for identification of referenced values | | GcOptions.getMaxAgeMicros | 7 days | age at which a commit starts to expire | | GcOptions.getTimeSlopMicros | 1 day | minimum age a values can be before it will be considered expired | | GcActionsConfig.getDynamoRegion | provider default | AWS Region of the Nessie DynamoDB | | GcActionsConfig.getDynamoEndpoint | provider default | Custom AWS endpoint of the Nessie DynamoDB | | GcActionsConfig.getStoreType | DYNAMO | only backend which supports GC | Running the action can be done simply by: GcActionsConfig actionsConfig = GcActionsConfig . builder (). build (); //use all defaults GcOptions gcOptions = GcOptions . builder (). build (); //use all defaults GcActions actions = new GcActions . Builder ( spark ) . setActionsConfig ( actionsConfig ) . setGcOptions ( gcOptions ) . setTable ( TABLE_IDENTIFIER ). build (); // (1) Dataset < Row > unreferencedAssets = actions . identifyUnreferencedAssets (); // (2) actions . updateUnreferencedAssetTable ( unreferencedAssets ); // (3) The first step above builds the action with known configs. Step 2 generates a DataFrame of unreferenced assets and Step 3 writes it as an iceberg table. Delete Unreferenced Assets \u00b6 The destructive garbage collection step is also a Spark job and takes as input the table that has been built above. This job is modelled as an Iceberg Action and has a similar API to the other Iceberg Actions. In the future it will be registered with Iceberg\u2019s Action APIs and callable via Iceberg\u2019s custom SQL statements . This Iceberg Action looks at the generated table from the Identify step and counts the number of times a distinct asset has been seen. Effectively it performs a group-by and count on this table. If the count of an asset is over a specified threshold AND it was seen in the last run of the Identify stage it is collectable. This asset is then deleted permanently. A report table of deleted object is returned to the user and either the records are removed from the \u2018identify\u2019 table or the whole table is purged. Configuration and running \u00b6 The relevant configuration items are: | parameter | default value | description | |\u2014|\u2014|\u2014| | seenCount | 10 | How many times an asset has been seen as unreferenced in order to be considered for deletion | | deleteOnPurge | true | Delete records from the underlying iceberg table of unreferenced assets | | dropGcTable | true | Drop the underlying iceberg table or attempt to clean only the missing rows | | table | null | The iceberg Table which stores the list of unreferenced assets | Running the action can be done simply by: Table table = catalog . loadTable ( TABLE_IDENTIFIER ); GcTableCleanAction . GcTableCleanResult result = new GcTableCleanAction ( table , spark ). dropGcTable ( true ). deleteCountThreshold ( 2 ). deleteOnPurge ( true ). execute (); The above snippet assumes a TABLE_IDENTIFIER which points to the unreferenced assets table. It also requires an active spark session and a nessie owned Catalog . See the demo directory for a complete example. The result object above returns the number of files the action tried to delete and the number that failed. Internal Garbage collection \u00b6 Currently the only garbage collection algorithm available is on the values and assets in a Nessie database only. The internal records of the Nessie Database are currently not cleaned up. Unreferenced objects stored in Nessie\u2019s internal database will be persisted forever currently. A future release will also clean up internal Nessie records if they are unreferenced. Time-based AutoTagging \u00b6 Info This service is currently in progress and is not yet included in a released version of Nessie. Nessie works against data based on a commit timeline. In many situations, it is useful to capture historical versions of data for analysis or comparison purposes. As such, you can configure Nessie to AutoTag (and auto-delete) using a timestamp based naming scheme. When enabled, Nessie will automatically generate and maintain tags based on time so that users can refer to historical data using timestamps as opposed to commits. This also works hand-in-hand with the Nessie garbage collection process by ensuring that older data is \u201creferenced\u201d and thus available for historical analysis. Currently there is one AutoTagging policy. By default, it creates the following tags: Hourly tags for the last 25 hours Daily tags for the last 8 days Weekly tags for the last 6 weeks Monthly tags for the last 13 months Yearly tags for the last 3 years Tags are automatically named using a date/ prefix and a zero-extended underscore based naming scheme. For example: date/2019_09_07_15_50 would be a tag for August 7, 2019 at 3:50pm. Warning AutoTags are automatically deleted once the policy rolls-over. As such, if retention is desired post roll-over, manual tags should be created. AutoTagging is currently done based on the UTC roll-over of each item. Manifest Reorganization \u00b6 Info This service is currently in progress and is not yet included in a released version of Nessie. Rewrites the manifests associated with a table so that manifest files are organized around partitions. This extends on the ideas in the Iceberg RewriteManifestsAction . Note Manifest reorganization will show up as a commit, like any other table operation. Key configuration parameters: Name Default Meaning effort medium How much rewriting is allowed to achieve the goals target manifest size 8mb What is the target partition priority medium How important achieving partition-oriented manifests. Compaction \u00b6 Info This service is currently in progress and is not yet included in a released version of Nessie. Because operations against table formats are done at the file level, a table can start to generate many small files. These small files will slow consumption. As such, Nessie can automatically run jobs to compact tables to ensure a consistent level of performance. Name Default Meaning Maximum Small Files 10.0 Maximum number of small files as a ratio to large files Maximum Delete Files 10.0 Maximum number of delete tombstones as a ratio to other files before merging the tombstones into a consolidated file Small File Size 100mb Size of file before it is considered small Target Rewrite Size 256mb The target size for splittable units when rewriting data. Note Compaction will show up as a commit, like any other table operation.","title":"Management Services"},{"location":"features/management/#management-services","text":"Nessie can and needs to manage several operations within your data lake. Each management service can be scheduled and Nessie reports the outcome of each scheduled operation. Scheduled operations require that Nessie have access to a Spark cluster to complete those operations and many of them are distributed compute operations.","title":"Management Services"},{"location":"features/management/#garbage-collection","text":"Since Nessie is maintaining many versions of metadata and data-pointers simultaneously, you must rely on Nessie to clean up old data. Nessie calls this garbage collection. There are at least two steps to a garbage collection action. The first steps are instructive, and the last step is destructive. Info currently the GC algorithm only works for Iceberg tables and Dynamo as a backend","title":"Garbage Collection"},{"location":"features/management/#identify-unreferenced-assets","text":"This is a spark job which should be run periodically to identify no longer referenced assets. Assets are defined as the set of files, records, entries etc that make up a table, view or other Nessie object. For example, iceberg assets are: * manifest files * manifest lists * data files * metadata files * the entire table directory on disk (if it is empty) To be marked as unreferenced an asset must either be: 1. No longer referenced by any branch or tag. For example, an entire branch was deleted, and a table on that branch is no longer accessible. 2. Assets created in a commit which has passed the (configurable) commit age. If they are not referenced by newer commits Identifying unreferenced assets is a non-destructive action. The result of the spark job is a Spark DataFrame of all the unreferenced assets. This dataframe is stored in an iceberg table managed by nessie at a configurable key. This table is referencable in Nessie so can be examined via Spark or any other Nessie/Iceberg compatible engine. An example of the table output is shown below. This action is designed to run concurrently to normal operational workloads and can/should be run regularly. This table is used as input into the destructive GC operation described below.","title":"Identify Unreferenced Assets"},{"location":"features/management/#configuration-and-running","text":"GcActionsConfig actionsConfig, GcOptions gcConfig, TableIdentifier table The relevant configuration items are: | parameter | default value | description | |\u2014|\u2014|\u2014| | table | null | The Iceberg TableIdentifier to which the unreferenced assets should be written | | GcOptions.getBloomFilterCapacity | 10000000 | Size (number of items) of bloom filter for identification of referenced values | | GcOptions.getMaxAgeMicros | 7 days | age at which a commit starts to expire | | GcOptions.getTimeSlopMicros | 1 day | minimum age a values can be before it will be considered expired | | GcActionsConfig.getDynamoRegion | provider default | AWS Region of the Nessie DynamoDB | | GcActionsConfig.getDynamoEndpoint | provider default | Custom AWS endpoint of the Nessie DynamoDB | | GcActionsConfig.getStoreType | DYNAMO | only backend which supports GC | Running the action can be done simply by: GcActionsConfig actionsConfig = GcActionsConfig . builder (). build (); //use all defaults GcOptions gcOptions = GcOptions . builder (). build (); //use all defaults GcActions actions = new GcActions . Builder ( spark ) . setActionsConfig ( actionsConfig ) . setGcOptions ( gcOptions ) . setTable ( TABLE_IDENTIFIER ). build (); // (1) Dataset < Row > unreferencedAssets = actions . identifyUnreferencedAssets (); // (2) actions . updateUnreferencedAssetTable ( unreferencedAssets ); // (3) The first step above builds the action with known configs. Step 2 generates a DataFrame of unreferenced assets and Step 3 writes it as an iceberg table.","title":"Configuration and running"},{"location":"features/management/#delete-unreferenced-assets","text":"The destructive garbage collection step is also a Spark job and takes as input the table that has been built above. This job is modelled as an Iceberg Action and has a similar API to the other Iceberg Actions. In the future it will be registered with Iceberg\u2019s Action APIs and callable via Iceberg\u2019s custom SQL statements . This Iceberg Action looks at the generated table from the Identify step and counts the number of times a distinct asset has been seen. Effectively it performs a group-by and count on this table. If the count of an asset is over a specified threshold AND it was seen in the last run of the Identify stage it is collectable. This asset is then deleted permanently. A report table of deleted object is returned to the user and either the records are removed from the \u2018identify\u2019 table or the whole table is purged.","title":"Delete Unreferenced Assets"},{"location":"features/management/#configuration-and-running_1","text":"The relevant configuration items are: | parameter | default value | description | |\u2014|\u2014|\u2014| | seenCount | 10 | How many times an asset has been seen as unreferenced in order to be considered for deletion | | deleteOnPurge | true | Delete records from the underlying iceberg table of unreferenced assets | | dropGcTable | true | Drop the underlying iceberg table or attempt to clean only the missing rows | | table | null | The iceberg Table which stores the list of unreferenced assets | Running the action can be done simply by: Table table = catalog . loadTable ( TABLE_IDENTIFIER ); GcTableCleanAction . GcTableCleanResult result = new GcTableCleanAction ( table , spark ). dropGcTable ( true ). deleteCountThreshold ( 2 ). deleteOnPurge ( true ). execute (); The above snippet assumes a TABLE_IDENTIFIER which points to the unreferenced assets table. It also requires an active spark session and a nessie owned Catalog . See the demo directory for a complete example. The result object above returns the number of files the action tried to delete and the number that failed.","title":"Configuration and running"},{"location":"features/management/#internal-garbage-collection","text":"Currently the only garbage collection algorithm available is on the values and assets in a Nessie database only. The internal records of the Nessie Database are currently not cleaned up. Unreferenced objects stored in Nessie\u2019s internal database will be persisted forever currently. A future release will also clean up internal Nessie records if they are unreferenced.","title":"Internal Garbage collection"},{"location":"features/management/#time-based-autotagging","text":"Info This service is currently in progress and is not yet included in a released version of Nessie. Nessie works against data based on a commit timeline. In many situations, it is useful to capture historical versions of data for analysis or comparison purposes. As such, you can configure Nessie to AutoTag (and auto-delete) using a timestamp based naming scheme. When enabled, Nessie will automatically generate and maintain tags based on time so that users can refer to historical data using timestamps as opposed to commits. This also works hand-in-hand with the Nessie garbage collection process by ensuring that older data is \u201creferenced\u201d and thus available for historical analysis. Currently there is one AutoTagging policy. By default, it creates the following tags: Hourly tags for the last 25 hours Daily tags for the last 8 days Weekly tags for the last 6 weeks Monthly tags for the last 13 months Yearly tags for the last 3 years Tags are automatically named using a date/ prefix and a zero-extended underscore based naming scheme. For example: date/2019_09_07_15_50 would be a tag for August 7, 2019 at 3:50pm. Warning AutoTags are automatically deleted once the policy rolls-over. As such, if retention is desired post roll-over, manual tags should be created. AutoTagging is currently done based on the UTC roll-over of each item.","title":"Time-based AutoTagging"},{"location":"features/management/#manifest-reorganization","text":"Info This service is currently in progress and is not yet included in a released version of Nessie. Rewrites the manifests associated with a table so that manifest files are organized around partitions. This extends on the ideas in the Iceberg RewriteManifestsAction . Note Manifest reorganization will show up as a commit, like any other table operation. Key configuration parameters: Name Default Meaning effort medium How much rewriting is allowed to achieve the goals target manifest size 8mb What is the target partition priority medium How important achieving partition-oriented manifests.","title":"Manifest Reorganization"},{"location":"features/management/#compaction","text":"Info This service is currently in progress and is not yet included in a released version of Nessie. Because operations against table formats are done at the file level, a table can start to generate many small files. These small files will slow consumption. As such, Nessie can automatically run jobs to compact tables to ensure a consistent level of performance. Name Default Meaning Maximum Small Files 10.0 Maximum number of small files as a ratio to large files Maximum Delete Files 10.0 Maximum number of delete tombstones as a ratio to other files before merging the tombstones into a consolidated file Small File Size 100mb Size of file before it is considered small Target Rewrite Size 256mb The target size for splittable units when rewriting data. Note Compaction will show up as a commit, like any other table operation.","title":"Compaction"},{"location":"features/security/","text":"Security \u00b6 Authentication \u00b6 Nessie currently supports 3 security modes: No Security Open Id Connect AWS IAM Roles (limited to API calls, UI not supported) Authorization \u00b6 Nessie authorization can only be done externally at the moment. However, because of the way that the REST APIs are defined, many operations can be controlled via a layer 7 firewall so that users and systems can be controlled depending on what read/write and types of operations should be allowed. This works especially well with Nessie run as a AWS Lambda using API gateway policies .","title":"Security"},{"location":"features/security/#security","text":"","title":"Security"},{"location":"features/security/#authentication","text":"Nessie currently supports 3 security modes: No Security Open Id Connect AWS IAM Roles (limited to API calls, UI not supported)","title":"Authentication"},{"location":"features/security/#authorization","text":"Nessie authorization can only be done externally at the moment. However, because of the way that the REST APIs are defined, many operations can be controlled via a layer 7 firewall so that users and systems can be controlled depending on what read/write and types of operations should be allowed. This works especially well with Nessie run as a AWS Lambda using API gateway policies .","title":"Authorization"},{"location":"features/transactions/","text":"Transactions \u00b6 Nessie extends existing table formats to provide a single serial view of transaction history. This is enabled across an unlimited number of tables. A user can view a commit log either through the UI or by using the Nessie CLI. Operations against each table are listed along with timestamp, user and helpful information about the operation. Cross-Table Transactions \u00b6 Nessie is the first technology to provide an open facility for cross-table transactions within your data lake. There are two ways that this can be accomplished: Via Branches: Because Nessie allows branches to be created and then reassigned, a sequence of commits on one branch can be exposed as a single consistent view to other users through the use of a merge operation. This allows use of systems that don\u2019t have internal cross-table transaction support to still control the visibility of connected changes. Single Commits: Nessie allows a single commit operation to include many object changes simultaneously. While Nessie operations internally use this capability, tools will need to be enhanced to take advantage of this powerful new capability. START TRANSACTION.. COMMIT \u00b6 The Nessie community is working with tool developers to introduce traditional data warehouse cross-table transactions. Nessie\u2019s catalog implementations already support the underlying capability of multi-table transactions. Isolation Levels \u00b6 Nessie exposes APIs to support three-levels of isolation: Read Committed, Repeated Read and Serialized. By supporting the recording of reads as part of a commit (via the Unchanged operation), tools can introduce full per operation serialized isolation. This is a transaction mode that has been traditionally limited to OLTP systems and unavailable to OLAP systems and data warehouses 1 . Info While Nessie exposes the necessary primitives to support configurable isolation, work still needs to be done with tool developers to ensure those tools expose these capabilities. As those integrations progress, we\u2019ll include more information about them here. At the moment, most tools operate in either Read Committed (Iceberg when calling refresh, Delta Lake) or Repeated Read (HMS Bridge operations, Iceberg when avoiding calls to refresh). Read Committed (Optimistic) \u00b6 Read Each time metadata for a table is retrieved, the latest version of that ref for the that current branch is exposed. Ownership A transaction only needs to be created on the client. There is not concept of a long-lived transaction. Write Safe writes are allowed. How Client goes to server to retrieve latest version of data for each operation Repeated Read (Optimistic) \u00b6 Read When a transaction is started, a ref is turned into a specific commit id. All metadata retrieved is locked to this hash or later, as long as future hashes have not changed any table already read. Ownership A transaction only needs to be created on the client. There is no concept of a long-lived transaction on the server. Write Safe writes are allowed. Unsafe writes fail. How Client resolves commit hash on first read and uses that for all subsequent operations. Note: this is stricter than the formal definition of repeatable read since that will allow new records to also be viewed on a second operation within the same transaction. However, both implementations are of similar complexity and a stricter form of repeated read seems easier to understand. Serializable (Optimistic) \u00b6 Read When a transaction is started, a ref is turned into a specific commit id. During the transaction, a recording of all read tables is recorded. Ownership A transaction only needs to be created on the client. There is no concept of a long-lived transaction on the server. Write All tables touched as part of the read operations must be in the same state when the commit operation is attempted. If they are not, the write operation is rejected. This is done internally via the Unchanged operation. How Client resolves commit hash on first read and uses that for all subsequent operations. Serializable transactions allow one to do guaranteed exactly once operations. For example- move all this data from table1 to table2. At the end of this operation there is a guarantee that any operations done against table1 will either show up in table2 or fail to apply to table1 (true before & after). Pessimistic Locking \u00b6 Currently, all three isolation models are supported only via optimistic locking. In the future, it is likely that we will also add support for pessimistic locking. To support pessimistic locking, transaction state must be held by the Nessie service as opposed to Nessie clients requiring a more formal protocol around start transaction, commit transaction with relevant timeouts, deadlock detection and cancellation capabilities. Lock Coarseness and Resolution \u00b6 Nessie maintains state and locks at table granularity. If a conflict is found at the table level, Nessie will either reject the operation or delegate the operation to the underlying table format to see if further conflict resolution can occur. Delta Lake does support serializable isolation against a single table . It does not support serializable across multiple tables. \u21a9","title":"Transactions"},{"location":"features/transactions/#transactions","text":"Nessie extends existing table formats to provide a single serial view of transaction history. This is enabled across an unlimited number of tables. A user can view a commit log either through the UI or by using the Nessie CLI. Operations against each table are listed along with timestamp, user and helpful information about the operation.","title":"Transactions"},{"location":"features/transactions/#cross-table-transactions","text":"Nessie is the first technology to provide an open facility for cross-table transactions within your data lake. There are two ways that this can be accomplished: Via Branches: Because Nessie allows branches to be created and then reassigned, a sequence of commits on one branch can be exposed as a single consistent view to other users through the use of a merge operation. This allows use of systems that don\u2019t have internal cross-table transaction support to still control the visibility of connected changes. Single Commits: Nessie allows a single commit operation to include many object changes simultaneously. While Nessie operations internally use this capability, tools will need to be enhanced to take advantage of this powerful new capability.","title":"Cross-Table Transactions"},{"location":"features/transactions/#start-transaction-commit","text":"The Nessie community is working with tool developers to introduce traditional data warehouse cross-table transactions. Nessie\u2019s catalog implementations already support the underlying capability of multi-table transactions.","title":"START TRANSACTION.. COMMIT"},{"location":"features/transactions/#isolation-levels","text":"Nessie exposes APIs to support three-levels of isolation: Read Committed, Repeated Read and Serialized. By supporting the recording of reads as part of a commit (via the Unchanged operation), tools can introduce full per operation serialized isolation. This is a transaction mode that has been traditionally limited to OLTP systems and unavailable to OLAP systems and data warehouses 1 . Info While Nessie exposes the necessary primitives to support configurable isolation, work still needs to be done with tool developers to ensure those tools expose these capabilities. As those integrations progress, we\u2019ll include more information about them here. At the moment, most tools operate in either Read Committed (Iceberg when calling refresh, Delta Lake) or Repeated Read (HMS Bridge operations, Iceberg when avoiding calls to refresh).","title":"Isolation Levels"},{"location":"features/transactions/#read-committed-optimistic","text":"Read Each time metadata for a table is retrieved, the latest version of that ref for the that current branch is exposed. Ownership A transaction only needs to be created on the client. There is not concept of a long-lived transaction. Write Safe writes are allowed. How Client goes to server to retrieve latest version of data for each operation","title":"Read Committed (Optimistic)"},{"location":"features/transactions/#repeated-read-optimistic","text":"Read When a transaction is started, a ref is turned into a specific commit id. All metadata retrieved is locked to this hash or later, as long as future hashes have not changed any table already read. Ownership A transaction only needs to be created on the client. There is no concept of a long-lived transaction on the server. Write Safe writes are allowed. Unsafe writes fail. How Client resolves commit hash on first read and uses that for all subsequent operations. Note: this is stricter than the formal definition of repeatable read since that will allow new records to also be viewed on a second operation within the same transaction. However, both implementations are of similar complexity and a stricter form of repeated read seems easier to understand.","title":"Repeated Read (Optimistic)"},{"location":"features/transactions/#serializable-optimistic","text":"Read When a transaction is started, a ref is turned into a specific commit id. During the transaction, a recording of all read tables is recorded. Ownership A transaction only needs to be created on the client. There is no concept of a long-lived transaction on the server. Write All tables touched as part of the read operations must be in the same state when the commit operation is attempted. If they are not, the write operation is rejected. This is done internally via the Unchanged operation. How Client resolves commit hash on first read and uses that for all subsequent operations. Serializable transactions allow one to do guaranteed exactly once operations. For example- move all this data from table1 to table2. At the end of this operation there is a guarantee that any operations done against table1 will either show up in table2 or fail to apply to table1 (true before & after).","title":"Serializable (Optimistic)"},{"location":"features/transactions/#pessimistic-locking","text":"Currently, all three isolation models are supported only via optimistic locking. In the future, it is likely that we will also add support for pessimistic locking. To support pessimistic locking, transaction state must be held by the Nessie service as opposed to Nessie clients requiring a more formal protocol around start transaction, commit transaction with relevant timeouts, deadlock detection and cancellation capabilities.","title":"Pessimistic Locking"},{"location":"features/transactions/#lock-coarseness-and-resolution","text":"Nessie maintains state and locks at table granularity. If a conflict is found at the table level, Nessie will either reject the operation or delegate the operation to the underlying table format to see if further conflict resolution can occur. Delta Lake does support serializable isolation against a single table . It does not support serializable across multiple tables. \u21a9","title":"Lock Coarseness and Resolution"},{"location":"tables/","text":"Overview \u00b6 Nessie is designed to work with table formats that support a write-once, immutable asset and metadata model . These types of formats rely on a transaction arbitrator to decide the order of operations within a table. Nessie developers have named this operation a \u201croot pointer store\u201d (or RPS). This is because these formats all have the same need of determining what is the \u201clatest\u201d version of data. This decision needs to be maintained via a check-and-set operation about what the current state of a table is. Root Pointer Store \u00b6 Each table format provides at least one RPS facility. Existing RPS models include: RPS by convention: E.g. \u201conly one writer is allowed\u201d RPS by consistent fileSystem: E.g. one file can be created with a certain name RPS by external locking: E.g. calling Hive Metastore lock apis Nessie formalizes and extends the concept of an RPS. It adds two main types of operations: coordination of multiple per-table root pointers and historical versioning across changes. This allows users to combine the rich capabilities of existing table formats with the Nessie capabilities around versioning and transactions. Table Formats \u00b6 Nessie currently works with the following formats. Iceberg Tables Delta Lake Tables Hive Tables We expect that Nessie will continue to add table formats as more are created. SQL Views \u00b6 In addition to table formats, Nessie also supports storing SQL views within the Nessie repository. This allows tools working in tandem with Nessie to provide very powerful versioned, semantic-layering system. See more in our documentation on SQL Views . Other Object Types \u00b6 There has been discussion about adding additional types of objects to Nessie for the purpose of creating a consistent repository between input assets (jobs, models, etc) and output assets. This is something that will evaluated based on demand. There are currently three options being considered: more structured object types (such as spark job), blob types and support for git sub-modules (where Nessie offers a new object type that refers to a particular commit within a git repository). If you have more thoughts on this, please provide feedback on the mailing list .","title":"Overview"},{"location":"tables/#overview","text":"Nessie is designed to work with table formats that support a write-once, immutable asset and metadata model . These types of formats rely on a transaction arbitrator to decide the order of operations within a table. Nessie developers have named this operation a \u201croot pointer store\u201d (or RPS). This is because these formats all have the same need of determining what is the \u201clatest\u201d version of data. This decision needs to be maintained via a check-and-set operation about what the current state of a table is.","title":"Overview"},{"location":"tables/#root-pointer-store","text":"Each table format provides at least one RPS facility. Existing RPS models include: RPS by convention: E.g. \u201conly one writer is allowed\u201d RPS by consistent fileSystem: E.g. one file can be created with a certain name RPS by external locking: E.g. calling Hive Metastore lock apis Nessie formalizes and extends the concept of an RPS. It adds two main types of operations: coordination of multiple per-table root pointers and historical versioning across changes. This allows users to combine the rich capabilities of existing table formats with the Nessie capabilities around versioning and transactions.","title":"Root Pointer Store"},{"location":"tables/#table-formats","text":"Nessie currently works with the following formats. Iceberg Tables Delta Lake Tables Hive Tables We expect that Nessie will continue to add table formats as more are created.","title":"Table Formats"},{"location":"tables/#sql-views","text":"In addition to table formats, Nessie also supports storing SQL views within the Nessie repository. This allows tools working in tandem with Nessie to provide very powerful versioned, semantic-layering system. See more in our documentation on SQL Views .","title":"SQL Views"},{"location":"tables/#other-object-types","text":"There has been discussion about adding additional types of objects to Nessie for the purpose of creating a consistent repository between input assets (jobs, models, etc) and output assets. This is something that will evaluated based on demand. There are currently three options being considered: more structured object types (such as spark job), blob types and support for git sub-modules (where Nessie offers a new object type that refers to a particular commit within a git repository). If you have more thoughts on this, please provide feedback on the mailing list .","title":"Other Object Types"},{"location":"tables/deltalake/","text":"Delta Lake \u00b6 Delta Lake is a table format open-sourced under the Apache License. It provides several benefits including: Single table ACID transactions Scalable metadata Appends, deletes, updates and merges via file re-statements Delta Lake is relatively Spark-centric. It does expose tables via manifests for tools that are not Delta Lake enabled and there are libraries for other tools 1 but the core library relies heavily on Spark. When using Nessie as the backing store for Delta Lake there are no longer restrictions on which types of filesystems/blob stores can be written to. When using Nessie you can write to Delta Lake regardless of the filesystem or number of writers. Client Integration Points \u00b6 Nessie provides a custom LogStore implementation for Delta Lake. Additionally, Nessie currently requires a small change to core Delta Lake code to enable use of Nessie. Without Nessie, Delta Lake normally maintains a single consistent version history through the use of a custom naming scheme within a known directory. While this works for one version history, with multiple additional work is required. As such, Nessie introduces a new abstraction that allows multiple file naming schemes thus enabling multiple version of the same dataset (with separate histories) to exist simultaneously. This is done by adding an extension point to the LogFileHandler interface. Server Integration Points \u00b6 There is a plan for Nessie to run table management tasks related to Delta Lake for manifest generation. This would expose manifests for selected branches and or tags that are maintained as references in HMS. This would target situations where the consumption tool doesn\u2019t have Delta Lake and Nessie libraries. For example, this would enable AWS Athena to consume Nessie-versioned Delta Lake tables via AWS Glue. These libraries look to be unmaintained and leverage old versions of Delta Lake. \u21a9","title":"Delta Lake"},{"location":"tables/deltalake/#delta-lake","text":"Delta Lake is a table format open-sourced under the Apache License. It provides several benefits including: Single table ACID transactions Scalable metadata Appends, deletes, updates and merges via file re-statements Delta Lake is relatively Spark-centric. It does expose tables via manifests for tools that are not Delta Lake enabled and there are libraries for other tools 1 but the core library relies heavily on Spark. When using Nessie as the backing store for Delta Lake there are no longer restrictions on which types of filesystems/blob stores can be written to. When using Nessie you can write to Delta Lake regardless of the filesystem or number of writers.","title":"Delta Lake"},{"location":"tables/deltalake/#client-integration-points","text":"Nessie provides a custom LogStore implementation for Delta Lake. Additionally, Nessie currently requires a small change to core Delta Lake code to enable use of Nessie. Without Nessie, Delta Lake normally maintains a single consistent version history through the use of a custom naming scheme within a known directory. While this works for one version history, with multiple additional work is required. As such, Nessie introduces a new abstraction that allows multiple file naming schemes thus enabling multiple version of the same dataset (with separate histories) to exist simultaneously. This is done by adding an extension point to the LogFileHandler interface.","title":"Client Integration Points"},{"location":"tables/deltalake/#server-integration-points","text":"There is a plan for Nessie to run table management tasks related to Delta Lake for manifest generation. This would expose manifests for selected branches and or tags that are maintained as references in HMS. This would target situations where the consumption tool doesn\u2019t have Delta Lake and Nessie libraries. For example, this would enable AWS Athena to consume Nessie-versioned Delta Lake tables via AWS Glue. These libraries look to be unmaintained and leverage old versions of Delta Lake. \u21a9","title":"Server Integration Points"},{"location":"tables/hive/","text":"Hive Tables \u00b6 Hive has now existed for the more than a decade. Hive is composed of two main components: the Hive Engine and the Hive Metastore service (HMS). The Hive metastore has become the defacto metadata storage standard. This includes managed service offerings from all three major cloud vendors and support in most tools including: Spark, Dremio, Presto, Athena, Apache Flink, etc. In many ways Hive tables are much less sophisticated than the offerings provided by Delta Lake and Iceberg. One key difference is in the level of resolution of metadata HMS maintains. While a key component of both Delta Lake and Iceberg is a complete listing of all files in the dataset, Hive only maintains lists of partitions and relies on underlying filesystem listing for things like file set consistency. Despite this, the prevalence of HMS as a metadata format and storage system means that providing a Nessie capability is still valuable. Nessie HMS Bridge \u00b6 Nessie provides the HMS bridge that exposes the Nessie catalog as a Hive Metastore service through the use of Hive extension APIs. Nessie\u2019s HMS bridge is run separately from the core Nessie service and leverages Nessie\u2019s APIs to operate against metadata. Hive Types Supported \u00b6 The Nessie HMS Bridge provides support for Hive views and external tables that are mutated by partition 1 Limitations \u00b6 Similar to AWS Glue, Nessie\u2019s HMS capabilities are focused on the highest value use cases. As such, certain features are not supported. See the HMS bridge documentation for what is and is not supported. In Hive, tables can be marked as immutable to disallow the addition of new data files to existing partitions. The tables can still be mutated through the use of OVERWRITE or ADD and DROP partition. This flag must be enabled for Nessie to accept the table for storage. This is to ensure that Nessie always exposes a consistent view of data. \u21a9","title":"Hive Tables"},{"location":"tables/hive/#hive-tables","text":"Hive has now existed for the more than a decade. Hive is composed of two main components: the Hive Engine and the Hive Metastore service (HMS). The Hive metastore has become the defacto metadata storage standard. This includes managed service offerings from all three major cloud vendors and support in most tools including: Spark, Dremio, Presto, Athena, Apache Flink, etc. In many ways Hive tables are much less sophisticated than the offerings provided by Delta Lake and Iceberg. One key difference is in the level of resolution of metadata HMS maintains. While a key component of both Delta Lake and Iceberg is a complete listing of all files in the dataset, Hive only maintains lists of partitions and relies on underlying filesystem listing for things like file set consistency. Despite this, the prevalence of HMS as a metadata format and storage system means that providing a Nessie capability is still valuable.","title":"Hive Tables"},{"location":"tables/hive/#nessie-hms-bridge","text":"Nessie provides the HMS bridge that exposes the Nessie catalog as a Hive Metastore service through the use of Hive extension APIs. Nessie\u2019s HMS bridge is run separately from the core Nessie service and leverages Nessie\u2019s APIs to operate against metadata.","title":"Nessie HMS Bridge"},{"location":"tables/hive/#hive-types-supported","text":"The Nessie HMS Bridge provides support for Hive views and external tables that are mutated by partition 1","title":"Hive Types Supported"},{"location":"tables/hive/#limitations","text":"Similar to AWS Glue, Nessie\u2019s HMS capabilities are focused on the highest value use cases. As such, certain features are not supported. See the HMS bridge documentation for what is and is not supported. In Hive, tables can be marked as immutable to disallow the addition of new data files to existing partitions. The tables can still be mutated through the use of OVERWRITE or ADD and DROP partition. This flag must be enabled for Nessie to accept the table for storage. This is to ensure that Nessie always exposes a consistent view of data. \u21a9","title":"Limitations"},{"location":"tables/iceberg/","text":"Apache Iceberg \u00b6 Apache Iceberg is a Apache Software Foundation project that provides a rich, relatively new table format. It provides: Single table ACID transactions Scalable metadata Appends via file addition Updates, deletes and merges via single record operations Iceberg Extension Points \u00b6 Iceberg exposes two primary classes for working with datasets. These are Catalog and TableOperations. Nessie implements each. These classes are available in the the Iceberg source code and are available directly in Iceberg releases (eg spark-runtime , spark3-runtime , flink-runtime ). Iceberg Snapshots \u00b6 Iceberg supports the concept of snapshots. Snapshots are point in time versions of a table and are managed as part of each commit operation. Snapshots are limited to single table versioning. Nessie versions and commits provide a broader set of snapshot capabilities as they support multiple tables. Nessie is happy to coexist with Iceberg Snapshots. When working with Nessie, Iceberg snapshots will also be versioned along the rest of Iceberg metadata within the Nessie commit model. Automatic Snapshot Import \u00b6 We are exploring the creation of a tool where a user can import table snapshots across multiple Iceberg tables into a single Nessie repository to capture historical data snapshots (interleaved across time). Hive Compatibility \u00b6 SerDe Compatibility \u00b6 There is currently work in progress that provides updates to the existing HiveCatalog so that it can recognize a Nessie Catalog pointer and reroute the metadata lookup to a Nessie server. This allows existing workflows to continue to work while also moving versioning responsibilities to Nessie. Hive Table Cloning \u00b6 In Nessie, we plan to add a capability to automatically update one or more Hive Metastore servers (including AWS Glue) every time a Iceberg table is updated in Nessie so that legacy systems can still be exposed to Nessie updates, even if the HMS Bridge service we offer is not used. HMS Bridge Compatibility \u00b6 There is a plan to expose Nessie native tables automatically to Hive users when they are interacting through the HMS bridge.","title":"Apache Iceberg"},{"location":"tables/iceberg/#apache-iceberg","text":"Apache Iceberg is a Apache Software Foundation project that provides a rich, relatively new table format. It provides: Single table ACID transactions Scalable metadata Appends via file addition Updates, deletes and merges via single record operations","title":"Apache Iceberg"},{"location":"tables/iceberg/#iceberg-extension-points","text":"Iceberg exposes two primary classes for working with datasets. These are Catalog and TableOperations. Nessie implements each. These classes are available in the the Iceberg source code and are available directly in Iceberg releases (eg spark-runtime , spark3-runtime , flink-runtime ).","title":"Iceberg Extension Points"},{"location":"tables/iceberg/#iceberg-snapshots","text":"Iceberg supports the concept of snapshots. Snapshots are point in time versions of a table and are managed as part of each commit operation. Snapshots are limited to single table versioning. Nessie versions and commits provide a broader set of snapshot capabilities as they support multiple tables. Nessie is happy to coexist with Iceberg Snapshots. When working with Nessie, Iceberg snapshots will also be versioned along the rest of Iceberg metadata within the Nessie commit model.","title":"Iceberg Snapshots"},{"location":"tables/iceberg/#automatic-snapshot-import","text":"We are exploring the creation of a tool where a user can import table snapshots across multiple Iceberg tables into a single Nessie repository to capture historical data snapshots (interleaved across time).","title":"Automatic Snapshot Import"},{"location":"tables/iceberg/#hive-compatibility","text":"","title":"Hive Compatibility"},{"location":"tables/iceberg/#serde-compatibility","text":"There is currently work in progress that provides updates to the existing HiveCatalog so that it can recognize a Nessie Catalog pointer and reroute the metadata lookup to a Nessie server. This allows existing workflows to continue to work while also moving versioning responsibilities to Nessie.","title":"SerDe Compatibility"},{"location":"tables/iceberg/#hive-table-cloning","text":"In Nessie, we plan to add a capability to automatically update one or more Hive Metastore servers (including AWS Glue) every time a Iceberg table is updated in Nessie so that legacy systems can still be exposed to Nessie updates, even if the HMS Bridge service we offer is not used.","title":"Hive Table Cloning"},{"location":"tables/iceberg/#hms-bridge-compatibility","text":"There is a plan to expose Nessie native tables automatically to Hive users when they are interacting through the HMS bridge.","title":"HMS Bridge Compatibility"},{"location":"tables/views/","text":"SQL Views \u00b6 Nessie supports versioning SQL views. A view is composed of the following properties: SQL Text Schema (representation to be finalized, likely Avro or Arrow) Dialect (currently includes Hive, Spark, Dremio, Presto) This enables SQL views to be versioned along with underlying datasets to provide a complete place for logical and physical experimentation. Because SQL dialects differ by system, Nessie does not parse or understand SQL. It relies on the creator of SQL statements to validate the provided SQL before being stored in Nessie. Info While Nessie can already store SQL Views, further work needs to be done in existing systems to fully expose this functionality.","title":"SQL Views"},{"location":"tables/views/#sql-views","text":"Nessie supports versioning SQL views. A view is composed of the following properties: SQL Text Schema (representation to be finalized, likely Avro or Arrow) Dialect (currently includes Hive, Spark, Dremio, Presto) This enables SQL views to be versioned along with underlying datasets to provide a complete place for logical and physical experimentation. Because SQL dialects differ by system, Nessie does not parse or understand SQL. It relies on the creator of SQL statements to validate the provided SQL before being stored in Nessie. Info While Nessie can already store SQL Views, further work needs to be done in existing systems to fully expose this functionality.","title":"SQL Views"},{"location":"tools/","text":"Overview \u00b6 Nessie is focused on working with the widest range of tools possible. If a tool creates or reads data, Nessie seeks to work with it. Current Nessie integrations/tools include the following: Spark 2 1 Spark 3 2 Hive 3 AWS Athena 4 Nessie CLI Read Default Branch Read Any Branch/Tag/Hash Write Default Branch Write Any Branch/Tag/Hash Create Branch Create Tag Iceberg Tables Delta Lake Tables Hive Tables (via HMS Bridge) Spark 2 currently only supports access via the Dataframe API due to weak generic catalog support. \u21a9 Spark 3 supports both SQL and dataframe access. Consumption can be done via existing Iceberg and Delta Lake catalogs with Nessie extensions or through the Nessie Catalog, which currently exposes both of these formats. \u21a9 Hive support is provided via the HMS Bridge Service \u21a9 Athena access is made possible via the AWS Athena Lambda for Hive Metastore and the HMS Bridge Service. \u21a9","title":"Overview"},{"location":"tools/#overview","text":"Nessie is focused on working with the widest range of tools possible. If a tool creates or reads data, Nessie seeks to work with it. Current Nessie integrations/tools include the following: Spark 2 1 Spark 3 2 Hive 3 AWS Athena 4 Nessie CLI Read Default Branch Read Any Branch/Tag/Hash Write Default Branch Write Any Branch/Tag/Hash Create Branch Create Tag Iceberg Tables Delta Lake Tables Hive Tables (via HMS Bridge) Spark 2 currently only supports access via the Dataframe API due to weak generic catalog support. \u21a9 Spark 3 supports both SQL and dataframe access. Consumption can be done via existing Iceberg and Delta Lake catalogs with Nessie extensions or through the Nessie Catalog, which currently exposes both of these formats. \u21a9 Hive support is provided via the HMS Bridge Service \u21a9 Athena access is made possible via the AWS Athena Lambda for Hive Metastore and the HMS Bridge Service. \u21a9","title":"Overview"},{"location":"tools/cli/","text":"Nessie CLI \u00b6 The Nessie CLI is an easy way to get started with Nessie. It supports multiple branch and tag management capabilities. This is installed as pynessie by pip . Installation \u00b6 # python 3 required pip install pynessie Usage \u00b6 All of the REST API calls are exposed via the command line interface. To see a list of what is available run: $ nessie --help Usage: nessie [ OPTIONS ] COMMAND [ ARGS ] ... Nessie cli tool. Interact with Nessie branches and tables via the command line Options: --json write output in json format. -v, --verbose Verbose output. --endpoint TEXT Optional endpoint, if different from config file. --version --help Show this message and exit. Commands: branch Branch operations. cherry-pick Transplant HASHES onto current branch. config Set and view config. contents Contents operations. log Show commit log. merge Merge BRANCH into current branch. remote Set and view remote endpoint. tag Tag operations. $ nessie branch --help Usage: nessie branch [ OPTIONS ] [ BRANCH ] [ NEW_BRANCH ] Branch operations. BRANCH name of branch to list or create/assign NEW_BRANCH name of branch to assign from or rename to Examples: nessie branch -l -> list all branches nessie branch -l main -> list only main nessie branch -d main -> delete main nessie branch -> list all branches nessie branch main -> create branch main at current head nessie branch main test -> create branch main at head of test nessie branch -f main test -> assign main to head of test Options: -l, --list list branches -d, --delete delete a branch -f, --force force branch assignment -c, --condition TEXT Conditional Hash. Only perform the action if branch currently points to condition. --help Show this message and exit. $ nessie cherry-pick --help Usage: nessie cherry-pick [ OPTIONS ] [ HASHES ] ... Transplant HASHES onto current branch. Options: -b, --branch TEXT branch to cherry-pick onto. If not supplied the default branch from config is used -f, --force force branch assignment -c, --condition TEXT Conditional Hash. Only perform the action if branch currently points to condition. --help Show this message and exit. $ nessie config --help Usage: nessie cherry-pick [ OPTIONS ] [ HASHES ] ... Transplant HASHES onto current branch. Options: -b, --branch TEXT branch to cherry-pick onto. If not supplied the default branch from config is used -f, --force force branch assignment -c, --condition TEXT Conditional Hash. Only perform the action if branch currently points to condition. --help Show this message and exit. $ nessie contents --help Usage: nessie contents [ OPTIONS ] [ KEY ] ... Contents operations. KEY name of object to view, delete. If listing the key will limit by namespace what is included. Options: -l, --list list tables -d, --delete delete a table -s, --set modify a table -c, --condition TEXT Conditional Hash. Only perform the action if branch currently points to condition. -r, --ref TEXT branch to list from. If not supplied the default branch from config is used -m, --message TEXT commit message --help Show this message and exit. !!! caution Please note that we ' re actively redefining the cli to better match Git syntax. You should expect that this syntax will change shortly. $ nessie log --help Usage: nessie log [ OPTIONS ] [ REVISION_RANGE ] [ PATHS ] ... Show commit log. REVISION_RANGE optional branch, tag or hash to start viewing log from. If of the form <hash>..<hash> only show log for given range PATHS optional list of paths. If given, only show commits which affected the given paths Options: -n, --number INTEGER number of log entries to return --since, --after TEXT Commits more recent than specific date --until, --before TEXT Commits older than specific date --author, --committer limit commits to specific committer --help Show this message and exit. $ nessie merge --help Usage: nessie merge [ OPTIONS ] [ MERGE_BRANCH ] Merge BRANCH into current branch. BRANCH can be a hash or branch. Options: -b, --branch TEXT branch to cherry-pick onto. If not supplied the default branch from config is used -f, --force force branch assignment -c, --condition TEXT Conditional Hash. Only perform the action if branch currently points to condition. --help Show this message and exit. $ nessie remote --help Usage: nessie remote [ OPTIONS ] COMMAND [ ARGS ] ... Set and view remote endpoint. Options: --help Show this message and exit. Commands: add Set current remote. set-head Set current default branch. show Show current remote. $ nessie tag --help Usage: nessie tag [ OPTIONS ] [ TAG_NAME ] [ NEW_TAG ] Tag operations. TAG_NAME name of branch to list or create/assign NEW_TAG name of branch to assign from or rename to Examples: nessie tag -l -> list all tags nessie tag -l main -> list only main nessie tag -d main -> delete main nessie tag -> list all tags nessie tag main -> create tag xxx at current head nessie tag main test -> create tag xxx at head of test nessie tag -f main test -> assign xxx to head of test Options: -l, --list list branches -d, --delete delete a branches -f, --force force branch assignment -c, --condition TEXT Conditional Hash. Only perform the action if branch currently points to condition. --help Show this message and exit. Configuration \u00b6 You can configure the Nessie CLI by creating a configuration file as described below: macOS: ~/.config/nessie and ~/Library/Application Support/nessie Other Unix: ~/.config/nessie and /etc/nessie Windows: %APPDATA%\\nessie where the APPDATA environment variable falls back to %HOME%\\AppData\\Roaming if undefined Via the environment variable DREMIO_CLIENTDIR The default config file is as follows: auth : # Type can be either basic or aws type : basic # Username and password required if using basic auth username : <username> password : <password> timeout : 10 # Nessie endpoint endpoint : http://localhost/api/v1 # whether to skip SSL cert verification verify : true When configuring authentication type aws , the client delegates to the Boto library. You can configure credentials using any of the standard Boto AWS methods . The command line interface can be configured with most of the above parameters via flags or by setting a config directory. The relevant configs can also be set via environment variables. These take precedence. The environment variable format is to append NESSIE_ to a config parameter and nested configs are separated by a _ . For example: NESSIE_AUTH_TIMEOUT maps to auth.timeout in the default configuration file above. Working with JSON \u00b6 The Nessie CLI can return data in json format and can be used effectively with jq . For example: $ nessie --json branch -l | jq . The Nessie CLI is built on the great Python Click library. It requires Python 3.x.","title":"Nessie CLI"},{"location":"tools/cli/#nessie-cli","text":"The Nessie CLI is an easy way to get started with Nessie. It supports multiple branch and tag management capabilities. This is installed as pynessie by pip .","title":"Nessie CLI"},{"location":"tools/cli/#installation","text":"# python 3 required pip install pynessie","title":"Installation"},{"location":"tools/cli/#usage","text":"All of the REST API calls are exposed via the command line interface. To see a list of what is available run: $ nessie --help Usage: nessie [ OPTIONS ] COMMAND [ ARGS ] ... Nessie cli tool. Interact with Nessie branches and tables via the command line Options: --json write output in json format. -v, --verbose Verbose output. --endpoint TEXT Optional endpoint, if different from config file. --version --help Show this message and exit. Commands: branch Branch operations. cherry-pick Transplant HASHES onto current branch. config Set and view config. contents Contents operations. log Show commit log. merge Merge BRANCH into current branch. remote Set and view remote endpoint. tag Tag operations. $ nessie branch --help Usage: nessie branch [ OPTIONS ] [ BRANCH ] [ NEW_BRANCH ] Branch operations. BRANCH name of branch to list or create/assign NEW_BRANCH name of branch to assign from or rename to Examples: nessie branch -l -> list all branches nessie branch -l main -> list only main nessie branch -d main -> delete main nessie branch -> list all branches nessie branch main -> create branch main at current head nessie branch main test -> create branch main at head of test nessie branch -f main test -> assign main to head of test Options: -l, --list list branches -d, --delete delete a branch -f, --force force branch assignment -c, --condition TEXT Conditional Hash. Only perform the action if branch currently points to condition. --help Show this message and exit. $ nessie cherry-pick --help Usage: nessie cherry-pick [ OPTIONS ] [ HASHES ] ... Transplant HASHES onto current branch. Options: -b, --branch TEXT branch to cherry-pick onto. If not supplied the default branch from config is used -f, --force force branch assignment -c, --condition TEXT Conditional Hash. Only perform the action if branch currently points to condition. --help Show this message and exit. $ nessie config --help Usage: nessie cherry-pick [ OPTIONS ] [ HASHES ] ... Transplant HASHES onto current branch. Options: -b, --branch TEXT branch to cherry-pick onto. If not supplied the default branch from config is used -f, --force force branch assignment -c, --condition TEXT Conditional Hash. Only perform the action if branch currently points to condition. --help Show this message and exit. $ nessie contents --help Usage: nessie contents [ OPTIONS ] [ KEY ] ... Contents operations. KEY name of object to view, delete. If listing the key will limit by namespace what is included. Options: -l, --list list tables -d, --delete delete a table -s, --set modify a table -c, --condition TEXT Conditional Hash. Only perform the action if branch currently points to condition. -r, --ref TEXT branch to list from. If not supplied the default branch from config is used -m, --message TEXT commit message --help Show this message and exit. !!! caution Please note that we ' re actively redefining the cli to better match Git syntax. You should expect that this syntax will change shortly. $ nessie log --help Usage: nessie log [ OPTIONS ] [ REVISION_RANGE ] [ PATHS ] ... Show commit log. REVISION_RANGE optional branch, tag or hash to start viewing log from. If of the form <hash>..<hash> only show log for given range PATHS optional list of paths. If given, only show commits which affected the given paths Options: -n, --number INTEGER number of log entries to return --since, --after TEXT Commits more recent than specific date --until, --before TEXT Commits older than specific date --author, --committer limit commits to specific committer --help Show this message and exit. $ nessie merge --help Usage: nessie merge [ OPTIONS ] [ MERGE_BRANCH ] Merge BRANCH into current branch. BRANCH can be a hash or branch. Options: -b, --branch TEXT branch to cherry-pick onto. If not supplied the default branch from config is used -f, --force force branch assignment -c, --condition TEXT Conditional Hash. Only perform the action if branch currently points to condition. --help Show this message and exit. $ nessie remote --help Usage: nessie remote [ OPTIONS ] COMMAND [ ARGS ] ... Set and view remote endpoint. Options: --help Show this message and exit. Commands: add Set current remote. set-head Set current default branch. show Show current remote. $ nessie tag --help Usage: nessie tag [ OPTIONS ] [ TAG_NAME ] [ NEW_TAG ] Tag operations. TAG_NAME name of branch to list or create/assign NEW_TAG name of branch to assign from or rename to Examples: nessie tag -l -> list all tags nessie tag -l main -> list only main nessie tag -d main -> delete main nessie tag -> list all tags nessie tag main -> create tag xxx at current head nessie tag main test -> create tag xxx at head of test nessie tag -f main test -> assign xxx to head of test Options: -l, --list list branches -d, --delete delete a branches -f, --force force branch assignment -c, --condition TEXT Conditional Hash. Only perform the action if branch currently points to condition. --help Show this message and exit.","title":"Usage"},{"location":"tools/cli/#configuration","text":"You can configure the Nessie CLI by creating a configuration file as described below: macOS: ~/.config/nessie and ~/Library/Application Support/nessie Other Unix: ~/.config/nessie and /etc/nessie Windows: %APPDATA%\\nessie where the APPDATA environment variable falls back to %HOME%\\AppData\\Roaming if undefined Via the environment variable DREMIO_CLIENTDIR The default config file is as follows: auth : # Type can be either basic or aws type : basic # Username and password required if using basic auth username : <username> password : <password> timeout : 10 # Nessie endpoint endpoint : http://localhost/api/v1 # whether to skip SSL cert verification verify : true When configuring authentication type aws , the client delegates to the Boto library. You can configure credentials using any of the standard Boto AWS methods . The command line interface can be configured with most of the above parameters via flags or by setting a config directory. The relevant configs can also be set via environment variables. These take precedence. The environment variable format is to append NESSIE_ to a config parameter and nested configs are separated by a _ . For example: NESSIE_AUTH_TIMEOUT maps to auth.timeout in the default configuration file above.","title":"Configuration"},{"location":"tools/cli/#working-with-json","text":"The Nessie CLI can return data in json format and can be used effectively with jq . For example: $ nessie --json branch -l | jq . The Nessie CLI is built on the great Python Click library. It requires Python 3.x.","title":"Working with JSON"},{"location":"tools/hive/","text":"Hive \u00b6 Nessie provides offers an optional component called the HMS Bridge Service. It allows you to run a vanilla Hive Metastore server that is enhanced to also support Nessie capabilities. This can be used to expose Nessie functionality to Hive or any Hive metastore compatible tool (such as AWS Athena ). Info HMS here stands for Hive Metastore and is not related to the HMS Queen Elizabeth or her sisterships. How It Works \u00b6 The HMS Bridge implements a new RawStore for the Hive metastore. The RawStore is an interface that Hive exposes to provide alternative backing storage systems. Typically, people use the default option of ObjectStore interacting with traditional relational databases. The HMS Bridge packages the Nessie REST client and leverages the Nessie service to provide versioning capabilities for Hive databases, tables and partitions. HMS Bridge Modes \u00b6 The HMS Bridge can run in two ways: Pure Nessie In this mode, all tables are Nessie based and no backing relational store is required. This is appropriate for new deployments. Delegation Mode In this mode the HMS Bridge can enhance your existing Hive Metastore. In this mode, your Hive metastore works as it does without the HMS Bridge for all but a set of whitelisted databases. When you interact with those whitelisted databases, you\u2019ll have Nessie\u2019s versioning capabilities. This allows people who are already using Hive metastore extensively to continue to do so while still taking advantage of Nessie\u2019s capabilities for some tables and databases. Setup \u00b6 Currently, the HMS Bridge works with Hive 2.x and Hive 3.x. To get started, do the following steps: Install Hive (2.3.7, 3.1.2, or similar). The easiest way is to download a tarball from Apache . Download the Hive 2 or Hive 3 Nessie plugin jar and copy into the Hive lib/ folder: Add the following properties to hive-site.xml: <property> <name> hive.metastore.rawstore.impl </name> <description> The specific Nessie store for Hive, see list below. </description> <value> org.projectnessie.Hive2NessieRawStore </value> </property> <property> <name> nessie.url </name> <description> The HTTP endpoint for the Nessie service. </description> <value> http://localhost:19120/api/v1 </value> </property> <property> <name> nessie.dbs </name> <description> A whitelist of databases that should be managed by Nessie, only relevant when using Delegation mode. `$nessie` admin database is always whitelisted. </description> <value> nessiedb,mynessiedb,git4data </value> </property> Class Hive Version Mode org.projectnessie.Hive2NessieRawStore Hive 2 Nessie Only org.projectnessie.DelegatingHive2NessieRawStore Hive 2 Nessie + Legacy Tables org.projectnessie.Hive3NessieRawStore Hive 3 Nessie Only org.projectnessie.DelegatingHive3NessieRawStore Hive 3 Nessie + Legacy Tables Start the metastore with bin/hive --service metastore Start working with Hive Metastore using HMS Bridge. If you\u2019ve chosen delegation mode, your existing Hive Metastore tables will continue to exist. Note In Hive 3 there is something called Hive Metastore \u201cStandalone\u201d mode. This was designed to try to support Kafka schema registry scenarios. It does not come packaged with sufficient capabilities to work as a Hive Metastore for querying purposes. To use Hive in this context you need a complete Hive install (the same as using Hive Metastore without Nessie). Use \u00b6 What\u2019s Supported \u00b6 Because Nessie Tables provide a set of new capabilities beyond what Hive has traditionally done, only a subset of Hive features are available with these tables. (If you are running in Nessie + Legacy mode, this only applies to Nessie tables, legacy tables will continue to operate as they do today.) These features currently include: CREATE, ALTER and DROP of: databases external tables views Inserting data into an empty table ADD and DROP for partitions Altering table properties Note: When using external tables with HMS Bridge, the tables must be created with the \u201cimmutable=true\u201d property. Nessie reject tables that are not configured with this property. This property does not mean the tables are immutable. What it does is disable Hive from mutating a tables state without informing the Hive metastore. This guarantees that Hive versions are maintained correctly. Query Semantics \u00b6 By default, Hive will work with the default branch configured in Nessie. You can do normal operations against your Hive metastore and Nessie will behave just like a traditional metastore. However, under the covers, Nessie is versioning all changes to your Hive metastore. This allows to do more fancy things as well. Because Hive does not natively support Branching and Tags, Nessie presents a pseudo-database called \u2018$nessie\u2019 which can be used within a HiveQL context to: * list available branches & tags * change the default ref for your session * create tags and branches List available branches and tags \u00b6 You can list available named references within Nessie by listing the tables in the $nessie database. hive> SHOW TABLES IN `$nessie` OK main dev ... Create New Branch or Tag \u00b6 To create a branch or tag, you will use CREATE VIEW syntax along with a dummy SQL query. The name of the view will be the new named reference. You can optionally provide type and ref parameters to further define your new object. ||property||description|| | type |Defaults to branch . Whether to create a named reference of type tag or branch | | ref |Defaults to the head of the current default branch (typically main ). Can be set to either a hash, branch or tag. If set to a tag or branch, resolves the named reference to a hash for creation.| Example uses: # create a new branch called my_new_branch pointing to the HEAD of the current ref context. hive> CREATE VIEW `$nessie`.my_new_branch AS SELECT 1; OK # create a new tag called my_new_tag pointing to the current tip of the default branch. hive> CREATE VIEW `$nessie`.my_new_tag TBLPROPERTIES(\"type\"=\"tag\") AS SELECT 1; OK # create a new tag called my_new_specific_tag that points to the reference \"abcd...\" hive> CREATE VIEW `$nessie`.my_new_specific_tag TBLPROPERTIES(\"type\"=\"tag\", \"ref\"=\"abcd...\") AS SELECT 1; OK Note: The SELECT 1 in the queries above is arbitrary and only serves to allow Hive to understand the HiveQL operation and then delegate it to Nessie. Any valid SQL can be provided there and will be thrown away, not stored. If you are running in Hive 2, it doesn\u2019t support creating views of queries without a FROM clause. In those cases, you\u2019ll need to use some other dummy statement such as SELECT * from <known table> LIMIT 1 . Set Your Context \u00b6 To change the context of the environment your session is working within, you will alter the $nessie database. You can set the default context to any valid Nessie reference. For example: hive> ALTER DATABASE `$nessie` SET DBPROPERTIES (\"ref\"=\"my_special_branch\") OK Once you change context, all read operations will default to that context for the life of that session. All write operations will also be done within that context. Querying across multiple contexts \u00b6 You can use absolute references to query across context. You do this by appending @<refname> to the table you want to query. For example, if you want to see all records in a table from yesterday and now, you might write a query such as: hive> SELECT * from t1 UNION ALL `t1@yesterday` OK This can be used to directly access any available reference within Nessie hive> SELECT * from `t1@prod_data_branch` UNION ALL `t1@my_special_tag` UNION ALL `t1@2019-12-25` OK Note You can only use absolute references (t1@ ) for SELECT queries. For mutation operations (such as CREATE/ALTER/DROP), you must first assign the context that you want to do the operation within. IOW, absolute references are read-only. Not Yet Supported \u00b6 Because Nessie has a special way of representing the data, the following types of objects and operations are not currently supported: Grants, role, privileges Statistics Functions Tokens Notifications Constraints and referential integrity (primary/foreign keys) File Management \u00b6 When using Nessie with Hive, you must rely on Nessie\u2019s garbage collection system in order to clean up files that are no longer referenced. Because there may be historical versions of tables pointing to specific files, using an external process to delete those files when not instructed/managed by Nessie will result in historical queries returning partial datasets. Because of this, the following tables types are unlikely to ever be supported by Nessie\u2019s versioning management system: Internal tables (Hive doesn\u2019t import metastore about changes) External tables without the \u201cimmutable=true\u201d property. Additionally, when creating a new external table, Nessie will always prepend the table\u2019s location with a randomly generated Guid. This ensures the following set of operations still maintain history and work as expectd: hive> CREATE EXTERNAL TABLE t1 (a int, b int) PARTITIONED BY (c int) TBLPROPERTIES (\"immutable\"=\"true\"); OK hive> INSERT TABLE t1 PARTITION(c) SELECT 1 AS a, 1 AS b,1 AS c UNION ALL SELECT 2,2,2 UNION ALL SELECT 3,3,3; ... OK hive> CREATE VIEW `$nessie`.original AS SELECT 1; OK hive> DROP TABLE t1; OK hive> CREATE EXTERNAL TABLE t1 (a int, b int) PARTITIONED BY (c int) TBLPROPERTIES (\"immutable\"=\"true\"); OK hive> INSERT TABLE t1 PARTITION(c) SELECT 10 AS a, 10 AS b, 1 AS c; ... OK In this situation, Nessie\u2019s versioning is in full effect. If I query t1 , I will see the record with a=10. If I query t1@original , I will see the records with a=[1,2,3]. Without Nessie\u2019s automatic prefixing, default Hive behavior would actually lead to an exception when the second insertion is done since it would find a file already existing on the path of the default external table.","title":"Hive"},{"location":"tools/hive/#hive","text":"Nessie provides offers an optional component called the HMS Bridge Service. It allows you to run a vanilla Hive Metastore server that is enhanced to also support Nessie capabilities. This can be used to expose Nessie functionality to Hive or any Hive metastore compatible tool (such as AWS Athena ). Info HMS here stands for Hive Metastore and is not related to the HMS Queen Elizabeth or her sisterships.","title":"Hive"},{"location":"tools/hive/#how-it-works","text":"The HMS Bridge implements a new RawStore for the Hive metastore. The RawStore is an interface that Hive exposes to provide alternative backing storage systems. Typically, people use the default option of ObjectStore interacting with traditional relational databases. The HMS Bridge packages the Nessie REST client and leverages the Nessie service to provide versioning capabilities for Hive databases, tables and partitions.","title":"How It Works"},{"location":"tools/hive/#hms-bridge-modes","text":"The HMS Bridge can run in two ways: Pure Nessie In this mode, all tables are Nessie based and no backing relational store is required. This is appropriate for new deployments. Delegation Mode In this mode the HMS Bridge can enhance your existing Hive Metastore. In this mode, your Hive metastore works as it does without the HMS Bridge for all but a set of whitelisted databases. When you interact with those whitelisted databases, you\u2019ll have Nessie\u2019s versioning capabilities. This allows people who are already using Hive metastore extensively to continue to do so while still taking advantage of Nessie\u2019s capabilities for some tables and databases.","title":"HMS Bridge Modes"},{"location":"tools/hive/#setup","text":"Currently, the HMS Bridge works with Hive 2.x and Hive 3.x. To get started, do the following steps: Install Hive (2.3.7, 3.1.2, or similar). The easiest way is to download a tarball from Apache . Download the Hive 2 or Hive 3 Nessie plugin jar and copy into the Hive lib/ folder: Add the following properties to hive-site.xml: <property> <name> hive.metastore.rawstore.impl </name> <description> The specific Nessie store for Hive, see list below. </description> <value> org.projectnessie.Hive2NessieRawStore </value> </property> <property> <name> nessie.url </name> <description> The HTTP endpoint for the Nessie service. </description> <value> http://localhost:19120/api/v1 </value> </property> <property> <name> nessie.dbs </name> <description> A whitelist of databases that should be managed by Nessie, only relevant when using Delegation mode. `$nessie` admin database is always whitelisted. </description> <value> nessiedb,mynessiedb,git4data </value> </property> Class Hive Version Mode org.projectnessie.Hive2NessieRawStore Hive 2 Nessie Only org.projectnessie.DelegatingHive2NessieRawStore Hive 2 Nessie + Legacy Tables org.projectnessie.Hive3NessieRawStore Hive 3 Nessie Only org.projectnessie.DelegatingHive3NessieRawStore Hive 3 Nessie + Legacy Tables Start the metastore with bin/hive --service metastore Start working with Hive Metastore using HMS Bridge. If you\u2019ve chosen delegation mode, your existing Hive Metastore tables will continue to exist. Note In Hive 3 there is something called Hive Metastore \u201cStandalone\u201d mode. This was designed to try to support Kafka schema registry scenarios. It does not come packaged with sufficient capabilities to work as a Hive Metastore for querying purposes. To use Hive in this context you need a complete Hive install (the same as using Hive Metastore without Nessie).","title":"Setup"},{"location":"tools/hive/#use","text":"","title":"Use"},{"location":"tools/hive/#whats-supported","text":"Because Nessie Tables provide a set of new capabilities beyond what Hive has traditionally done, only a subset of Hive features are available with these tables. (If you are running in Nessie + Legacy mode, this only applies to Nessie tables, legacy tables will continue to operate as they do today.) These features currently include: CREATE, ALTER and DROP of: databases external tables views Inserting data into an empty table ADD and DROP for partitions Altering table properties Note: When using external tables with HMS Bridge, the tables must be created with the \u201cimmutable=true\u201d property. Nessie reject tables that are not configured with this property. This property does not mean the tables are immutable. What it does is disable Hive from mutating a tables state without informing the Hive metastore. This guarantees that Hive versions are maintained correctly.","title":"What's Supported"},{"location":"tools/hive/#query-semantics","text":"By default, Hive will work with the default branch configured in Nessie. You can do normal operations against your Hive metastore and Nessie will behave just like a traditional metastore. However, under the covers, Nessie is versioning all changes to your Hive metastore. This allows to do more fancy things as well. Because Hive does not natively support Branching and Tags, Nessie presents a pseudo-database called \u2018$nessie\u2019 which can be used within a HiveQL context to: * list available branches & tags * change the default ref for your session * create tags and branches","title":"Query Semantics"},{"location":"tools/hive/#list-available-branches-and-tags","text":"You can list available named references within Nessie by listing the tables in the $nessie database. hive> SHOW TABLES IN `$nessie` OK main dev ...","title":"List available branches and tags"},{"location":"tools/hive/#create-new-branch-or-tag","text":"To create a branch or tag, you will use CREATE VIEW syntax along with a dummy SQL query. The name of the view will be the new named reference. You can optionally provide type and ref parameters to further define your new object. ||property||description|| | type |Defaults to branch . Whether to create a named reference of type tag or branch | | ref |Defaults to the head of the current default branch (typically main ). Can be set to either a hash, branch or tag. If set to a tag or branch, resolves the named reference to a hash for creation.| Example uses: # create a new branch called my_new_branch pointing to the HEAD of the current ref context. hive> CREATE VIEW `$nessie`.my_new_branch AS SELECT 1; OK # create a new tag called my_new_tag pointing to the current tip of the default branch. hive> CREATE VIEW `$nessie`.my_new_tag TBLPROPERTIES(\"type\"=\"tag\") AS SELECT 1; OK # create a new tag called my_new_specific_tag that points to the reference \"abcd...\" hive> CREATE VIEW `$nessie`.my_new_specific_tag TBLPROPERTIES(\"type\"=\"tag\", \"ref\"=\"abcd...\") AS SELECT 1; OK Note: The SELECT 1 in the queries above is arbitrary and only serves to allow Hive to understand the HiveQL operation and then delegate it to Nessie. Any valid SQL can be provided there and will be thrown away, not stored. If you are running in Hive 2, it doesn\u2019t support creating views of queries without a FROM clause. In those cases, you\u2019ll need to use some other dummy statement such as SELECT * from <known table> LIMIT 1 .","title":"Create New Branch or Tag"},{"location":"tools/hive/#set-your-context","text":"To change the context of the environment your session is working within, you will alter the $nessie database. You can set the default context to any valid Nessie reference. For example: hive> ALTER DATABASE `$nessie` SET DBPROPERTIES (\"ref\"=\"my_special_branch\") OK Once you change context, all read operations will default to that context for the life of that session. All write operations will also be done within that context.","title":"Set Your Context"},{"location":"tools/hive/#querying-across-multiple-contexts","text":"You can use absolute references to query across context. You do this by appending @<refname> to the table you want to query. For example, if you want to see all records in a table from yesterday and now, you might write a query such as: hive> SELECT * from t1 UNION ALL `t1@yesterday` OK This can be used to directly access any available reference within Nessie hive> SELECT * from `t1@prod_data_branch` UNION ALL `t1@my_special_tag` UNION ALL `t1@2019-12-25` OK Note You can only use absolute references (t1@ ) for SELECT queries. For mutation operations (such as CREATE/ALTER/DROP), you must first assign the context that you want to do the operation within. IOW, absolute references are read-only.","title":"Querying across multiple contexts"},{"location":"tools/hive/#not-yet-supported","text":"Because Nessie has a special way of representing the data, the following types of objects and operations are not currently supported: Grants, role, privileges Statistics Functions Tokens Notifications Constraints and referential integrity (primary/foreign keys)","title":"Not Yet Supported"},{"location":"tools/hive/#file-management","text":"When using Nessie with Hive, you must rely on Nessie\u2019s garbage collection system in order to clean up files that are no longer referenced. Because there may be historical versions of tables pointing to specific files, using an external process to delete those files when not instructed/managed by Nessie will result in historical queries returning partial datasets. Because of this, the following tables types are unlikely to ever be supported by Nessie\u2019s versioning management system: Internal tables (Hive doesn\u2019t import metastore about changes) External tables without the \u201cimmutable=true\u201d property. Additionally, when creating a new external table, Nessie will always prepend the table\u2019s location with a randomly generated Guid. This ensures the following set of operations still maintain history and work as expectd: hive> CREATE EXTERNAL TABLE t1 (a int, b int) PARTITIONED BY (c int) TBLPROPERTIES (\"immutable\"=\"true\"); OK hive> INSERT TABLE t1 PARTITION(c) SELECT 1 AS a, 1 AS b,1 AS c UNION ALL SELECT 2,2,2 UNION ALL SELECT 3,3,3; ... OK hive> CREATE VIEW `$nessie`.original AS SELECT 1; OK hive> DROP TABLE t1; OK hive> CREATE EXTERNAL TABLE t1 (a int, b int) PARTITIONED BY (c int) TBLPROPERTIES (\"immutable\"=\"true\"); OK hive> INSERT TABLE t1 PARTITION(c) SELECT 10 AS a, 10 AS b, 1 AS c; ... OK In this situation, Nessie\u2019s versioning is in full effect. If I query t1 , I will see the record with a=10. If I query t1@original , I will see the records with a=[1,2,3]. Without Nessie\u2019s automatic prefixing, default Hive behavior would actually lead to an exception when the second insertion is done since it would find a file already existing on the path of the default external table.","title":"File Management"},{"location":"tools/spark/","text":"Spark \u00b6 Nessie can be used with Spark in several different ways. These include: Iceberg client \u00b6 Nessie works seamlessly with Iceberg in Spark2 and Spark3. Nessie is implemented as a custom iceberg catalog and therefore supports all features available to any Iceberg client. This includes Spark structured streaming, Presto and Flink. See the iceberg docs for more info. Note You can follow along interactively in a Jupyter notebook by following the instructions here . To access Nessie from a spark cluster make sure the spark.jars spark option is set to include the Spark 2 or Spark 3 Nessie plugin jar. This fat jar is distributed by the Apache Iceberg project and contains all Apache Iceberg libraries required for operation, including the built in Nessie Catalog. In pyspark this would look like SparkSession . builder . config ( 'spark.jars' , 'path/to/iceberg-spark3-0.11.1.jar' ) ... rest of spark config . getOrCreate () The docs for the Java api in Iceberg explain how to use a Catalog . The only change is that a Nessie catalog should be instantiated Java Catalog catalog = new NessieCatalog ( spark . sparkContext (). hadoopConfiguration ()) Python catalog = jvm . NessieCatalog ( sc . _jsc . hadoopConfiguration ()) Note Iceberg\u2019s python libraries are still under active development. Actions against catalogs in pyspark still have to go through the jvm objects. See the demo directory for details. The Nessie Catalog needs the following parameters set in the Spark/Hadoop config. url = full url to nessie warehouse = where to store nessie tables ref = the ref or context that nessie will operate on (if different from default branch) auth_type = authentication type (BASIC, NONE or AWS) These are set as follows in code (or through other methods as described here ) Java //for a local spark instance conf . set ( \"spark.sql.catalog.nessie.url\" , url ) . set ( \"spark.sql.catalog.nessie.ref\" , branch ) . set ( \"spark.sql.nessie.nessie.auth_type\" , authType ) . set ( \"spark.sql.nessie.nessie.catalog-impl\" , \"org.apache.iceberg.nessie.NessieCatalog\" ) . set ( \"spark.sql.nessie.nessie.warehouse\" , fullPathToWarehouse ) . set ( \"spark.sql.catalog.nessie\" , \"org.apache.iceberg.spark.SparkCatalog\" ); spark = SparkSession . builder () . master ( \"local[2]\" ) . config ( conf ) . getOrCreate (); Python # here we are assuming NONE authorisation spark = SparkSession . builder \\ . config ( \"spark.jars\" , \"/path/to/downloaded/iceberg-spark3-0.11.1.jar\" ) \\ . config ( \"spark.sql.catalog.nessie.url\" , \"http://localhost:19120/api/v1\" ) . config ( \"spark.sql.catalog.nessie.ref\" , \"main\" ) . config ( \"spark.sql.nessie.nessie.auth_type\" , \"NONE\" ) . config ( \"spark.sql.nessie.nessie.catalog-impl\" , \"org.apache.iceberg.nessie.NessieCatalog\" ) . config ( \"spark.sql.nessie.nessie.warehouse\" , fullPathToWarehouse ) . config ( \"spark.sql.catalog.nessie\" , \"org.apache.iceberg.spark.SparkCatalog\" ); . getOrCreate () Note above we specified the option spark.sql.catalog.nessie.ref . This value sets the default branch that the iceberg catalog will use. We have specified spark.sql.catalog.nessie to point to SparkCatalog . This is a Spark option to set the catalog nessie to be managed by Nessie\u2019s Catalog implementation. All configuration for the Nessie catalog exists below this spark.sql.catalog.nessie configuration namespace. The catalog name is not important, it is important that the required options are all given below the catalog name. Writing \u00b6 Spark support is constantly evolving and the differences in Spark3 vs Spark2.4 are considerable. See the iceberg docs for an up to date support table. Spark2 \u00b6 Spark2.4 supports reads, appends, overwrites in Iceberg. Nessie tables in iceberg can be written via the Nessie Iceberg Catalog instantiated above. Iceberg in Spark2.4 has no ability to create tables so before a table can be appended to or overwritten the table must be first created via an Iceberg Catalog. This is straightforward in Java but requires addressing jvm objects directly in Python (until the python library for iceberg is released). Java 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // first instantiate the catalog NessieCatalog catalog = new NessieCatalog (); catalog . setConf ( sc . hadoopConfiguration ()); catalog . initialize ( \"nessie\" , ImmutableMap . of ( \"ref\" , ref , \"url\" , url , \"warehouse\" , pathToWarehouse )); // Creating table by first creating a table name with namespace TableIdentifier region_name = TableIdentifier . parse ( \"testing.region\" ); // next create the schema Schema region_schema = Schema ( [ Types . NestedField . optional ( 1 , \"R_REGIONKEY\" , Types . LongType . get ()), Types . NestedField . optional ( 2 , \"R_NAME\" , Types . StringType . get ()), Types . NestedField . optional ( 3 , \"R_COMMENT\" , Types . StringType . get ()), ] ); // and the partition PartitionSpec region_spec = PartitionSpec . unpartitioned (); // finally create the table catalog . createTable ( region_name , region_schema , region_spec ); Python 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 sc = spark . sparkContext jvm = sc . _gateway . jvm # import jvm libraries for iceberg catalogs and schemas java_import ( jvm , \"org.projectnessie.iceberg.NessieCatalog\" ) java_import ( jvm , \"org.apache.iceberg.catalog.TableIdentifier\" ) java_import ( jvm , \"org.apache.iceberg.Schema\" ) java_import ( jvm , \"org.apache.iceberg.types.Types\" ) java_import ( jvm , \"org.apache.iceberg.PartitionSpec\" ) # first instantiate the catalog catalog = jvm . NessieCatalog () catalog . setConf ( sc . _jsc . hadoopConfiguration ()) catalog . initialize ( \"nessie\" , { \"ref\" : ref , \"url\" : url , \"warehouse\" : pathToWarehouse }) # Creating table by first creating a table name with namespace region_name = jvm . TableIdentifier . parse ( \"testing.region\" ) # next create the schema region_schema = jvm . Schema ([ jvm . Types . NestedField . optional ( 1 , \"R_REGIONKEY\" , jvm . Types . LongType . get ()), jvm . Types . NestedField . optional ( 2 , \"R_NAME\" , jvm . Types . StringType . get ()), jvm . Types . NestedField . optional ( 3 , \"R_COMMENT\" , jvm . Types . StringType . get ()), ]) # and the partition region_spec = jvm . PartitionSpec . unpartitioned () # finally create the table region_table = catalog . createTable ( region_name , region_schema , region_spec ) When looking at the Python code above, lines 1-11 are importing jvm objects into pyspark. Lines 12-25 create the table name, schema and partition spec. These actions will be familiar to seasoned iceberg users and are wholly iceberg operations. Line 29 is where our initial iceberg metadata is finally written to disk and a commit takes place on Nessie. Now that we have created an Iceberg table in nessie we can write to it. The iceberg DataSourceV2 allows for either overwrite or append mode in a standard spark.write . Java regionDf = spark . read (). load ( \"data/region.parquet\" ); regionDf . write (). format ( \"iceberg\" ). mode ( \"overwrite\" ). save ( \"nessie.testing.region\" ); Python region_df = spark . read . load ( \"data/region.parquet\" ) region_df . write . format ( \"iceberg\" ) . mode ( \"overwrite\" ) . save ( \"nessie.testing.region\" ) Here we simply read a file from the default filesystem and write it to an existing nessie iceberg table. This will trigger a commit on current context\u2019s branch. For the examples above we have performed commits on the branch specified when we set our spark configuration. Had we not specified the context in our spark configuration all operations would have defaulted to the default branch defined by the server. This is a strong pattern for a spark job which is for example writing data as part of a wider ETL job. It will only ever need one context or branch to write to. If however you are running an interactive session and would like to write to a specific branch you would have to create a new Spark Conf. Java spark_dev = spark . newSession (); spark_dev . conf . set ( \"spark.sql.catalog.nessie.ref\" , \"dev\" ); regionDf = spark_dev . read (). load ( \"data/region.parquet\" ); regionDf . write (). format ( \"iceberg\" ). mode ( \"overwrite\" ). save ( \"nessie.testing.region\" ); Python spark_dev = spark . newSession () spark_dev . conf . set ( \"spark.sql.catalog.nessie.ref\" , \"dev\" ) region_df = spark_dev . read . load ( \"data/region.parquet\" ) region_df . write . format ( \"iceberg\" ) . mode ( \"overwrite\" ) . save ( \"nessie.testing.region\" ) Note the extra option clause in the write command. This will ensure the commit happens on the dev branch rather than the default branch. Spark3 \u00b6 The write path for Spark3 is slightly different and easier to work with. These changes haven\u2019t made it to pyspark yet so writing dataframes looks much the same there, including having to create the table. Spark3 table creation/insertion is as follows: Java regionDf = spark . read (). load ( ' data / region . parquet ' ) //create regionDf . writeTo ( \"nessie.testing.region\" ). create () //append regionDf . writeTo ( \"nessie.testing.region\" ). append () //overwrite partition regionDf . writeTo ( \"nessie.testing.region\" ). overwritePartitions () Python # same code as the spark2 section above to create the testing.region table region_df = spark . read . load ( \"data/region.parquet\" ) region_df . write . format ( \"iceberg\" ) . mode ( \"overwrite\" ) . save ( \"nessie.testing.region\" ) SQL CREATE TABLE nessie . testing . city ( C_CITYKEY BIGINT , C_NAME STRING , N_NATIONKEY BIGINT , C_COMMENT STRING ) USING iceberg PARTITIONED BY ( N_NATIONKEY ) -- AS SELECT .. can be added to the sql statement to perform a CTAS INSERT INTO nessie . testing . city VALUES ( 1 , 'a' , 1 , 'comment' ) The full list of operations can be found here . Everything that Iceberg supports the Nessie Iceberg Catalog also supports. Reading \u00b6 Reading is more straightforward between spark 2 and spark 3. We will look at both versions together in this section. To read a Nessie table in iceberg simply: Java regionDf = spark . read (). format ( \"iceberg\" ). load ( \"nessie.testing.region\" ) // Spark2 regionDf = spark . table ( \"nessie.testing.region\" ) // Spark3 Python # same code as above to create the testing.region table region_df = spark . read . format ( \"iceberg\" ) . load ( \"nessie.testing.region\" ) SQL SELECT * FROM nessie . testing . city -- Spark3 only The examples above all use the default branch defined on initialisation. There are several ways to reference specific branches or hashes from within a read statement. We will take a look at a few now from pyspark3, the rules are the same across all environments though. The general pattern is <table>@<branch> . Table must be present and either branch and/or hash are optional. We will throw an error if branch or hash don\u2019t exist. Branch or hash references in the table name will override passed option s and the settings in the Spark/Hadoop configs. 1 2 3 4 spark . read () . format ( \"iceberg\" ) . load ( \"testing.region@dev\" ) # read from branch dev spark . read () . format ( \"iceberg\" ) . load ( \"testing.region@<hash>\" ) # read specifically from hash spark . sql ( \"SELECT * FROM nessie.testing.`region@dev`\" ) spark . sql ( \"SELECT * FROM nessie.testing.`region@<hash>`\" ) Notice in the SQL statements the table@branch must be escaped separately from namespace or catalog arguments. Future versions may add the ability to specify a timestamp to query the data at a specific point in time (time-travel). In the meantime the history can be viewed on the command line or via the python client and a specific hash based on commit time can be extracted for use in the spark catalog. It is recommended to use the time-travel features of Nessie over the Iceberg features as Nessie history is consistent across the entire database. Delta Lake \u00b6 Note You can follow along interactively in a Jupyter notebook by following the instructions here . Delta Lake support in Nessie requires some minor modifications to the core Delta libraries. This patch is still ongoing, in the meantime Nessie will not work on Databricks and must be used with the open source Delta. Nessie is able to interact with Delta Lake by implementing a custom version of Delta\u2019s LogStore interface . This ensures that all filesystem changes are recorded by Nessie as commits. The benefit of this approach is the core ACID primitives are handled by Nessie. The limitations around concurrency that Delta would normally have are removed, any number of readers and writers can simultaneously interact with a Nessie managed Delta Lake table. To access Nessie from a spark cluster make sure the spark.jars spark option is set to include the Spark 2 or Spark 3 jar. These jars contain all Nessie and Delta Lake libraries required for operation. Note the spark3 jar is for Delta versions >7.0 on Spark3 and the spark2 jar is for Delta versions 6.x on Spark2.4 In pyspark this would look like SparkSession . builder . config ( 'spark.jars' , 'path/to/nessie-deltalake-spark2-0.5.1.jar' ) ... rest of spark config . getOrCreate () The Nessie LogStore needs the following parameters set in the Spark/Hadoop config. nessie.url = full url to nessie nessie.username = username if using basic auth, omitted otherwise nessie.password = password if using basic auth, omitted otherwise nessie.auth.type = authentication type (BASIC, NONE or AWS) spark.delta.logFileHandler.class=org.projectnessie.deltalake.NessieLogFileMetaParser spark.delta.logStore.class=org.projectnessie.deltalake.NessieLogStore These are set as follows in code (or through other methods as described here ) === \u201cJava\u201d`` //for a local spark instance conf . set ( \"spark.hadoop.nessie.url\" , url ) . set ( \"spark.hadoop.nessie.ref\" , branch ) . set ( \"spark.hadoop.nessie.auth_type\" , authType ) . set ( \"spark.sql.catalog.spark_catalog\" , \"org.apache.spark.sql.delta.catalog.DeltaCatalog\" ) \\ . set ( \"spark.sql.extensions\" , \"io.delta.sql.DeltaSparkSessionExtension\" ) \\ . set ( \"spark.delta.logStore.class\" , \"org.projectnessie.deltalake.NessieLogStore\" ) . set ( \"spark.delta.logFileHandler.class\" , \"org.projectnessie.deltalake.NessieLogFileMetaParser\" ) spark = SparkSession . builder () . master ( \"local[2]\" ) . config ( conf ) . getOrCreate (); Python # here we are assuming NONE authorisation spark = SparkSession . builder \\ . config ( \"spark.jars\" , \"../../clients/deltalake-spark3/target/nessie-deltalake-spark3-0.5.1.jar\" ) \\ . config ( \"spark.hadoop.nessie.url\" , \"http://localhost:19120/api/v1\" ) \\ . config ( \"spark.hadoop.nessie.ref\" , \"main\" ) \\ . config ( \"spark.sql.catalog.spark_catalog\" , \"org.apache.spark.sql.delta.catalog.DeltaCatalog\" ) \\ . config ( \"spark.sql.extensions\" , \"io.delta.sql.DeltaSparkSessionExtension\" ) \\ . config ( \"spark.delta.logFileHandler.class\" , \"org.projectnessie.deltalake.NessieLogFileMetaParser\" ) \\ . config ( \"spark.delta.logStore.class\" , \"org.projectnessie.deltalake.NessieLogStore\" ) \\ . getOrCreate () Note above we specified the option spark.hadoop.nessie.ref . This value sets the default branch that the delta catalog will use. This can be changed by changing the hadoopConfiguration however best practice would be to use a single write context (branch) for the duration of the spark session. The key to enabling Nessie is to instruct Delta to use the Nessie specific LogStore and LogFileHandler . With these enabled the Delta core library will delegate transaction handling to Nessie. Finally, note we have explicitly enabled Delta\u2019s SQL extensions which enable Delta specific SQL in Spark3. Warning Currently Delta metadata operations like VACUUM are descructive to Nessie managed Delta tables. Do not run these operations. Future versions of Nessie will disable these commands when Nessie is activated. Writing \u00b6 Spark support is constantly evolving and the differences in Spark3 vs Spark2.4 is considerable. See the delta docs for an up to date support table. Spark2 \u00b6 Spark2.4 supports reads, appends, overwrites in Delta via data frames. Spark 3 additionally supports SQL syntax. Nessie tables in delta can be written via the Nessi enabled Delta client. The Delta writer allows for either overwrite or append mode in a standard spark.write . Java regionDf = spark . read (). load ( \"data/region.parquet\" ) regionDf . write (). format ( \"delta\" ). mode ( \"overwrite\" ). save ( \"/location/to/delta/testing/region\" ) Python region_df = spark . read . load ( \"data/region.parquet\" ) region_df . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . save ( \"/location/to/delta/testing/region\" ) === \u201cSQL\u201d CREATE TABLE nessie . testing . city ( C_CITYKEY BIGINT , C_NAME STRING , N_NATIONKEY BIGINT , C_COMMENT STRING ) USING delta PARTITIONED BY ( N_NATIONKEY ) LOCATION 'path/to/delta/testing/city' -- SELECT .. can be added to the sql statement to perform a CTAS INSERT [ OVERWRITE ] INTO nessie . testing . city VALUES ( 1 , 'a' , 1 , 'comment' ) Here we simply read a file from the default filesystem and write it to a new nessie Delta table. This will trigger a commit on current context\u2019s branch. For the examples above we have performed commits on the branch specified when we set our spark configuration. Had we not specified the context in our spark configuration all operations would have defaulted to the default branch defined by the server. This is a strong pattern for a spark job which is for example writing data as part of a wider ETL job. It will only ever need one context or branch to write to. If however you are running an interactive session and would like to write to a specific branch without changing context the following should be used to change the context. Java spark . sparkContext (). hadoopConfiguration (). set ( \"nessie.ref\" , \"dev\" ) regionDf = spark . read (). load ( \"data/region.parquet\" ) regionDf . write (). format ( \"delta\" ). mode ( \"overwrite\" ). save ( \"/location/to/delta/testing/region\" ) Python spark . sparkContext . _jsc . hadoopConfiguration () . set ( \"nessie.ref\" , \"dev\" ) region_df = spark . read . load ( \"data/region.parquet\" ) region_df . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . save ( \"/location/to/delta/testing/region\" ) SQL -- change hadoop configuration externally using the Java or Python syntax CREATE TABLE nessie . testing . city ( C_CITYKEY BIGINT , C_NAME STRING , N_NATIONKEY BIGINT , C_COMMENT STRING ) USING iceberg PARTITIONED BY ( N_NATIONKEY ) -- AS SELECT .. can be added to the sql statement to perform a CTAS INSERT INTO nessie . testing . city VALUES ( 1 , 'a' , 1 , 'comment' ) We have to manually change the hadoopConfiguration for the SparkContext for a Delta table to be initialised with the correct reference. This will change in the near future when it will be possible to use the same branch@ref syntax as Iceberg inside of delta. Currently it isn\u2019t possible to change the ref from SQL directly. This should be fixed in an upcomming release. Note Delta by default caches tables internally. If an action has to happen on the same table but a different branch the cache first should be cleared. DeltaLog.clearCache() . Reading \u00b6 Reading is similar between Spark2 and Spark3. We will look at both versions together in this section. To read a Nessie table in Delta Lake simply: Java regionDf = spark . read (). format ( \"delta\" ). load ( \"/path/to/delta/testing/region\" ) Python region_df = spark . read . format ( \"delta\" ) . load ( \"/path/to/delta/testing/region\" ) SQL SELECT * FROM '/path/to/delta/testing/region' The examples above all use the default branch defined on initialisation. Future versions will add the ability to specify a branch and timestamp similar to Iceberg. Currently to switch branches a similar technique as writing is required (manually changing the hadoopConfiguration). History can be viewed on the command line or via the python client and a specific hash based on commit time can be extracted for use in the spark config. It is recommended to use the time-travel features of Nessie over the Delta features as Nessie history is consistent across the entire database.","title":"Spark"},{"location":"tools/spark/#spark","text":"Nessie can be used with Spark in several different ways. These include:","title":"Spark"},{"location":"tools/spark/#iceberg-client","text":"Nessie works seamlessly with Iceberg in Spark2 and Spark3. Nessie is implemented as a custom iceberg catalog and therefore supports all features available to any Iceberg client. This includes Spark structured streaming, Presto and Flink. See the iceberg docs for more info. Note You can follow along interactively in a Jupyter notebook by following the instructions here . To access Nessie from a spark cluster make sure the spark.jars spark option is set to include the Spark 2 or Spark 3 Nessie plugin jar. This fat jar is distributed by the Apache Iceberg project and contains all Apache Iceberg libraries required for operation, including the built in Nessie Catalog. In pyspark this would look like SparkSession . builder . config ( 'spark.jars' , 'path/to/iceberg-spark3-0.11.1.jar' ) ... rest of spark config . getOrCreate () The docs for the Java api in Iceberg explain how to use a Catalog . The only change is that a Nessie catalog should be instantiated Java Catalog catalog = new NessieCatalog ( spark . sparkContext (). hadoopConfiguration ()) Python catalog = jvm . NessieCatalog ( sc . _jsc . hadoopConfiguration ()) Note Iceberg\u2019s python libraries are still under active development. Actions against catalogs in pyspark still have to go through the jvm objects. See the demo directory for details. The Nessie Catalog needs the following parameters set in the Spark/Hadoop config. url = full url to nessie warehouse = where to store nessie tables ref = the ref or context that nessie will operate on (if different from default branch) auth_type = authentication type (BASIC, NONE or AWS) These are set as follows in code (or through other methods as described here ) Java //for a local spark instance conf . set ( \"spark.sql.catalog.nessie.url\" , url ) . set ( \"spark.sql.catalog.nessie.ref\" , branch ) . set ( \"spark.sql.nessie.nessie.auth_type\" , authType ) . set ( \"spark.sql.nessie.nessie.catalog-impl\" , \"org.apache.iceberg.nessie.NessieCatalog\" ) . set ( \"spark.sql.nessie.nessie.warehouse\" , fullPathToWarehouse ) . set ( \"spark.sql.catalog.nessie\" , \"org.apache.iceberg.spark.SparkCatalog\" ); spark = SparkSession . builder () . master ( \"local[2]\" ) . config ( conf ) . getOrCreate (); Python # here we are assuming NONE authorisation spark = SparkSession . builder \\ . config ( \"spark.jars\" , \"/path/to/downloaded/iceberg-spark3-0.11.1.jar\" ) \\ . config ( \"spark.sql.catalog.nessie.url\" , \"http://localhost:19120/api/v1\" ) . config ( \"spark.sql.catalog.nessie.ref\" , \"main\" ) . config ( \"spark.sql.nessie.nessie.auth_type\" , \"NONE\" ) . config ( \"spark.sql.nessie.nessie.catalog-impl\" , \"org.apache.iceberg.nessie.NessieCatalog\" ) . config ( \"spark.sql.nessie.nessie.warehouse\" , fullPathToWarehouse ) . config ( \"spark.sql.catalog.nessie\" , \"org.apache.iceberg.spark.SparkCatalog\" ); . getOrCreate () Note above we specified the option spark.sql.catalog.nessie.ref . This value sets the default branch that the iceberg catalog will use. We have specified spark.sql.catalog.nessie to point to SparkCatalog . This is a Spark option to set the catalog nessie to be managed by Nessie\u2019s Catalog implementation. All configuration for the Nessie catalog exists below this spark.sql.catalog.nessie configuration namespace. The catalog name is not important, it is important that the required options are all given below the catalog name.","title":"Iceberg client"},{"location":"tools/spark/#writing","text":"Spark support is constantly evolving and the differences in Spark3 vs Spark2.4 are considerable. See the iceberg docs for an up to date support table.","title":"Writing"},{"location":"tools/spark/#spark2","text":"Spark2.4 supports reads, appends, overwrites in Iceberg. Nessie tables in iceberg can be written via the Nessie Iceberg Catalog instantiated above. Iceberg in Spark2.4 has no ability to create tables so before a table can be appended to or overwritten the table must be first created via an Iceberg Catalog. This is straightforward in Java but requires addressing jvm objects directly in Python (until the python library for iceberg is released). Java 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // first instantiate the catalog NessieCatalog catalog = new NessieCatalog (); catalog . setConf ( sc . hadoopConfiguration ()); catalog . initialize ( \"nessie\" , ImmutableMap . of ( \"ref\" , ref , \"url\" , url , \"warehouse\" , pathToWarehouse )); // Creating table by first creating a table name with namespace TableIdentifier region_name = TableIdentifier . parse ( \"testing.region\" ); // next create the schema Schema region_schema = Schema ( [ Types . NestedField . optional ( 1 , \"R_REGIONKEY\" , Types . LongType . get ()), Types . NestedField . optional ( 2 , \"R_NAME\" , Types . StringType . get ()), Types . NestedField . optional ( 3 , \"R_COMMENT\" , Types . StringType . get ()), ] ); // and the partition PartitionSpec region_spec = PartitionSpec . unpartitioned (); // finally create the table catalog . createTable ( region_name , region_schema , region_spec ); Python 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 sc = spark . sparkContext jvm = sc . _gateway . jvm # import jvm libraries for iceberg catalogs and schemas java_import ( jvm , \"org.projectnessie.iceberg.NessieCatalog\" ) java_import ( jvm , \"org.apache.iceberg.catalog.TableIdentifier\" ) java_import ( jvm , \"org.apache.iceberg.Schema\" ) java_import ( jvm , \"org.apache.iceberg.types.Types\" ) java_import ( jvm , \"org.apache.iceberg.PartitionSpec\" ) # first instantiate the catalog catalog = jvm . NessieCatalog () catalog . setConf ( sc . _jsc . hadoopConfiguration ()) catalog . initialize ( \"nessie\" , { \"ref\" : ref , \"url\" : url , \"warehouse\" : pathToWarehouse }) # Creating table by first creating a table name with namespace region_name = jvm . TableIdentifier . parse ( \"testing.region\" ) # next create the schema region_schema = jvm . Schema ([ jvm . Types . NestedField . optional ( 1 , \"R_REGIONKEY\" , jvm . Types . LongType . get ()), jvm . Types . NestedField . optional ( 2 , \"R_NAME\" , jvm . Types . StringType . get ()), jvm . Types . NestedField . optional ( 3 , \"R_COMMENT\" , jvm . Types . StringType . get ()), ]) # and the partition region_spec = jvm . PartitionSpec . unpartitioned () # finally create the table region_table = catalog . createTable ( region_name , region_schema , region_spec ) When looking at the Python code above, lines 1-11 are importing jvm objects into pyspark. Lines 12-25 create the table name, schema and partition spec. These actions will be familiar to seasoned iceberg users and are wholly iceberg operations. Line 29 is where our initial iceberg metadata is finally written to disk and a commit takes place on Nessie. Now that we have created an Iceberg table in nessie we can write to it. The iceberg DataSourceV2 allows for either overwrite or append mode in a standard spark.write . Java regionDf = spark . read (). load ( \"data/region.parquet\" ); regionDf . write (). format ( \"iceberg\" ). mode ( \"overwrite\" ). save ( \"nessie.testing.region\" ); Python region_df = spark . read . load ( \"data/region.parquet\" ) region_df . write . format ( \"iceberg\" ) . mode ( \"overwrite\" ) . save ( \"nessie.testing.region\" ) Here we simply read a file from the default filesystem and write it to an existing nessie iceberg table. This will trigger a commit on current context\u2019s branch. For the examples above we have performed commits on the branch specified when we set our spark configuration. Had we not specified the context in our spark configuration all operations would have defaulted to the default branch defined by the server. This is a strong pattern for a spark job which is for example writing data as part of a wider ETL job. It will only ever need one context or branch to write to. If however you are running an interactive session and would like to write to a specific branch you would have to create a new Spark Conf. Java spark_dev = spark . newSession (); spark_dev . conf . set ( \"spark.sql.catalog.nessie.ref\" , \"dev\" ); regionDf = spark_dev . read (). load ( \"data/region.parquet\" ); regionDf . write (). format ( \"iceberg\" ). mode ( \"overwrite\" ). save ( \"nessie.testing.region\" ); Python spark_dev = spark . newSession () spark_dev . conf . set ( \"spark.sql.catalog.nessie.ref\" , \"dev\" ) region_df = spark_dev . read . load ( \"data/region.parquet\" ) region_df . write . format ( \"iceberg\" ) . mode ( \"overwrite\" ) . save ( \"nessie.testing.region\" ) Note the extra option clause in the write command. This will ensure the commit happens on the dev branch rather than the default branch.","title":"Spark2"},{"location":"tools/spark/#spark3","text":"The write path for Spark3 is slightly different and easier to work with. These changes haven\u2019t made it to pyspark yet so writing dataframes looks much the same there, including having to create the table. Spark3 table creation/insertion is as follows: Java regionDf = spark . read (). load ( ' data / region . parquet ' ) //create regionDf . writeTo ( \"nessie.testing.region\" ). create () //append regionDf . writeTo ( \"nessie.testing.region\" ). append () //overwrite partition regionDf . writeTo ( \"nessie.testing.region\" ). overwritePartitions () Python # same code as the spark2 section above to create the testing.region table region_df = spark . read . load ( \"data/region.parquet\" ) region_df . write . format ( \"iceberg\" ) . mode ( \"overwrite\" ) . save ( \"nessie.testing.region\" ) SQL CREATE TABLE nessie . testing . city ( C_CITYKEY BIGINT , C_NAME STRING , N_NATIONKEY BIGINT , C_COMMENT STRING ) USING iceberg PARTITIONED BY ( N_NATIONKEY ) -- AS SELECT .. can be added to the sql statement to perform a CTAS INSERT INTO nessie . testing . city VALUES ( 1 , 'a' , 1 , 'comment' ) The full list of operations can be found here . Everything that Iceberg supports the Nessie Iceberg Catalog also supports.","title":"Spark3"},{"location":"tools/spark/#reading","text":"Reading is more straightforward between spark 2 and spark 3. We will look at both versions together in this section. To read a Nessie table in iceberg simply: Java regionDf = spark . read (). format ( \"iceberg\" ). load ( \"nessie.testing.region\" ) // Spark2 regionDf = spark . table ( \"nessie.testing.region\" ) // Spark3 Python # same code as above to create the testing.region table region_df = spark . read . format ( \"iceberg\" ) . load ( \"nessie.testing.region\" ) SQL SELECT * FROM nessie . testing . city -- Spark3 only The examples above all use the default branch defined on initialisation. There are several ways to reference specific branches or hashes from within a read statement. We will take a look at a few now from pyspark3, the rules are the same across all environments though. The general pattern is <table>@<branch> . Table must be present and either branch and/or hash are optional. We will throw an error if branch or hash don\u2019t exist. Branch or hash references in the table name will override passed option s and the settings in the Spark/Hadoop configs. 1 2 3 4 spark . read () . format ( \"iceberg\" ) . load ( \"testing.region@dev\" ) # read from branch dev spark . read () . format ( \"iceberg\" ) . load ( \"testing.region@<hash>\" ) # read specifically from hash spark . sql ( \"SELECT * FROM nessie.testing.`region@dev`\" ) spark . sql ( \"SELECT * FROM nessie.testing.`region@<hash>`\" ) Notice in the SQL statements the table@branch must be escaped separately from namespace or catalog arguments. Future versions may add the ability to specify a timestamp to query the data at a specific point in time (time-travel). In the meantime the history can be viewed on the command line or via the python client and a specific hash based on commit time can be extracted for use in the spark catalog. It is recommended to use the time-travel features of Nessie over the Iceberg features as Nessie history is consistent across the entire database.","title":"Reading"},{"location":"tools/spark/#delta-lake","text":"Note You can follow along interactively in a Jupyter notebook by following the instructions here . Delta Lake support in Nessie requires some minor modifications to the core Delta libraries. This patch is still ongoing, in the meantime Nessie will not work on Databricks and must be used with the open source Delta. Nessie is able to interact with Delta Lake by implementing a custom version of Delta\u2019s LogStore interface . This ensures that all filesystem changes are recorded by Nessie as commits. The benefit of this approach is the core ACID primitives are handled by Nessie. The limitations around concurrency that Delta would normally have are removed, any number of readers and writers can simultaneously interact with a Nessie managed Delta Lake table. To access Nessie from a spark cluster make sure the spark.jars spark option is set to include the Spark 2 or Spark 3 jar. These jars contain all Nessie and Delta Lake libraries required for operation. Note the spark3 jar is for Delta versions >7.0 on Spark3 and the spark2 jar is for Delta versions 6.x on Spark2.4 In pyspark this would look like SparkSession . builder . config ( 'spark.jars' , 'path/to/nessie-deltalake-spark2-0.5.1.jar' ) ... rest of spark config . getOrCreate () The Nessie LogStore needs the following parameters set in the Spark/Hadoop config. nessie.url = full url to nessie nessie.username = username if using basic auth, omitted otherwise nessie.password = password if using basic auth, omitted otherwise nessie.auth.type = authentication type (BASIC, NONE or AWS) spark.delta.logFileHandler.class=org.projectnessie.deltalake.NessieLogFileMetaParser spark.delta.logStore.class=org.projectnessie.deltalake.NessieLogStore These are set as follows in code (or through other methods as described here ) === \u201cJava\u201d`` //for a local spark instance conf . set ( \"spark.hadoop.nessie.url\" , url ) . set ( \"spark.hadoop.nessie.ref\" , branch ) . set ( \"spark.hadoop.nessie.auth_type\" , authType ) . set ( \"spark.sql.catalog.spark_catalog\" , \"org.apache.spark.sql.delta.catalog.DeltaCatalog\" ) \\ . set ( \"spark.sql.extensions\" , \"io.delta.sql.DeltaSparkSessionExtension\" ) \\ . set ( \"spark.delta.logStore.class\" , \"org.projectnessie.deltalake.NessieLogStore\" ) . set ( \"spark.delta.logFileHandler.class\" , \"org.projectnessie.deltalake.NessieLogFileMetaParser\" ) spark = SparkSession . builder () . master ( \"local[2]\" ) . config ( conf ) . getOrCreate (); Python # here we are assuming NONE authorisation spark = SparkSession . builder \\ . config ( \"spark.jars\" , \"../../clients/deltalake-spark3/target/nessie-deltalake-spark3-0.5.1.jar\" ) \\ . config ( \"spark.hadoop.nessie.url\" , \"http://localhost:19120/api/v1\" ) \\ . config ( \"spark.hadoop.nessie.ref\" , \"main\" ) \\ . config ( \"spark.sql.catalog.spark_catalog\" , \"org.apache.spark.sql.delta.catalog.DeltaCatalog\" ) \\ . config ( \"spark.sql.extensions\" , \"io.delta.sql.DeltaSparkSessionExtension\" ) \\ . config ( \"spark.delta.logFileHandler.class\" , \"org.projectnessie.deltalake.NessieLogFileMetaParser\" ) \\ . config ( \"spark.delta.logStore.class\" , \"org.projectnessie.deltalake.NessieLogStore\" ) \\ . getOrCreate () Note above we specified the option spark.hadoop.nessie.ref . This value sets the default branch that the delta catalog will use. This can be changed by changing the hadoopConfiguration however best practice would be to use a single write context (branch) for the duration of the spark session. The key to enabling Nessie is to instruct Delta to use the Nessie specific LogStore and LogFileHandler . With these enabled the Delta core library will delegate transaction handling to Nessie. Finally, note we have explicitly enabled Delta\u2019s SQL extensions which enable Delta specific SQL in Spark3. Warning Currently Delta metadata operations like VACUUM are descructive to Nessie managed Delta tables. Do not run these operations. Future versions of Nessie will disable these commands when Nessie is activated.","title":"Delta Lake"},{"location":"tools/spark/#writing_1","text":"Spark support is constantly evolving and the differences in Spark3 vs Spark2.4 is considerable. See the delta docs for an up to date support table.","title":"Writing"},{"location":"tools/spark/#spark2_1","text":"Spark2.4 supports reads, appends, overwrites in Delta via data frames. Spark 3 additionally supports SQL syntax. Nessie tables in delta can be written via the Nessi enabled Delta client. The Delta writer allows for either overwrite or append mode in a standard spark.write . Java regionDf = spark . read (). load ( \"data/region.parquet\" ) regionDf . write (). format ( \"delta\" ). mode ( \"overwrite\" ). save ( \"/location/to/delta/testing/region\" ) Python region_df = spark . read . load ( \"data/region.parquet\" ) region_df . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . save ( \"/location/to/delta/testing/region\" ) === \u201cSQL\u201d CREATE TABLE nessie . testing . city ( C_CITYKEY BIGINT , C_NAME STRING , N_NATIONKEY BIGINT , C_COMMENT STRING ) USING delta PARTITIONED BY ( N_NATIONKEY ) LOCATION 'path/to/delta/testing/city' -- SELECT .. can be added to the sql statement to perform a CTAS INSERT [ OVERWRITE ] INTO nessie . testing . city VALUES ( 1 , 'a' , 1 , 'comment' ) Here we simply read a file from the default filesystem and write it to a new nessie Delta table. This will trigger a commit on current context\u2019s branch. For the examples above we have performed commits on the branch specified when we set our spark configuration. Had we not specified the context in our spark configuration all operations would have defaulted to the default branch defined by the server. This is a strong pattern for a spark job which is for example writing data as part of a wider ETL job. It will only ever need one context or branch to write to. If however you are running an interactive session and would like to write to a specific branch without changing context the following should be used to change the context. Java spark . sparkContext (). hadoopConfiguration (). set ( \"nessie.ref\" , \"dev\" ) regionDf = spark . read (). load ( \"data/region.parquet\" ) regionDf . write (). format ( \"delta\" ). mode ( \"overwrite\" ). save ( \"/location/to/delta/testing/region\" ) Python spark . sparkContext . _jsc . hadoopConfiguration () . set ( \"nessie.ref\" , \"dev\" ) region_df = spark . read . load ( \"data/region.parquet\" ) region_df . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . save ( \"/location/to/delta/testing/region\" ) SQL -- change hadoop configuration externally using the Java or Python syntax CREATE TABLE nessie . testing . city ( C_CITYKEY BIGINT , C_NAME STRING , N_NATIONKEY BIGINT , C_COMMENT STRING ) USING iceberg PARTITIONED BY ( N_NATIONKEY ) -- AS SELECT .. can be added to the sql statement to perform a CTAS INSERT INTO nessie . testing . city VALUES ( 1 , 'a' , 1 , 'comment' ) We have to manually change the hadoopConfiguration for the SparkContext for a Delta table to be initialised with the correct reference. This will change in the near future when it will be possible to use the same branch@ref syntax as Iceberg inside of delta. Currently it isn\u2019t possible to change the ref from SQL directly. This should be fixed in an upcomming release. Note Delta by default caches tables internally. If an action has to happen on the same table but a different branch the cache first should be cleared. DeltaLog.clearCache() .","title":"Spark2"},{"location":"tools/spark/#reading_1","text":"Reading is similar between Spark2 and Spark3. We will look at both versions together in this section. To read a Nessie table in Delta Lake simply: Java regionDf = spark . read (). format ( \"delta\" ). load ( \"/path/to/delta/testing/region\" ) Python region_df = spark . read . format ( \"delta\" ) . load ( \"/path/to/delta/testing/region\" ) SQL SELECT * FROM '/path/to/delta/testing/region' The examples above all use the default branch defined on initialisation. Future versions will add the ability to specify a branch and timestamp similar to Iceberg. Currently to switch branches a similar technique as writing is required (manually changing the hadoopConfiguration). History can be viewed on the command line or via the python client and a specific hash based on commit time can be extracted for use in the spark config. It is recommended to use the time-travel features of Nessie over the Delta features as Nessie history is consistent across the entire database.","title":"Reading"},{"location":"tools/ui/","text":"Web UI \u00b6 Nessie comes with a simple web ui that allows you to understand what is in your Nessie repository. You can view existing tags and branches as well as the content within them. The UI automatically runs as part of starting the Nessie service. If running locally, you can find the UI at localhost:19120 . Swagger UI \u00b6 The Swagger UI rest api test UI can also be found running in the service. It is located at localhost:19120/swagger-ui .","title":"Web UI"},{"location":"tools/ui/#web-ui","text":"Nessie comes with a simple web ui that allows you to understand what is in your Nessie repository. You can view existing tags and branches as well as the content within them. The UI automatically runs as part of starting the Nessie service. If running locally, you can find the UI at localhost:19120 .","title":"Web UI"},{"location":"tools/ui/#swagger-ui","text":"The Swagger UI rest api test UI can also be found running in the service. It is located at localhost:19120/swagger-ui .","title":"Swagger UI"},{"location":"try/","text":"Getting Started \u00b6 As part of each release, Nessie is made available as a fast-start docker image. This is the easiest and fastest way to try out nessie. The image is relatively small and builds on top of standard base images. To get started: $ docker pull projectnessie/nessie Pulling from projectnessie/nessie 0fd3b5213a9b: Already exists aebb8c556853: Already exists a50558612231: Pull complete Digest: sha256:bda3dead4eb51a4c0ff87c7ce5a81ad49a37dd17d785f2549f4559f06cbf24d6 Status: Downloaded newer image for projectnessie/nessie $ docker run -p 19120 :19120 projectnessie/nessie __ ____ __ _____ ___ __ ____ ______ --/ __ \\/ / / / _ | / _ \\/ //_/ / / / __/ -/ /_/ / /_/ / __ | / , _/ ,< / /_/ / \\ \\ -- \\_ __ \\_\\_ ___/_/ | _/_/ | _/_/ | _ | \\_ ___/___/ 2020 -10-01 21 :50:27,166 INFO [ io.quarkus ] ( main ) nessie-quarkus 0 .1-SNAPSHOT native ( powered by Quarkus 1 .8.1.Final ) started in 0 .025s. Listening on: http://0.0.0.0:19120 2020 -10-01 21 :50:27,166 INFO [ io.quarkus ] ( main ) Profile prod activated. 2020 -10-01 21 :50:27,166 INFO [ io.quarkus ] ( main ) Installed features: [ amazon-dynamodb, cdi, hibernate-validator, jaeger, resteasy, resteasy-jackson, security, security-properties-file, sentry, smallrye-health, smallrye-metrics, smallrye-openapi, smallrye-opentracing ] Once the docker image is up and running, you can install the Nessie cli . $ pip install pynessie You\u2019re now ready to start using Nessie. To create a new branch, you can do the following: # create a branch pointing to the same hash as # the current default branch (typically the main branch) $ nessie branch my_branch From there, you can use one of the three main Nessie integrations of: Take a look at your current empty repository in the Web UI NessieCatalog for Iceberg within Spark Nessie Log Handle for Delta Lake within Spark Hive or HMS compatible tool use with the Nessie HMS Bridge","title":"Getting Started"},{"location":"try/#getting-started","text":"As part of each release, Nessie is made available as a fast-start docker image. This is the easiest and fastest way to try out nessie. The image is relatively small and builds on top of standard base images. To get started: $ docker pull projectnessie/nessie Pulling from projectnessie/nessie 0fd3b5213a9b: Already exists aebb8c556853: Already exists a50558612231: Pull complete Digest: sha256:bda3dead4eb51a4c0ff87c7ce5a81ad49a37dd17d785f2549f4559f06cbf24d6 Status: Downloaded newer image for projectnessie/nessie $ docker run -p 19120 :19120 projectnessie/nessie __ ____ __ _____ ___ __ ____ ______ --/ __ \\/ / / / _ | / _ \\/ //_/ / / / __/ -/ /_/ / /_/ / __ | / , _/ ,< / /_/ / \\ \\ -- \\_ __ \\_\\_ ___/_/ | _/_/ | _/_/ | _ | \\_ ___/___/ 2020 -10-01 21 :50:27,166 INFO [ io.quarkus ] ( main ) nessie-quarkus 0 .1-SNAPSHOT native ( powered by Quarkus 1 .8.1.Final ) started in 0 .025s. Listening on: http://0.0.0.0:19120 2020 -10-01 21 :50:27,166 INFO [ io.quarkus ] ( main ) Profile prod activated. 2020 -10-01 21 :50:27,166 INFO [ io.quarkus ] ( main ) Installed features: [ amazon-dynamodb, cdi, hibernate-validator, jaeger, resteasy, resteasy-jackson, security, security-properties-file, sentry, smallrye-health, smallrye-metrics, smallrye-openapi, smallrye-opentracing ] Once the docker image is up and running, you can install the Nessie cli . $ pip install pynessie You\u2019re now ready to start using Nessie. To create a new branch, you can do the following: # create a branch pointing to the same hash as # the current default branch (typically the main branch) $ nessie branch my_branch From there, you can use one of the three main Nessie integrations of: Take a look at your current empty repository in the Web UI NessieCatalog for Iceberg within Spark Nessie Log Handle for Delta Lake within Spark Hive or HMS compatible tool use with the Nessie HMS Bridge","title":"Getting Started"},{"location":"try/configuration/","text":"Configuration \u00b6 Nessie is configurable via setting available properties as listed in the application.properties file. These configuration settings are able to be set when starting up the docker image by adding them to the Docker invocation prefixed with -D . For example, if you want to set Nessie to use the INMEMORY version store running on port 8080, you would run the following: docker run -p 8080 :8080 projectnessie/nessie \\ -Dnessie.version.store.type = INMEMORY \\ -Dquarkus.http.port = 8080 Core Nessie Configuration Settings \u00b6 # which type of version store to use: JGIT, INMEMORY, DYNAMO. JGIT is for local testing, DYNAMO preferred for production nessie.version.store.type = DYNAMO # path if using JGIT nessie.version.store.jgit.directory = /tmp/jgit ## Dynamo version store specific configuration # should Nessie create its own dynamo tables nessie.version.store.dynamo.initialize = false ## Dynamo Configuration quarkus.dynamodb.aws.region = us-west-2 quarkus.dynamodb.aws.credentials.type = DEFAULT # quarkus.dynamodb.endpoint-override=http://localhost:8000 General Server Settings \u00b6 # Quarkus settings ## Visit here for all configs: https://quarkus.io/guides/all-config ## some parameters are only configured at build time. These have been marked as such https://quarkus.io/guides/config#overriding-properties-at-runtime quarkus.log.level = INFO ## Quarkus http related settings quarkus.http.port = 19120 quarkus.http.test-port = 19121 quarkus.http.access-log.enabled = true # fixed at buildtime quarkus.resteasy.path = /api/v1 quarkus.resteasy.gzip.enabled = true ## Quarkus auth settings #quarkus.oidc.credentials.secret= #quarkus.oidc.client-id= #quarkus.oidc.auth-server-url= # fixed at buildtime quarkus.http.auth.basic = false quarkus.oidc.enabled = false ## Quarkus swagger settings # fixed at buildtime quarkus.swagger-ui.always-include = false quarkus.swagger-ui.enable = false ## Quarkus monitoring and tracing settings ## jaeger specific settings quarkus.jaeger.service-name = nessie quarkus.jaeger.sampler-type = ratelimiting quarkus.jaeger.sampler-param = 1 #quarkus.jaeger.endpoint=http://localhost:14268/api/traces # fixed at buildtime quarkus.jaeger.metrics.enabled = true ## sentry specific settings quarkus.log.sentry.level = ERROR quarkus.log.sentry.in-app-packages = org.projectnessie quarkus.log.sentry = false #quarkus.log.sentry.dsn=https://<fillin>.ingest.sentry.io/<fillin> Info A complete set of configuration options for Quarkus can be found on quarkus.io Metrics \u00b6 Metrics are published using prometheus and can be collected via standard methods. See: Prometheus . Swagger \u00b6 The Swagger UI allows for testing the REST API and reading the API docs. It is available via localhost:19120/swagger-ui","title":"Configuration"},{"location":"try/configuration/#configuration","text":"Nessie is configurable via setting available properties as listed in the application.properties file. These configuration settings are able to be set when starting up the docker image by adding them to the Docker invocation prefixed with -D . For example, if you want to set Nessie to use the INMEMORY version store running on port 8080, you would run the following: docker run -p 8080 :8080 projectnessie/nessie \\ -Dnessie.version.store.type = INMEMORY \\ -Dquarkus.http.port = 8080","title":"Configuration"},{"location":"try/configuration/#core-nessie-configuration-settings","text":"# which type of version store to use: JGIT, INMEMORY, DYNAMO. JGIT is for local testing, DYNAMO preferred for production nessie.version.store.type = DYNAMO # path if using JGIT nessie.version.store.jgit.directory = /tmp/jgit ## Dynamo version store specific configuration # should Nessie create its own dynamo tables nessie.version.store.dynamo.initialize = false ## Dynamo Configuration quarkus.dynamodb.aws.region = us-west-2 quarkus.dynamodb.aws.credentials.type = DEFAULT # quarkus.dynamodb.endpoint-override=http://localhost:8000","title":"Core Nessie Configuration Settings"},{"location":"try/configuration/#general-server-settings","text":"# Quarkus settings ## Visit here for all configs: https://quarkus.io/guides/all-config ## some parameters are only configured at build time. These have been marked as such https://quarkus.io/guides/config#overriding-properties-at-runtime quarkus.log.level = INFO ## Quarkus http related settings quarkus.http.port = 19120 quarkus.http.test-port = 19121 quarkus.http.access-log.enabled = true # fixed at buildtime quarkus.resteasy.path = /api/v1 quarkus.resteasy.gzip.enabled = true ## Quarkus auth settings #quarkus.oidc.credentials.secret= #quarkus.oidc.client-id= #quarkus.oidc.auth-server-url= # fixed at buildtime quarkus.http.auth.basic = false quarkus.oidc.enabled = false ## Quarkus swagger settings # fixed at buildtime quarkus.swagger-ui.always-include = false quarkus.swagger-ui.enable = false ## Quarkus monitoring and tracing settings ## jaeger specific settings quarkus.jaeger.service-name = nessie quarkus.jaeger.sampler-type = ratelimiting quarkus.jaeger.sampler-param = 1 #quarkus.jaeger.endpoint=http://localhost:14268/api/traces # fixed at buildtime quarkus.jaeger.metrics.enabled = true ## sentry specific settings quarkus.log.sentry.level = ERROR quarkus.log.sentry.in-app-packages = org.projectnessie quarkus.log.sentry = false #quarkus.log.sentry.dsn=https://<fillin>.ingest.sentry.io/<fillin> Info A complete set of configuration options for Quarkus can be found on quarkus.io","title":"General Server Settings"},{"location":"try/configuration/#metrics","text":"Metrics are published using prometheus and can be collected via standard methods. See: Prometheus .","title":"Metrics"},{"location":"try/configuration/#swagger","text":"The Swagger UI allows for testing the REST API and reading the API docs. It is available via localhost:19120/swagger-ui","title":"Swagger"},{"location":"try/releases/","text":"Releases \u00b6 0.6.0 Release (xxx xx, 2021) \u00b6 TreeApi.createReference() + commitMultipleOperations() return commit information 0.5.1 Release (April 9, 2021) \u00b6 Fix Gradle plugin (non-deterministic order of dependencies causing failures) Fix Web-UI 0.5.0 Release (April 8, 2021) \u00b6 Iceberg table GC support Consistency fixes under high load Breaking changes to the backend to support richer commit metadata and data types Performance, metrics and tracing improvements Gradle plugin improvement for incremental builds 0.4.0 Release (March 8, 2020) \u00b6 rename base package to org.projectnessie NessieClient is now an interface and some easier builders initial implementation of GC algorithm major refactor of tiered classes for better modularity and extensibility observability improvements including better DynamoDB metrics and opentracing support for the client 0.3.0 Release (December 30, 2020) \u00b6 118 commits since 0.2.1 Replace jax-rs client with one based on HttpURLConnection Update Quarkus to 1.10.5 Improvements to Server including better UI routing, validation checks on inputs etc Various improvements to python client and cli. Including python3.9 support 0.2.1 Release (October 30, 2020) \u00b6 Fix missing dateutil requirement for pynessie install Address path discovery in Gradle plugin (for testing in external integrations) 0.2.0 Release (October 29, 2020) \u00b6 Update Nessie CLI commands to better match git syntax Update REST Apis to be more consistent and better Add support for merge & cherry-pick in DynamoDB storage backend Add WebUI Introduce new DynamoDB optimizations to support faster log and entry retrieval Update to Quarkus 1.9.1 Expose the new Store interface for low level storage implementations Introduce Quarkus Gradle runner plugin for easier third-party testing (e.g. Iceberg) Enable swagger-ui by default in Nessie service 0.1.0 Release (October 1, 2020) \u00b6 Initial release","title":"Releases"},{"location":"try/releases/#releases","text":"","title":"Releases"},{"location":"try/releases/#060-release-xxx-xx-2021","text":"TreeApi.createReference() + commitMultipleOperations() return commit information","title":"0.6.0 Release (xxx xx, 2021)"},{"location":"try/releases/#051-release-april-9-2021","text":"Fix Gradle plugin (non-deterministic order of dependencies causing failures) Fix Web-UI","title":"0.5.1 Release (April 9, 2021)"},{"location":"try/releases/#050-release-april-8-2021","text":"Iceberg table GC support Consistency fixes under high load Breaking changes to the backend to support richer commit metadata and data types Performance, metrics and tracing improvements Gradle plugin improvement for incremental builds","title":"0.5.0 Release (April 8, 2021)"},{"location":"try/releases/#040-release-march-8-2020","text":"rename base package to org.projectnessie NessieClient is now an interface and some easier builders initial implementation of GC algorithm major refactor of tiered classes for better modularity and extensibility observability improvements including better DynamoDB metrics and opentracing support for the client","title":"0.4.0 Release (March 8, 2020)"},{"location":"try/releases/#030-release-december-30-2020","text":"118 commits since 0.2.1 Replace jax-rs client with one based on HttpURLConnection Update Quarkus to 1.10.5 Improvements to Server including better UI routing, validation checks on inputs etc Various improvements to python client and cli. Including python3.9 support","title":"0.3.0 Release (December 30, 2020)"},{"location":"try/releases/#021-release-october-30-2020","text":"Fix missing dateutil requirement for pynessie install Address path discovery in Gradle plugin (for testing in external integrations)","title":"0.2.1 Release (October 30, 2020)"},{"location":"try/releases/#020-release-october-29-2020","text":"Update Nessie CLI commands to better match git syntax Update REST Apis to be more consistent and better Add support for merge & cherry-pick in DynamoDB storage backend Add WebUI Introduce new DynamoDB optimizations to support faster log and entry retrieval Update to Quarkus 1.9.1 Expose the new Store interface for low level storage implementations Introduce Quarkus Gradle runner plugin for easier third-party testing (e.g. Iceberg) Enable swagger-ui by default in Nessie service","title":"0.2.0 Release (October 29, 2020)"},{"location":"try/releases/#010-release-october-1-2020","text":"Initial release","title":"0.1.0 Release (October 1, 2020)"}]}